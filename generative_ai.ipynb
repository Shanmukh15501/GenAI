{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document loaders\n",
    "- Document Loaders are responsible for loading documents from a variety of sources.\n",
    "- Reference https://python.langchain.com/docs/how_to/#document-loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert text to vectors\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=os.getenv('OPENAI_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'D://GenAI//kalam.pdf', 'page': 0}, page_content=\"Avul Pakir Jainulabdeen Abdul Kalam BR (/Àà…ôbd äl k…ôÀàl…ëÀêm/ ‚ìò; 15 October 1931 ‚Äì 27 July \\n2015) was an Indian aerospace scientist and statesman who served as the 11th president of \\nIndia from 2002 to 2007. Born and raised in a Muslim family in Rameswaram, Tamil Nadu, \\nhe studied physics and aerospace engineering. He spent the next four decades as a \\nscientist and science administrator, mainly at the Defence Research and Development \\nOrganisation (DRDO) and Indian Space Research Organisation (ISRO) and was intimately \\ninvolved in India's civilian space programme and military missile development efforts.[2] He \\nthus came to be known as the Missile Man of India for his work on the development \\nof ballistic missile and launch vehicle technology.[3][4][5] He also played a pivotal \\norganisational, technical, and political role in India's Pokhran-II nuclear tests in 1998, the first \\nsince the original nuclear test by India in 1974.[6] \")]\n"
     ]
    }
   ],
   "source": [
    "#How to load PDFs\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"D://GenAI//kalam.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(file_path)\n",
    "pages = []\n",
    "for page in loader.load():\n",
    "    pages.append(page)\n",
    "\n",
    "print(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'D://GenAI//kalam.txt'}, page_content='The Defence Research and Development Organisation.\\nDefence Research and Development in Ministry of Defence of the Government of India.The Defence Research and Development Organisation.\\nDefence Research and Development in Ministry of Defence of the Government of India.The Defence Research and Development Organisation.\\nDefence Research and Development in Ministry of Defence of the Government of India.\\nThe Defence Research and Development Organisation.\\nDefence Research and Development in Ministry of Defence of the Government of India.\\nAPJ was born in india')]\n"
     ]
    }
   ],
   "source": [
    "#How to load txt file\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "file_path = \"D://GenAI//kalam.txt\"\n",
    "\n",
    "loader = TextLoader(file_path)\n",
    "pages = []\n",
    "\n",
    "for page in loader.load():\n",
    "    pages.append(page)\n",
    "\n",
    "print(pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Architecture | Guides | Tutorials | How-to guides | Conceptual guide | Integrations | API reference | Ecosystem | ü¶úüõ†Ô∏è LangSmith | ü¶úüï∏Ô∏è LangGraph | Additional resources | Versions | Security | Contributing' metadata={'source': 'https://python.langchain.com/docs/introduction/'}\n"
     ]
    }
   ],
   "source": [
    "#How to load web pages\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "page_url = \"https://python.langchain.com/docs/introduction/\"\n",
    "\n",
    "loader = WebBaseLoader(web_paths=[page_url],\n",
    "                       bs_kwargs={\n",
    "        \"parse_only\": bs4.SoupStrainer(class_=\"table-of-contents__link toc-highlight\"),\n",
    "    },\n",
    "    bs_get_text_kwargs={\"separator\": \" | \", \"strip\": True},)\n",
    "docs = []\n",
    "for doc in loader.load():\n",
    "    docs.append(doc)\n",
    "\n",
    "assert len(docs) == 1\n",
    "doc = docs[0]\n",
    "\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large language models (LLMs) have demonstrated impressive reasoning\n",
      "abilities, but they still strugg\n",
      "{'Entry ID': 'http://arxiv.org/abs/2410.13080v1', 'Published': datetime.date(2024, 10, 16), 'Title': 'Graph-constrained Reasoning: Faithful Reasoning on Knowledge Graphs with Large Language Models', 'Authors': 'Linhao Luo, Zicheng Zhao, Chen Gong, Gholamreza Haffari, Shirui Pan'}\n"
     ]
    }
   ],
   "source": [
    "#https://python.langchain.com/docs/integrations/providers/arxiv/#installation-and-setup\n",
    "#ArxivLoader is a tool used to fetch and load research papers from the arXiv database, which is a popular repository for academic papers in fields like physics, computer science, and mathematics. It allows users to retrieve papers in a structured format, enabling them to process and analyze the content programmatically.\n",
    "#for more data source providers go through this link https://python.langchain.com/docs/integrations/providers/all/\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "loader = ArxivLoader(\n",
    "    query=\"reasoning\"\n",
    ")\n",
    "\n",
    "docs = loader.get_summaries_as_docs()\n",
    "print(docs[0].page_content[:100])\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon Reeve Musk (; born June 28, 1971) is a businessman known for his key roles in the space company\n"
     ]
    }
   ],
   "source": [
    "#load from wikipedia\n",
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "\n",
    "retriever = WikipediaRetriever()\n",
    "docs = retriever.invoke(\"Elon Musk\")\n",
    "print(docs[0].page_content[:100])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation\n",
    "- How to recursively split by characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://python.langchain.com/docs/how_to/recursive_text_splitter/\n",
    "#Let's go through the parameters set above for RecursiveCharacterTextSplitter:\n",
    "\n",
    "#chunk_size: The maximum size of a chunk, where size is determined by the length_function.\n",
    "#chunk_overlap: Target overlap between chunks. Overlapping chunks helps to mitigate loss of information when context is divided between chunks.\n",
    "#length_function: Function determining the chunk size.\n",
    "#is_separator_regex: Whether the separator list (defaulting to [\"\\n\\n\", \"\\n\", \" \", \"\"]) should be interpreted as regex.\n",
    "\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load example document\n",
    "with open(\"elon.txt\", encoding=\"utf-8\") as f:\n",
    "    state_of_the_union = f.read()\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "texts = text_splitter.create_documents([state_of_the_union])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://python.langchain.com/docs/how_to/character_text_splitter/\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# Load example document\n",
    "with open(\"elon.txt\", encoding=\"utf-8\") as f:\n",
    "    state_of_the_union = f.read()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\t\",\n",
    "    chunk_size=10,\n",
    "    chunk_overlap=5,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "texts = text_splitter.create_documents([state_of_the_union])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://python.langchain.com/docs/how_to/split_html/#overview-of-the-splitters\n",
    "#How to: split HTML\n",
    "\n",
    "#Choosing the Right Splitter\n",
    "    #Use HTMLHeaderTextSplitter when:\n",
    "        #You need to split an HTML document based on its header hierarchy and maintain metadata about the headers.\n",
    "    \n",
    "    #Use HTMLSectionSplitter when:\n",
    "        #You need to split the document into larger, more general sections, possibly based on custom tags or font sizes.\n",
    "    \n",
    "    #Use HTMLSemanticPreservingSplitter when: \n",
    "        #You need to split the document into chunks while preserving semantic elements like tables and lists, ensuring that they are not split and that their context is maintained.\n",
    "\n",
    "html_string = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "  <html lang='en'>\n",
    "  <head>\n",
    "    <meta charset='UTF-8'>\n",
    "    <meta name='viewport' content='width=device-width, initial-scale=1.0'>\n",
    "    <title>Fancy Example HTML Page</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <h1>Main Title</h1>\n",
    "    \n",
    "    <h2>Section 1: Introduction</h2>\n",
    "    <p>This section introduces the topic. Below is a list:</p>\n",
    "    \n",
    "    <h3>Subsection 1.1: Details</h3>\n",
    "    <p>This subsection provides additional details. Here's a table:</p>\n",
    "    \n",
    "    <h2>Section 2: Media Content</h2>\n",
    "    \n",
    "\n",
    "    <h2>Section 3: Code Example</h2>\n",
    "  \n",
    "    <h2>Conclusion</h2>\n",
    "  </body>\n",
    "  </html>\n",
    " \"\"\"\n",
    "\n",
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header-1\"),\n",
    "    (\"h2\", \"Header-2\"),\n",
    "    (\"h3\", \"Header-3\"),\n",
    "]\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on)\n",
    "html_header_splits = html_splitter.split_text(html_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method 2\n",
    "\n",
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "url=\"https://python.langchain.com/docs/how_to/split_html/#choosing-the-right-splitter\"\n",
    "\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on)\n",
    "html_header_splits = html_splitter.split_text_from_url(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'openapi': '3.1.0', 'info': {'title': 'LangSmith', 'version': '0.1.0'}, 'paths': {'/api/v1/sessions/{session_id}': {'get': {'tags': ['tracer-sessions'], 'summary': 'Read Tracer Session', 'description': 'Get a specific session.'}}}}\n",
      "{'paths': {'/api/v1/sessions/{session_id}': {'get': {'operationId': 'read_tracer_session_api_v1_sessions__session_id__get', 'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}}}}\n",
      "{'paths': {'/api/v1/sessions/{session_id}': {'get': {'parameters': [{'name': 'session_id', 'in': 'path', 'required': True, 'schema': {'type': 'string', 'format': 'uuid', 'title': 'Session Id'}}, {'name': 'include_stats', 'in': 'query', 'required': False, 'schema': {'type': 'boolean', 'default': False, 'title': 'Include Stats'}}, {'name': 'accept', 'in': 'header', 'required': False, 'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Accept'}}]}}}}\n",
      "page_content='{\"openapi\": \"3.1.0\", \"info\": {\"title\": \"LangSmith\", \"version\": \"0.1.0\"}, \"paths\": {\"/api/v1/sessions/{session_id}\": {\"get\": {\"tags\": [\"tracer-sessions\"], \"summary\": \"Read Tracer Session\", \"description\": \"Get a specific session.\"}}}}'\n",
      "page_content='{\"paths\": {\"/api/v1/sessions/{session_id}\": {\"get\": {\"operationId\": \"read_tracer_session_api_v1_sessions__session_id__get\", \"security\": [{\"API Key\": []}, {\"Tenant ID\": []}, {\"Bearer Auth\": []}]}}}}'\n",
      "page_content='{\"paths\": {\"/api/v1/sessions/{session_id}\": {\"get\": {\"parameters\": [{\"name\": \"session_id\", \"in\": \"path\", \"required\": true, \"schema\": {\"type\": \"string\", \"format\": \"uuid\", \"title\": \"Session Id\"}}, {\"name\": \"include_stats\", \"in\": \"query\", \"required\": false, \"schema\": {\"type\": \"boolean\", \"default\": false, \"title\": \"Include Stats\"}}, {\"name\": \"accept\", \"in\": \"header\", \"required\": false, \"schema\": {\"anyOf\": [{\"type\": \"string\"}, {\"type\": \"null\"}], \"title\": \"Accept\"}}]}}}}'\n"
     ]
    }
   ],
   "source": [
    "#https://python.langchain.com/docs/how_to/recursive_json_splitter/\n",
    "import json\n",
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "import requests\n",
    "\n",
    "# This is a large nested json object and will be loaded as a python dict\n",
    "json_data = requests.get(\"https://api.smith.langchain.com/openapi.json\").json()\n",
    "\n",
    "\n",
    "splitter = RecursiveJsonSplitter(max_chunk_size=300)\n",
    "\n",
    "# Recursively split json data - If you need to access/manipulate the smaller json chunks\n",
    "json_chunks = splitter.split_json(json_data=json_data)\n",
    "\n",
    "#printing top 3 chunks\n",
    "for chunk in json_chunks[:3]:\n",
    "    print(chunk)\n",
    "\n",
    "# The splitter can also output documents\n",
    "docs = splitter.create_documents(texts=[json_data])\n",
    "\n",
    "for doc in docs[:3]:\n",
    "    print(doc)\n",
    "\n",
    "#Or use .split_text to obtain string content directly:\n",
    "texts = splitter.split_text(json_data=json_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.001864203019067645\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "#https://platform.openai.com/docs/guides/embeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "text = \"Hi this is shanmukh\"\n",
    "result = embeddings.embed_query(text)\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.embeddings.Embeddings object at 0x000001E0068AC1F0> async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x000001E00AA1B7F0> model='text-embedding-3-small' dimensions=199 deployment='text-embedding-ada-002' openai_api_version=None openai_api_base=None openai_api_type=None openai_proxy=None embedding_ctx_length=8191 openai_api_key=SecretStr('**********') openai_organization=None allowed_special=None disallowed_special=None chunk_size=1000 max_retries=2 request_timeout=None headers=None tiktoken_enabled=True tiktoken_model_name=None show_progress_bar=False model_kwargs={} skip_empty=False default_headers=None default_query=None retry_min_seconds=4 retry_max_seconds=20 http_client=None http_async_client=None check_embedding_ctx_length=True\n",
      "[-0.00402143644168973, -0.11767087131738663, 0.051633257418870926, -0.00761921564117074, -0.03791734576225281, -0.1177767887711525, 0.08880920708179474, 0.11724721640348434, -0.08690274506807327, -0.05719376355409622, 0.0032684514299035072, -0.08367235958576202, -0.04085646942257881, 0.0028017661534249783, 0.030132640153169632, 0.10263103246688843, -0.010578198358416557, -0.01563560962677002, 0.030953476205468178, 0.0014008830767124891, 0.046681761741638184, -0.041147734969854355, 0.10421974956989288, 0.024148477241396904, 0.005004454404115677, -0.049223706126213074, 0.021699208766222, -0.08817371726036072, -0.00342566822655499, -0.2133115530014038, 0.05269240215420723, -0.05841177701950073, 0.0023913481272757053, 0.02751125954091549, -0.0048290337435901165, -0.030794605612754822, 0.031324177980422974, 0.04117421433329582, -0.07525216788053513, 0.013298873789608479, 0.03291289135813713, -0.0585176944732666, 0.004514600150287151, 0.14192526042461395, 0.04361024498939514, -0.052957188338041306, -0.21670082211494446, -0.060265280306339264, 0.017184607684612274, 0.031774312257766724, 0.010730450041592121, 0.029550110921263695, 0.015768002718687057, 0.1474328190088272, 0.04464291036128998, -0.11968324333429337, 0.08271912485361099, 0.06736154109239578, -0.009624969214200974, 0.03291289135813713, -0.07398119568824768, -0.12497896701097488, -0.01856149360537529, 0.017052214592695236, -0.10533184558153152, -0.07684087753295898, -0.05846473574638367, 0.06476663798093796, -0.05608166381716728, -0.09452857822179794, 0.06105963885784149, 0.03638158738613129, -0.04504008963704109, 0.03217149153351784, -0.013338591903448105, 0.03534892201423645, 0.010333271697163582, -0.021394703537225723, 0.09071566164493561, 0.04583444818854332, -0.07752932608127594, 0.08732639998197556, 0.008214984089136124, -0.0547577328979969, -0.14849194884300232, 0.06857956200838089, -0.20091956853866577, 0.1054377630352974, -0.07218065112829208, 0.003839396173134446, -0.036063846200704575, 0.06868547201156616, -0.08992130309343338, 0.03918831795454025, 0.020322320982813835, 0.03005320392549038, -0.053963374346494675, 0.029841376468539238, 0.09733530879020691, -0.02364538423717022, 0.038288045674562454, 0.03256867080926895, 0.10549072176218033, 0.05624053254723549, -0.006917532533407211, -0.0896565169095993, 0.024426503106951714, -0.05872952193021774, 0.008910046890377998, 0.1063380315899849, -0.2165949046611786, -0.1123751550912857, 0.05666419118642807, 0.09526998549699783, 0.02253328450024128, 0.10713239014148712, 0.16046027839183807, -0.02180512249469757, -0.09384013712406158, -0.002626345492899418, -0.08790893107652664, 0.04726428911089897, 0.014986883848905563, -0.09754714369773865, -0.13366393744945526, -0.05221578851342201, 0.06810294091701508, -0.05422816053032875, -0.15421132743358612, -0.01347760483622551, 0.05740559101104736, -0.07514625042676926, -0.010756928473711014, -0.07202177494764328, -0.017568547278642654, -0.03590497374534607, -0.04890596494078636, 0.06969165802001953, -0.08822667598724365, -0.034289781004190445, -0.0030963406898081303, -0.030847562476992607, -0.0018501917365938425, -0.04559613764286041, 0.09669982641935349, -0.09569364041090012, 0.08743231743574142, 0.0773174986243248, -0.0481380857527256, -0.035507794469594955, 0.014430833980441093, 0.09648799896240234, 0.014669140800833702, 0.029099974781274796, 0.054069288074970245, -0.09034496545791626, 0.007970056496560574, -0.009207931347191334, 0.035746101289987564, 0.09929472953081131, 0.048985399305820465, -0.01926317811012268, 0.007188938092440367, -0.002950708381831646, -0.027802523225545883, 0.09881811589002609, -0.08955060690641403, 0.03148304671049118, -0.0849962905049324, -0.016919821500778198, -0.026690423488616943, -0.012458178214728832, -0.0017326930537819862, -0.07154516130685806, -0.029576590284705162, 0.02793491631746292, 0.10051274299621582, -0.09786488115787506, -0.0594179667532444, 0.0612185113132, 0.0020471264142543077, 0.11618807166814804, 0.01198156364262104, 0.053274933248758316, 0.0035977789666503668, 0.052612967789173126, 0.09060975164175034, -0.017224324867129326, -0.04639049619436264, 0.056399405002593994, 0.009260888211429119, 0.17698292434215546, -0.009711024351418018, 0.018402623012661934, -0.007791326381266117, -0.07890620827674866, 0.10930363833904266, -0.021818362176418304, -0.019276415929198265]\n",
      "199\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "#https://platform.openai.com/docs/guides/embeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\",dimensions=199)\n",
    "print(embeddings)\n",
    "\n",
    "\n",
    "text = \"Hi this is shanmukh\"\n",
    "result = embeddings.embed_query(text)\n",
    "print(result)\n",
    "print(len(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Building an exciting new project with LangChain - come check it out! [{'source': 'tweet'}]\n",
      "* Building an exciting new project with LangChain - come check it out! [{'source': 'tweet'}]\n",
      "* [SIM=0.893613] The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees. [{'source': 'news'}]\n",
      "* I had chocolate chip pancakes and scrambled eggs for breakfast this morning. [{'source': 'tweet'}]\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from uuid import uuid4\n",
    "from langchain_core.documents import Document\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
    ")\n",
    "\n",
    "document_1 = Document(\n",
    "    page_content=\"I had chocolate chip pancakes and scrambled eggs for breakfast this morning.\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=1,\n",
    ")\n",
    "\n",
    "document_2 = Document(\n",
    "    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    "    id=2,\n",
    ")\n",
    "\n",
    "document_3 = Document(\n",
    "    page_content=\"Building an exciting new project with LangChain - come check it out!\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=3,\n",
    ")\n",
    "\n",
    "document_4 = Document(\n",
    "    page_content=\"Robbers broke into the city bank and stole $1 million in cash.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    "    id=4,\n",
    ")\n",
    "\n",
    "document_5 = Document(\n",
    "    page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=5,\n",
    ")\n",
    "\n",
    "document_6 = Document(\n",
    "    page_content=\"Is the new iPhone worth the price? Read this review to find out.\",\n",
    "    metadata={\"source\": \"website\"},\n",
    "    id=6,\n",
    ")\n",
    "\n",
    "document_7 = Document(\n",
    "    page_content=\"The top 10 soccer players in the world right now.\",\n",
    "    metadata={\"source\": \"website\"},\n",
    "    id=7,\n",
    ")\n",
    "\n",
    "document_8 = Document(\n",
    "    page_content=\"LangGraph is the best framework for building stateful, agentic applications!\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=8,\n",
    ")\n",
    "\n",
    "document_9 = Document(\n",
    "    page_content=\"The stock market is down 500 points today due to fears of a recession.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    "    id=9,\n",
    ")\n",
    "\n",
    "document_10 = Document(\n",
    "    page_content=\"I have a bad feeling I am going to get deleted :(\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=10,\n",
    ")\n",
    "\n",
    "documents = [\n",
    "    document_1,\n",
    "    document_2,\n",
    "    document_3,\n",
    "    document_4,\n",
    "    document_5,\n",
    "    document_6,\n",
    "    document_7,\n",
    "    document_8,\n",
    "    document_9,\n",
    "    document_10,\n",
    "]\n",
    "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "\n",
    "vector_store.add_documents(documents=documents, ids=uuids)\n",
    "\n",
    "\n",
    "results = vector_store.similarity_search(\n",
    "    \"LangChain provides abstractions to make working with LLMs easy\",\n",
    "    k=2,\n",
    "    filter={\"source\": \"tweet\"},\n",
    ")\n",
    "for res in results:\n",
    "    print(f\"* {res.page_content} [{res.metadata}]\")\n",
    "\n",
    "\n",
    "\n",
    "#k=1 specifies the number of results that the similarity search should return.\n",
    "results = vector_store.similarity_search_with_score(\n",
    "    \"Will it be hot tomorrow?\", k=1, filter={\"source\": \"news\"}\n",
    ")\n",
    "for res, score in results:\n",
    "    print(f\"* [SIM={score:3f}] {res.page_content} [{res.metadata}]\")\n",
    "\n",
    "#Search by vector\n",
    "results = vector_store.similarity_search_by_vector(\n",
    "    embedding=embeddings.embed_query(\"I love green eggs and ham!\"), k=1\n",
    ")\n",
    "for doc in results:\n",
    "    print(f\"* {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* APJ was born in india [{'source': 'D://GenAI//kalam.txt'}]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "file_path = \"D://GenAI//kalam.txt\"\n",
    "\n",
    "loader = TextLoader(file_path)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "final_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection1\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db1\",  # Where to save data locally, remove if not necessary\n",
    ")\n",
    "\n",
    "vector_store.add_documents(final_docs)\n",
    "\n",
    "results = vector_store.similarity_search(\n",
    "    \"Where is APJ Born\",\n",
    "    k=1,\n",
    ")\n",
    "for res in results:\n",
    "    print(f\"* {res.page_content} [{res.metadata}]\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
