{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document loaders\n",
    "- Document Loaders are responsible for loading documents from a variety of sources.\n",
    "- Reference https://python.langchain.com/docs/how_to/#document-loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert text to vectors\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=os.getenv('OPENAI_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'D://GenAI//kalam.pdf', 'page': 0}, page_content=\"Avul Pakir Jainulabdeen Abdul Kalam BR (/Àà…ôbd äl k…ôÀàl…ëÀêm/ ‚ìò; 15 October 1931 ‚Äì 27 July \\n2015) was an Indian aerospace scientist and statesman who served as the 11th president of \\nIndia from 2002 to 2007. Born and raised in a Muslim family in Rameswaram, Tamil Nadu, \\nhe studied physics and aerospace engineering. He spent the next four decades as a \\nscientist and science administrator, mainly at the Defence Research and Development \\nOrganisation (DRDO) and Indian Space Research Organisation (ISRO) and was intimately \\ninvolved in India's civilian space programme and military missile development efforts.[2] He \\nthus came to be known as the Missile Man of India for his work on the development \\nof ballistic missile and launch vehicle technology.[3][4][5] He also played a pivotal \\norganisational, technical, and political role in India's Pokhran-II nuclear tests in 1998, the first \\nsince the original nuclear test by India in 1974.[6] \")]\n"
     ]
    }
   ],
   "source": [
    "#How to load PDFs\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"D://GenAI//kalam.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(file_path)\n",
    "pages = []\n",
    "for page in loader.load():\n",
    "    pages.append(page)\n",
    "\n",
    "print(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'D://GenAI//kalam.txt'}, page_content='The Defence Research and Development Organisation.\\nDefence Research and Development in Ministry of Defence of the Government of India.The Defence Research and Development Organisation.\\nDefence Research and Development in Ministry of Defence of the Government of India.The Defence Research and Development Organisation.\\nDefence Research and Development in Ministry of Defence of the Government of India.\\nThe Defence Research and Development Organisation.\\nDefence Research and Development in Ministry of Defence of the Government of India.\\nAPJ was born in india')]\n"
     ]
    }
   ],
   "source": [
    "#How to load txt file\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "file_path = \"D://GenAI//kalam.txt\"\n",
    "\n",
    "loader = TextLoader(file_path)\n",
    "pages = []\n",
    "\n",
    "for page in loader.load():\n",
    "    pages.append(page)\n",
    "\n",
    "print(pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Architecture | Guides | Tutorials | How-to guides | Conceptual guide | Integrations | API reference | Ecosystem | ü¶úüõ†Ô∏è LangSmith | ü¶úüï∏Ô∏è LangGraph | Additional resources | Versions | Security | Contributing' metadata={'source': 'https://python.langchain.com/docs/introduction/'}\n"
     ]
    }
   ],
   "source": [
    "#How to load web pages\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "page_url = \"https://python.langchain.com/docs/introduction/\"\n",
    "\n",
    "loader = WebBaseLoader(web_paths=[page_url],\n",
    "                       bs_kwargs={\n",
    "        \"parse_only\": bs4.SoupStrainer(class_=\"table-of-contents__link toc-highlight\"),\n",
    "    },\n",
    "    bs_get_text_kwargs={\"separator\": \" | \", \"strip\": True},)\n",
    "docs = []\n",
    "for doc in loader.load():\n",
    "    docs.append(doc)\n",
    "\n",
    "assert len(docs) == 1\n",
    "doc = docs[0]\n",
    "\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large language models (LLMs) have demonstrated impressive reasoning\n",
      "abilities, but they still strugg\n",
      "{'Entry ID': 'http://arxiv.org/abs/2410.13080v1', 'Published': datetime.date(2024, 10, 16), 'Title': 'Graph-constrained Reasoning: Faithful Reasoning on Knowledge Graphs with Large Language Models', 'Authors': 'Linhao Luo, Zicheng Zhao, Chen Gong, Gholamreza Haffari, Shirui Pan'}\n"
     ]
    }
   ],
   "source": [
    "#https://python.langchain.com/docs/integrations/providers/arxiv/#installation-and-setup\n",
    "#ArxivLoader is a tool used to fetch and load research papers from the arXiv database, which is a popular repository for academic papers in fields like physics, computer science, and mathematics. It allows users to retrieve papers in a structured format, enabling them to process and analyze the content programmatically.\n",
    "#for more data source providers go through this link https://python.langchain.com/docs/integrations/providers/all/\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "loader = ArxivLoader(\n",
    "    query=\"reasoning\"\n",
    ")\n",
    "\n",
    "docs = loader.get_summaries_as_docs()\n",
    "print(docs[0].page_content[:100])\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon Reeve Musk (; born June 28, 1971) is a businessman known for his key roles in the space company\n"
     ]
    }
   ],
   "source": [
    "#load from wikipedia\n",
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "\n",
    "retriever = WikipediaRetriever()\n",
    "docs = retriever.invoke(\"Elon Musk\")\n",
    "print(docs[0].page_content[:100])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation\n",
    "- How to recursively split by characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://python.langchain.com/docs/how_to/recursive_text_splitter/\n",
    "#Let's go through the parameters set above for RecursiveCharacterTextSplitter:\n",
    "\n",
    "#chunk_size: The maximum size of a chunk, where size is determined by the length_function.\n",
    "#chunk_overlap: Target overlap between chunks. Overlapping chunks helps to mitigate loss of information when context is divided between chunks.\n",
    "#length_function: Function determining the chunk size.\n",
    "#is_separator_regex: Whether the separator list (defaulting to [\"\\n\\n\", \"\\n\", \" \", \"\"]) should be interpreted as regex.\n",
    "\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load example document\n",
    "with open(\"elon.txt\", encoding=\"utf-8\") as f:\n",
    "    state_of_the_union = f.read()\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "texts = text_splitter.create_documents([state_of_the_union])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://python.langchain.com/docs/how_to/character_text_splitter/\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# Load example document\n",
    "with open(\"elon.txt\", encoding=\"utf-8\") as f:\n",
    "    state_of_the_union = f.read()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\t\",\n",
    "    chunk_size=10,\n",
    "    chunk_overlap=5,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "texts = text_splitter.create_documents([state_of_the_union])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header-1': 'Main Title', 'Header-2': 'Section 1: Introduction'}, page_content='This section introduces the topic. Below is a list:'),\n",
       " Document(metadata={'Header-1': 'Main Title', 'Header-2': 'Section 1: Introduction', 'Header-3': 'Subsection 1.1: Details'}, page_content=\"This subsection provides additional details. Here's a table:\")]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://python.langchain.com/docs/how_to/split_html/#overview-of-the-splitters\n",
    "#How to: split HTML\n",
    "\n",
    "#Choosing the Right Splitter\n",
    "    #Use HTMLHeaderTextSplitter when:\n",
    "        #You need to split an HTML document based on its header hierarchy and maintain metadata about the headers.\n",
    "    \n",
    "    #Use HTMLSectionSplitter when:\n",
    "        #You need to split the document into larger, more general sections, possibly based on custom tags or font sizes.\n",
    "    \n",
    "    #Use HTMLSemanticPreservingSplitter when: \n",
    "        #You need to split the document into chunks while preserving semantic elements like tables and lists, ensuring that they are not split and that their context is maintained.\n",
    "\n",
    "html_string = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "  <html lang='en'>\n",
    "  <head>\n",
    "    <meta charset='UTF-8'>\n",
    "    <meta name='viewport' content='width=device-width, initial-scale=1.0'>\n",
    "    <title>Fancy Example HTML Page</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <h1>Main Title</h1>\n",
    "    \n",
    "    <h2>Section 1: Introduction</h2>\n",
    "    <p>This section introduces the topic. Below is a list:</p>\n",
    "    \n",
    "    <h3>Subsection 1.1: Details</h3>\n",
    "    <p>This subsection provides additional details. Here's a table:</p>\n",
    "    \n",
    "    <h2>Section 2: Media Content</h2>\n",
    "    \n",
    "\n",
    "    <h2>Section 3: Code Example</h2>\n",
    "  \n",
    "    <h2>Conclusion</h2>\n",
    "  </body>\n",
    "  </html>\n",
    " \"\"\"\n",
    "\n",
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header-1\"),\n",
    "    (\"h2\", \"Header-2\"),\n",
    "    (\"h3\", \"Header-3\"),\n",
    "]\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on)\n",
    "html_header_splits = html_splitter.split_text(html_string)\n",
    "html_header_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={}, page_content='Skip to main content  \\nIntegrationsAPI Reference  \\nMore  \\nContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TS  \\nüí¨  \\nv0.3  \\nv0.3v0.2v0.1  \\nSearch  \\nIntroductionSecurity Policy  \\nTutorials  \\nBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize Text  \\nHow-to guides  \\nHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector stores  \\nConceptual guide  \\nAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?  \\nEcosystem  \\nü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraph  \\nVersions  \\nv0.3Pydantic compatibilityRelease policy  \\nv0.2  \\nMigrating from v0.0 chains  \\nHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChain  \\nUpgrading to LangGraph memory  \\nHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory Agent  \\nHow-to guidesHow to split HTML  \\nOn this page  \\nHow to split HTML Overview of the Splitters\\u200b HTMLHeaderTextSplitter\\u200b HTMLSectionSplitter\\u200b HTMLSemanticPreservingSplitter\\u200b Choosing the Right Splitter\\u200b FeatureHTMLHeaderTextSplitterHTMLSectionSplitterHTMLSemanticPreservingSplitterSplits based on headersYesYesYesPreserves semantic elements (tables, lists)NoNoYesAdds metadata for headersYesYesYesCustom handlers for HTML tagsNoNoYesPreserves media (images, videos)NoNoYesConsiders font sizesNoYesNoUses XSLT transformationsNoYesNo Example HTML Document\\u200b Using HTMLHeaderTextSplitter\\u200b How to split from a URL or HTML file:\\u200b How to constrain chunk sizes:\\u200b Limitations\\u200b Using HTMLSectionSplitter\\u200b How to split HTML strings:\\u200b How to constrain chunk sizes:\\u200b Using HTMLSemanticPreservingSplitter\\u200b Preserving Tables and Lists\\u200b Explanation\\u200b Using a Custom Handler\\u200b Explanation\\u200b Using a custom handler to analyze an image with an LLM\\u200b Explanation:\\u200b'), Document(metadata={'Header 1': 'How to split HTML'}, page_content='Splitting HTML documents into manageable chunks is essential for various text processing tasks such as natural language processing, search indexing, and more. In this guide, we will explore three different text splitters provided by LangChain that you can use to split HTML content effectively:  \\nHTMLHeaderTextSplitter HTMLSectionSplitter HTMLSemanticPreservingSplitter  \\nEach of these splitters has unique features and use cases. This guide will help you understand the differences between them, why you might choose one over the others, and how to use them effectively.  \\n%pip install -qU langchain-text-splitters'), Document(metadata={'Header 1': 'How to split HTML', 'Header 2': 'Overview of the Splitters\\u200b', 'Header 3': 'HTMLHeaderTextSplitter\\u200b'}, page_content='info  \\nUseful when you want to preserve the hierarchical structure of a document based on its headings.  \\nDescription: Splits HTML text based on header tags (e.g., <h1>, <h2>, <h3>, etc.), and adds metadata for each header relevant to any given chunk.  \\nCapabilities:  \\nSplits text at the HTML element level. Preserves context-rich information encoded in document structures. Can return chunks element by element or combine elements with the same metadata.'), Document(metadata={'Header 1': 'How to split HTML', 'Header 2': 'Overview of the Splitters\\u200b', 'Header 3': 'HTMLSectionSplitter\\u200b'}, page_content='info  \\nUseful when you want to split HTML documents into larger sections, such as <section>, <div>, or custom-defined sections.  \\nDescription: Similar to HTMLHeaderTextSplitter but focuses on splitting HTML into sections based on specified tags.  \\nCapabilities:  \\nUses XSLT transformations to detect and split sections. Internally uses RecursiveCharacterTextSplitter for large sections. Considers font sizes to determine sections.'), Document(metadata={'Header 1': 'How to split HTML', 'Header 2': 'Overview of the Splitters\\u200b', 'Header 3': 'HTMLSemanticPreservingSplitter\\u200b'}, page_content='info  \\nIdeal when you need to ensure that structured elements are not split across chunks, preserving contextual relevancy.  \\nDescription: Splits HTML content into manageable chunks while preserving the semantic structure of important elements like tables, lists, and other HTML components.  \\nCapabilities:  \\nPreserves tables, lists, and other specified HTML elements. Allows custom handlers for specific HTML tags. Ensures that the semantic meaning of the document is maintained. Built in normalization & stopword removal'), Document(metadata={'Header 1': 'How to split HTML', 'Header 2': 'Overview of the Splitters\\u200b', 'Header 3': 'Choosing the Right Splitter\\u200b'}, page_content='Use HTMLHeaderTextSplitter when: You need to split an HTML document based on its header hierarchy and maintain metadata about the headers. Use HTMLSectionSplitter when: You need to split the document into larger, more general sections, possibly based on custom tags or font sizes. Use HTMLSemanticPreservingSplitter when: You need to split the document into chunks while preserving semantic elements like tables and lists, ensuring that they are not split and that their context is maintained.'), Document(metadata={'Header 1': 'How to split HTML', 'Header 2': 'Example HTML Document\\u200b'}, page_content='Let\\'s use the following HTML document as an example:  \\nhtml_string = \"\"\"<!DOCTYPE html> <html lang=\\'en\\'> <head> <meta charset=\\'UTF-8\\'> <meta name=\\'viewport\\' content=\\'width=device-width, initial-scale=1.0\\'> <title>Fancy Example HTML Page</title> </head> <body> <h1>Main Title</h1> <p>This is an introductory paragraph with some basic content.</p> <h2>Section 1: Introduction</h2> <p>This section introduces the topic. Below is a list:</p> <ul> <li>First item</li> <li>Second item</li> <li>Third item with <strong>bold text</strong> and <a href=\\'#\\'>a link</a></li> </ul> <h3>Subsection 1.1: Details</h3> <p>This subsection provides additional details. Here\\'s a table:</p> <table border=\\'1\\'> <thead> <tr> <th>Header 1</th> <th>Header 2</th> <th>Header 3</th> </tr> </thead> <tbody> <tr> <td>Row 1, Cell 1</td> <td>Row 1, Cell 2</td> <td>Row 1, Cell 3</td> </tr> <tr> <td>Row 2, Cell 1</td> <td>Row 2, Cell 2</td> <td>Row 2, Cell 3</td> </tr> </tbody> </table> <h2>Section 2: Media Content</h2> <p>This section contains an image and a video:</p> <img src=\\'example_image_link.mp4\\' alt=\\'Example Image\\'> <video controls width=\\'250\\' src=\\'example_video_link.mp4\\' type=\\'video/mp4\\'> Your browser does not support the video tag. </video> <h2>Section 3: Code Example</h2> <p>This section contains a code block:</p> <pre><code data-lang=\"html\"> &lt;div&gt; &lt;p&gt;This is a paragraph inside a div.&lt;/p&gt; &lt;/div&gt; </code></pre> <h2>Conclusion</h2> <p>This is the conclusion of the document.</p> </body> </html>\"\"\"'), Document(metadata={'Header 1': 'How to split HTML', 'Header 2': 'Using HTMLHeaderTextSplitter\\u200b'}, page_content='HTMLHeaderTextSplitter is a \"structure-aware\" text splitter that splits text at the HTML element level and adds metadata for each header \"relevant\" to any given chunk. It can return chunks element by element or combine elements with the same metadata, with the objectives of (a) keeping related text grouped (more or less) semantically and (b) preserving context-rich information encoded in document structures. It can be used with other text splitters as part of a chunking pipeline.  \\nIt is analogous to the MarkdownHeaderTextSplitter for markdown files.  \\nTo specify what headers to split on, specify headers_to_split_on when instantiating HTMLHeaderTextSplitter as shown below.  \\nfrom langchain_text_splitters import HTMLHeaderTextSplitterheaders_to_split_on = [ (\"h1\", \"Header 1\"), (\"h2\", \"Header 2\"), (\"h3\", \"Header 3\"),]html_splitter = HTMLHeaderTextSplitter(headers_to_split_on)html_header_splits = html_splitter.split_text(html_string)html_header_splits  \\nAPI Reference:HTMLHeaderTextSplitter  \\n[Document(metadata={\\'Header 1\\': \\'Main Title\\'}, page_content=\\'This is an introductory paragraph with some basic content.\\'), Document(metadata={\\'Header 1\\': \\'Main Title\\', \\'Header 2\\': \\'Section 1: Introduction\\'}, page_content=\\'This section introduces the topic. Below is a list: \\\\nFirst item Second item Third item with bold text and a link\\'), Document(metadata={\\'Header 1\\': \\'Main Title\\', \\'Header 2\\': \\'Section 1: Introduction\\', \\'Header 3\\': \\'Subsection 1.1: Details\\'}, page_content=\"This subsection provides additional details. Here\\'s a table:\"), Document(metadata={\\'Header 1\\': \\'Main Title\\', \\'Header 2\\': \\'Section 2: Media Content\\'}, page_content=\\'This section contains an image and a video:\\'), Document(metadata={\\'Header 1\\': \\'Main Title\\', \\'Header 2\\': \\'Section 3: Code Example\\'}, page_content=\\'This section contains a code block:\\'), Document(metadata={\\'Header 1\\': \\'Main Title\\', \\'Header 2\\': \\'Conclusion\\'}, page_content=\\'This is the conclusion of the document.\\')]  \\nTo return each element together with their associated headers, specify return_each_element=True when instantiating HTMLHeaderTextSplitter:  \\nhtml_splitter = HTMLHeaderTextSplitter( headers_to_split_on, return_each_element=True,)html_header_splits_elements = html_splitter.split_text(html_string)  \\nComparing with the above, where elements are aggregated by their headers:  \\nfor element in html_header_splits[:2]: print(element)  \\npage_content=\\'This is an introductory paragraph with some basic content.\\' metadata={\\'Header 1\\': \\'Main Title\\'}page_content=\\'This section introduces the topic. Below is a list: First item Second item Third item with bold text and a link\\' metadata={\\'Header 1\\': \\'Main Title\\', \\'Header 2\\': \\'Section 1: Introduction\\'}  \\nNow each element is returned as a distinct Document:  \\nfor element in html_header_splits_elements[:3]: print(element)  \\npage_content=\\'This is an introductory paragraph with some basic content.\\' metadata={\\'Header 1\\': \\'Main Title\\'}page_content=\\'This section introduces the topic. Below is a list:\\' metadata={\\'Header 1\\': \\'Main Title\\', \\'Header 2\\': \\'Section 1: Introduction\\'}page_content=\\'First item Second item Third item with bold text and a link\\' metadata={\\'Header 1\\': \\'Main Title\\', \\'Header 2\\': \\'Section 1: Introduction\\'}'), Document(metadata={'Header 1': 'How to split HTML', 'Header 2': 'Using HTMLHeaderTextSplitter\\u200b', 'Header 3': 'How to split from a URL or HTML file:\\u200b'}, page_content='To read directly from a URL, pass the URL string into the split_text_from_url method.  \\nSimilarly, a local HTML file can be passed to the split_text_from_file method.  \\nurl = \"https://plato.stanford.edu/entries/goedel/\"headers_to_split_on = [ (\"h1\", \"Header 1\"), (\"h2\", \"Header 2\"), (\"h3\", \"Header 3\"), (\"h4\", \"Header 4\"),]html_splitter = HTMLHeaderTextSplitter(headers_to_split_on)# for local file use html_splitter.split_text_from_file(<path_to_file>)html_header_splits = html_splitter.split_text_from_url(url)'), Document(metadata={'Header 1': 'How to split HTML', 'Header 2': 'Using HTMLHeaderTextSplitter\\u200b', 'Header 3': 'How to constrain chunk sizes:\\u200b'}, page_content=\"HTMLHeaderTextSplitter, which splits based on HTML headers, can be composed with another splitter which constrains splits based on character lengths, such as RecursiveCharacterTextSplitter.  \\nThis can be done using the .split_documents method of the second splitter:  \\nfrom langchain_text_splitters import RecursiveCharacterTextSplitterchunk_size = 500chunk_overlap = 30text_splitter = RecursiveCharacterTextSplitter( chunk_size=chunk_size, chunk_overlap=chunk_overlap)# Splitsplits = text_splitter.split_documents(html_header_splits)splits[80:85]  \\nAPI Reference:RecursiveCharacterTextSplitter  \\n[Document(metadata={'Header 1': 'Kurt G√∂del', 'Header 2': '2. G√∂del‚Äôs Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}, page_content='We see that G√∂del first tried to reduce the consistency problem for analysis to that of arithmetic. This seemed to require a truth definition for arithmetic, which in turn led to paradoxes, such as the Liar paradox (‚ÄúThis sentence is false‚Äù) and Berry‚Äôs paradox (‚ÄúThe least number not defined by an expression consisting of just fourteen English words‚Äù). G√∂del then noticed that such paradoxes would not necessarily arise if truth were replaced by provability. But this means that arithmetic truth'), Document(metadata={'Header 1': 'Kurt G√∂del', 'Header 2': '2. G√∂del‚Äôs Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}, page_content='means that arithmetic truth and arithmetic provability are not co-extensive ‚Äî whence the First Incompleteness Theorem.'), Document(metadata={'Header 1': 'Kurt G√∂del', 'Header 2': '2. G√∂del‚Äôs Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}, page_content='This account of G√∂del‚Äôs discovery was told to Hao Wang very much after the fact; but in G√∂del‚Äôs contemporary correspondence with Bernays and Zermelo, essentially the same description of his path to the theorems is given. (See G√∂del 2003a and G√∂del 2003b respectively.) From those accounts we see that the undefinability of truth in arithmetic, a result credited to Tarski, was likely obtained in some form by G√∂del by 1931. But he neither publicized nor published the result; the biases logicians'), Document(metadata={'Header 1': 'Kurt G√∂del', 'Header 2': '2. G√∂del‚Äôs Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}, page_content='result; the biases logicians had expressed at the time concerning the notion of truth, biases which came vehemently to the fore when Tarski announced his results on the undefinability of truth in formal systems 1935, may have served as a deterrent to G√∂del‚Äôs publication of that theorem.'), Document(metadata={'Header 1': 'Kurt G√∂del', 'Header 2': '2. G√∂del‚Äôs Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.2 The proof of the First Incompleteness Theorem'}, page_content='We now describe the proof of the two theorems, formulating G√∂del‚Äôs results in Peano arithmetic. G√∂del himself used a system related to that defined in Principia Mathematica, but containing Peano arithmetic. In our presentation of the First and Second Incompleteness Theorems we refer to Peano arithmetic as P, following G√∂del‚Äôs notation.')]\"), Document(metadata={'Header 1': 'How to split HTML', 'Header 2': 'Using HTMLHeaderTextSplitter\\u200b', 'Header 3': 'Limitations\\u200b'}, page_content='There can be quite a bit of structural variation from one HTML document to another, and while HTMLHeaderTextSplitter will attempt to attach all \"relevant\" headers to any given chunk, it can sometimes miss certain headers. For example, the algorithm assumes an informational hierarchy in which headers are always at nodes \"above\" associated text, i.e. prior siblings, ancestors, and combinations thereof. In the following news article (as of the writing of this document), the document is structured such that the text of the top-level headline, while tagged \"h1\", is in a distinct subtree from the text elements that we\\'d expect it to be \"above\"‚Äîso we can observe that the \"h1\" element and its associated text do not show up in the chunk metadata (but, where applicable, we do see \"h2\" and its associated text):  \\nurl = \"https://www.cnn.com/2023/09/25/weather/el-nino-winter-us-climate/index.html\"headers_to_split_on = [ (\"h1\", \"Header 1\"), (\"h2\", \"Header 2\"),]html_splitter = HTMLHeaderTextSplitter(headers_to_split_on)html_header_splits = html_splitter.split_text_from_url(url)print(html_header_splits[1].page_content[:500])  \\nNo two El Ni√±o winters are the same, but many have temperature and precipitation trends in common. Average conditions during an El Ni√±o winter across the continental US. One of the major reasons is the position of the jet stream, which often shifts south during an El Ni√±o winter. This shift typically brings wetter and cooler weather to the South while the North becomes drier and warmer, according to NOAA. Because the jet stream is essentially a river of air that storms flow through, they c'), Document(metadata={'Header 1': 'How to split HTML', 'Header 2': 'Using HTMLSectionSplitter\\u200b'}, page_content='Similar in concept to the HTMLHeaderTextSplitter, the HTMLSectionSplitter is a \"structure-aware\" text splitter that splits text at the element level and adds metadata for each header \"relevant\" to any given chunk. It lets you split HTML by sections.  \\nIt can return chunks element by element or combine elements with the same metadata, with the objectives of (a) keeping related text grouped (more or less) semantically and (b) preserving context-rich information encoded in document structures.  \\nUse xslt_path to provide an absolute path to transform the HTML so that it can detect sections based on provided tags. The default is to use the converting_to_header.xslt file in the data_connection/document_transformers directory. This is for converting the html to a format/layout that is easier to detect sections. For example, span based on their font size can be converted to header tags to be detected as a section.'), Document(metadata={'Header 1': 'How to split HTML', 'Header 2': 'Using HTMLSectionSplitter\\u200b', 'Header 3': 'How to split HTML strings:\\u200b'}, page_content='from langchain_text_splitters import HTMLSectionSplitterheaders_to_split_on = [ (\"h1\", \"Header 1\"), (\"h2\", \"Header 2\"),]html_splitter = HTMLSectionSplitter(headers_to_split_on)html_header_splits = html_splitter.split_text(html_string)html_header_splits  \\nAPI Reference:HTMLSectionSplitter  \\n[Document(metadata={\\'Header 1\\': \\'Main Title\\'}, page_content=\\'Main Title \\\\n This is an introductory paragraph with some basic content.\\'), Document(metadata={\\'Header 2\\': \\'Section 1: Introduction\\'}, page_content=\"Section 1: Introduction \\\\n This section introduces the topic. Below is a list: \\\\n \\\\n First item \\\\n Second item \\\\n Third item with bold text and a link \\\\n \\\\n \\\\n Subsection 1.1: Details \\\\n This subsection provides additional details. Here\\'s a table: \\\\n \\\\n \\\\n \\\\n Header 1 \\\\n Header 2 \\\\n Header 3 \\\\n \\\\n \\\\n \\\\n \\\\n Row 1, Cell 1 \\\\n Row 1, Cell 2 \\\\n Row 1, Cell 3 \\\\n \\\\n \\\\n Row 2, Cell 1 \\\\n Row 2, Cell 2 \\\\n Row 2, Cell 3\"), Document(metadata={\\'Header 2\\': \\'Section 2: Media Content\\'}, page_content=\\'Section 2: Media Content \\\\n This section contains an image and a video: \\\\n \\\\n \\\\n Your browser does not support the video tag.\\'), Document(metadata={\\'Header 2\\': \\'Section 3: Code Example\\'}, page_content=\\'Section 3: Code Example \\\\n This section contains a code block: \\\\n \\\\n <div>\\\\n <p>This is a paragraph inside a div.</p>\\\\n </div>\\'), Document(metadata={\\'Header 2\\': \\'Conclusion\\'}, page_content=\\'Conclusion \\\\n This is the conclusion of the document.\\')]'), Document(metadata={'Header 1': 'How to split HTML', 'Header 2': 'Using HTMLSectionSplitter\\u200b', 'Header 3': 'How to constrain chunk sizes:\\u200b'}, page_content='HTMLSectionSplitter can be used with other text splitters as part of a chunking pipeline. Internally, it uses the RecursiveCharacterTextSplitter when the section size is larger than the chunk size. It also considers the font size of the text to determine whether it is a section or not based on the determined font size threshold.  \\nfrom langchain_text_splitters import RecursiveCharacterTextSplitterheaders_to_split_on = [ (\"h1\", \"Header 1\"), (\"h2\", \"Header 2\"), (\"h3\", \"Header 3\"),]html_splitter = HTMLSectionSplitter(headers_to_split_on)html_header_splits = html_splitter.split_text(html_string)chunk_size = 50chunk_overlap = 5text_splitter = RecursiveCharacterTextSplitter( chunk_size=chunk_size, chunk_overlap=chunk_overlap)# Splitsplits = text_splitter.split_documents(html_header_splits)splits  \\nAPI Reference:RecursiveCharacterTextSplitter  \\n[Document(metadata={\\'Header 1\\': \\'Main Title\\'}, page_content=\\'Main Title\\'), Document(metadata={\\'Header 1\\': \\'Main Title\\'}, page_content=\\'This is an introductory paragraph with some\\'), Document(metadata={\\'Header 1\\': \\'Main Title\\'}, page_content=\\'some basic content.\\'), Document(metadata={\\'Header 2\\': \\'Section 1: Introduction\\'}, page_content=\\'Section 1: Introduction\\'), Document(metadata={\\'Header 2\\': \\'Section 1: Introduction\\'}, page_content=\\'This section introduces the topic. Below is a\\'), Document(metadata={\\'Header 2\\': \\'Section 1: Introduction\\'}, page_content=\\'is a list:\\'), Document(metadata={\\'Header 2\\': \\'Section 1: Introduction\\'}, page_content=\\'First item \\\\n Second item\\'), Document(metadata={\\'Header 2\\': \\'Section 1: Introduction\\'}, page_content=\\'Third item with bold text and a link\\'), Document(metadata={\\'Header 3\\': \\'Subsection 1.1: Details\\'}, page_content=\\'Subsection 1.1: Details\\'), Document(metadata={\\'Header 3\\': \\'Subsection 1.1: Details\\'}, page_content=\\'This subsection provides additional details.\\'), Document(metadata={\\'Header 3\\': \\'Subsection 1.1: Details\\'}, page_content=\"Here\\'s a table:\"), Document(metadata={\\'Header 3\\': \\'Subsection 1.1: Details\\'}, page_content=\\'Header 1 \\\\n Header 2 \\\\n Header 3\\'), Document(metadata={\\'Header 3\\': \\'Subsection 1.1: Details\\'}, page_content=\\'Row 1, Cell 1 \\\\n Row 1, Cell 2\\'), Document(metadata={\\'Header 3\\': \\'Subsection 1.1: Details\\'}, page_content=\\'Row 1, Cell 3 \\\\n \\\\n \\\\n Row 2, Cell 1\\'), Document(metadata={\\'Header 3\\': \\'Subsection 1.1: Details\\'}, page_content=\\'Row 2, Cell 2 \\\\n Row 2, Cell 3\\'), Document(metadata={\\'Header 2\\': \\'Section 2: Media Content\\'}, page_content=\\'Section 2: Media Content\\'), Document(metadata={\\'Header 2\\': \\'Section 2: Media Content\\'}, page_content=\\'This section contains an image and a video:\\'), Document(metadata={\\'Header 2\\': \\'Section 2: Media Content\\'}, page_content=\\'Your browser does not support the video\\'), Document(metadata={\\'Header 2\\': \\'Section 2: Media Content\\'}, page_content=\\'tag.\\'), Document(metadata={\\'Header 2\\': \\'Section 3: Code Example\\'}, page_content=\\'Section 3: Code Example\\'), Document(metadata={\\'Header 2\\': \\'Section 3: Code Example\\'}, page_content=\\'This section contains a code block: \\\\n \\\\n <div>\\'), Document(metadata={\\'Header 2\\': \\'Section 3: Code Example\\'}, page_content=\\'<p>This is a paragraph inside a div.</p>\\'), Document(metadata={\\'Header 2\\': \\'Section 3: Code Example\\'}, page_content=\\'</div>\\'), Document(metadata={\\'Header 2\\': \\'Conclusion\\'}, page_content=\\'Conclusion\\'), Document(metadata={\\'Header 2\\': \\'Conclusion\\'}, page_content=\\'This is the conclusion of the document.\\')]'), Document(metadata={'Header 1': 'How to split HTML', 'Header 2': 'Using HTMLSemanticPreservingSplitter\\u200b'}, page_content='The HTMLSemanticPreservingSplitter is designed to split HTML content into manageable chunks while preserving the semantic structure of important elements like tables, lists, and other HTML components. This ensures that such elements are not split across chunks, causing loss of contextual relevancy such as table headers, list headers etc.  \\nThis splitter is designed at its heart, to create contextually relevant chunks. General Recursive splitting with HTMLHeaderTextSplitter can cause tables, lists and other structered elements to be split in the middle, losing signifcant context and creating bad chunks.  \\nThe HTMLSemanticPreservingSplitter is essential for splitting HTML content that includes structured elements like tables and lists, especially when it\\'s critical to preserve these elements intact. Additionally, its ability to define custom handlers for specific HTML tags makes it a versatile tool for processing complex HTML documents.  \\nIMPORTANT: max_chunk_size is not a definite maximum size of a chunk, the calculation of max size, occurs when the preserved content is not apart of the chunk, to ensure it is not split. When we add the preserved data back in to the chunk, there is a chance the chunk size will exceed the max_chunk_size. This is crucial to ensure we maintain the structure of the original document  \\ninfo  \\nNotes:  \\nWe have defined a custom handler to re-format the contents of code blocks We defined a deny list for specific html elements, to decompose them and their contents pre-processing We have intentionally set a small chunk size to demonstrate the non-splitting of elements  \\n# BeautifulSoup is required to use the custom handlersfrom bs4 import Tagfrom langchain_text_splitters import HTMLSemanticPreservingSplitterheaders_to_split_on = [ (\"h1\", \"Header 1\"), (\"h2\", \"Header 2\"),]def code_handler(element: Tag) -> str: data_lang = element.get(\"data-lang\") code_format = f\"<code:{data_lang}>{element.get_text()}</code>\" return code_formatsplitter = HTMLSemanticPreservingSplitter( headers_to_split_on=headers_to_split_on, separators=[\"\\\\n\\\\n\", \"\\\\n\", \". \", \"! \", \"? \"], max_chunk_size=50, preserve_images=True, preserve_videos=True, elements_to_preserve=[\"table\", \"ul\", \"ol\", \"code\"], denylist_tags=[\"script\", \"style\", \"head\"], custom_handlers={\"code\": code_handler},)documents = splitter.split_text(html_string)documents  \\nAPI Reference:HTMLSemanticPreservingSplitter  \\n[Document(metadata={\\'Header 1\\': \\'Main Title\\'}, page_content=\\'This is an introductory paragraph with some basic content.\\'), Document(metadata={\\'Header 2\\': \\'Section 1: Introduction\\'}, page_content=\\'This section introduces the topic\\'), Document(metadata={\\'Header 2\\': \\'Section 1: Introduction\\'}, page_content=\\'. Below is a list: First item Second item Third item with bold text and a link Subsection 1.1: Details This subsection provides additional details\\'), Document(metadata={\\'Header 2\\': \\'Section 1: Introduction\\'}, page_content=\". Here\\'s a table: Header 1 Header 2 Header 3 Row 1, Cell 1 Row 1, Cell 2 Row 1, Cell 3 Row 2, Cell 1 Row 2, Cell 2 Row 2, Cell 3\"), Document(metadata={\\'Header 2\\': \\'Section 2: Media Content\\'}, page_content=\\'This section contains an image and a video: ![image:example_image_link.mp4](example_image_link.mp4) ![video:example_video_link.mp4](example_video_link.mp4)\\'), Document(metadata={\\'Header 2\\': \\'Section 3: Code Example\\'}, page_content=\\'This section contains a code block: <code:html> <div> <p>This is a paragraph inside a div.</p> </div> </code>\\'), Document(metadata={\\'Header 2\\': \\'Conclusion\\'}, page_content=\\'This is the conclusion of the document.\\')]'), Document(metadata={'Header 1': 'How to split HTML', 'Header 2': 'Using HTMLSemanticPreservingSplitter\\u200b', 'Header 3': 'Preserving Tables and Lists\\u200b'}, page_content='In this example, we will demonstrate how the HTMLSemanticPreservingSplitter can preserve a table and a large list within an HTML document. The chunk size will be set to 50 characters to illustrate how the splitter ensures that these elements are not split, even when they exceed the maximum defined chunk size.  \\nfrom langchain_text_splitters import HTMLSemanticPreservingSplitterhtml_string = \"\"\"<!DOCTYPE html><html> <body> <div> <h1>Section 1</h1> <p>This section contains an important table and list that should not be split across chunks.</p> <table> <tr> <th>Item</th> <th>Quantity</th> <th>Price</th> </tr> <tr> <td>Apples</td> <td>10</td> <td>$1.00</td> </tr> <tr> <td>Oranges</td> <td>5</td> <td>$0.50</td> </tr> <tr> <td>Bananas</td> <td>50</td> <td>$1.50</td> </tr> </table> <h2>Subsection 1.1</h2> <p>Additional text in subsection 1.1 that is separated from the table and list.</p> <p>Here is a detailed list:</p> <ul> <li>Item 1: Description of item 1, which is quite detailed and important.</li> <li>Item 2: Description of item 2, which also contains significant information.</li> <li>Item 3: Description of item 3, another item that we don\\'t want to split across chunks.</li> </ul> </div> </body></html>\"\"\"headers_to_split_on = [(\"h1\", \"Header 1\"), (\"h2\", \"Header 2\")]splitter = HTMLSemanticPreservingSplitter( headers_to_split_on=headers_to_split_on, max_chunk_size=50, elements_to_preserve=[\"table\", \"ul\"],)documents = splitter.split_text(html_string)print(documents)  \\nAPI Reference:HTMLSemanticPreservingSplitter  \\n[Document(metadata={\\'Header 1\\': \\'Section 1\\'}, page_content=\\'This section contains an important table and list\\'), Document(metadata={\\'Header 1\\': \\'Section 1\\'}, page_content=\\'that should not be split across chunks.\\'), Document(metadata={\\'Header 1\\': \\'Section 1\\'}, page_content=\\'Item Quantity Price Apples 10 $1.00 Oranges 5 $0.50 Bananas 50 $1.50\\'), Document(metadata={\\'Header 2\\': \\'Subsection 1.1\\'}, page_content=\\'Additional text in subsection 1.1 that is\\'), Document(metadata={\\'Header 2\\': \\'Subsection 1.1\\'}, page_content=\\'separated from the table and list. Here is a\\'), Document(metadata={\\'Header 2\\': \\'Subsection 1.1\\'}, page_content=\"detailed list: Item 1: Description of item 1, which is quite detailed and important. Item 2: Description of item 2, which also contains significant information. Item 3: Description of item 3, another item that we don\\'t want to split across chunks.\")]  \\nIn this example, the HTMLSemanticPreservingSplitter ensures that the entire table and the unordered list (<ul>) are preserved within their respective chunks. Even though the chunk size is set to 50 characters, the splitter recognizes that these elements should not be split and keeps them intact.  \\nThis is particularly important when dealing with data tables or lists, where splitting the content could lead to loss of context or confusion. The resulting Document objects retain the full structure of these elements, ensuring that the contextual relevance of the information is maintained.'), Document(metadata={'Header 1': 'How to split HTML', 'Header 2': 'Using HTMLSemanticPreservingSplitter\\u200b', 'Header 3': 'Using a Custom Handler\\u200b'}, page_content='The HTMLSemanticPreservingSplitter allows you to define custom handlers for specific HTML elements. Some platforms, have custom HTML tags that are not natively parsed by BeautifulSoup, when this occurs, you can utilize custom handlers to add the formatting logic easily.  \\nThis can be particularly useful for elements that require special processing, such as <iframe> tags or specific \\'data-\\' elements. In this example, we\\'ll create a custom handler for iframe tags that converts them into Markdown-like links.  \\ndef custom_iframe_extractor(iframe_tag): iframe_src = iframe_tag.get(\"src\", \"\") return f\"[iframe:{iframe_src}]({iframe_src})\"splitter = HTMLSemanticPreservingSplitter( headers_to_split_on=headers_to_split_on, max_chunk_size=50, separators=[\"\\\\n\\\\n\", \"\\\\n\", \". \"], elements_to_preserve=[\"table\", \"ul\", \"ol\"], custom_handlers={\"iframe\": custom_iframe_extractor},)html_string = \"\"\"<!DOCTYPE html><html> <body> <div> <h1>Section with Iframe</h1> <iframe src=\"https://example.com/embed\"></iframe> <p>Some text after the iframe.</p> <ul> <li>Item 1: Description of item 1, which is quite detailed and important.</li> <li>Item 2: Description of item 2, which also contains significant information.</li> <li>Item 3: Description of item 3, another item that we don\\'t want to split across chunks.</li> </ul> </div> </body></html>\"\"\"documents = splitter.split_text(html_string)print(documents)  \\n[Document(metadata={\\'Header 1\\': \\'Section with Iframe\\'}, page_content=\\'[iframe:https://example.com/embed](https://example.com/embed) Some text after the iframe\\'), Document(metadata={\\'Header 1\\': \\'Section with Iframe\\'}, page_content=\". Item 1: Description of item 1, which is quite detailed and important. Item 2: Description of item 2, which also contains significant information. Item 3: Description of item 3, another item that we don\\'t want to split across chunks.\")]  \\nIn this example, we defined a custom handler for iframe tags that converts them into Markdown-like links. When the splitter processes the HTML content, it uses this custom handler to transform the iframe tags while preserving other elements like tables and lists. The resulting Document objects show how the iframe is handled according to the custom logic you provided.  \\nImportant: When presvering items such as links, you should be mindful not to include . in your seperators, or leave seperators blank. RecursiveCharacterTextSplitter splits on full stop, which will cut links in half. Ensure you provide a seperator list with . instead.'), Document(metadata={'Header 1': 'How to split HTML', 'Header 2': 'Using HTMLSemanticPreservingSplitter\\u200b', 'Header 3': 'Using a custom handler to analyze an image with an LLM\\u200b'}, page_content='With custom handler\\'s, we can also override the default processing for any element. A great example of this, is inserting semantic analysis of an image within a document, directly in the chunking flow.  \\nSince our function is called when the tag is discovered, we can override the <img> tag and turn off preserve_images to insert any content we would like to embed in our chunks.  \\n\"\"\"This example assumes you have helper methods `load_image_from_url` and an LLM agent `llm` that can process image data.\"\"\"from langchain.agents import AgentExecutor# This example needs to be replaced with your own agentllm = AgentExecutor(...)# This method is a placeholder for loading image data from a URL and is not implemented heredef load_image_from_url(image_url: str) -> bytes: # Assuming this method fetches the image data from the URL return b\"image_data\"html_string = \"\"\"<!DOCTYPE html><html> <body> <div> <h1>Section with Image and Link</h1> <p> <img src=\"https://example.com/image.jpg\" alt=\"An example image\" /> Some text after the image. </p> <ul> <li>Item 1: Description of item 1, which is quite detailed and important.</li> <li>Item 2: Description of item 2, which also contains significant information.</li> <li>Item 3: Description of item 3, another item that we don\\'t want to split across chunks.</li> </ul> </div> </body></html>\"\"\"def custom_image_handler(img_tag) -> str: img_src = img_tag.get(\"src\", \"\") img_alt = img_tag.get(\"alt\", \"No alt text provided\") image_data = load_image_from_url(img_src) semantic_meaning = llm.invoke(image_data) markdown_text = f\"[Image Alt Text: {img_alt} | Image Source: {img_src} | Image Semantic Meaning: {semantic_meaning}]\" return markdown_textsplitter = HTMLSemanticPreservingSplitter( headers_to_split_on=headers_to_split_on, max_chunk_size=50, separators=[\"\\\\n\\\\n\", \"\\\\n\", \". \"], elements_to_preserve=[\"ul\"], preserve_images=False, custom_handlers={\"img\": custom_image_handler},)documents = splitter.split_text(html_string)print(documents)  \\nAPI Reference:AgentExecutor  \\n[Document(metadata={\\'Header 1\\': \\'Section with Image and Link\\'}, page_content=\\'[Image Alt Text: An example image | Image Source: https://example.com/image.jpg | Image Semantic Meaning: semantic-meaning] Some text after the image\\'), Document(metadata={\\'Header 1\\': \\'Section with Image and Link\\'}, page_content=\". Item 1: Description of item 1, which is quite detailed and important. Item 2: Description of item 2, which also contains significant information. Item 3: Description of item 3, another item that we don\\'t want to split across chunks.\")]  \\nWith our custom handler written to extract the specific fields from a <img> element in HTML, we can further process the data with our agent, and insert the result directly into our chunk. It is important to ensure preserve_images is set to False otherwise the default processing of <img> fields will take place.'), Document(metadata={}, page_content='Edit this page  \\nWas this page helpful?  \\nPrevious  \\nHow to split text by tokens  \\nNext  \\nHow to do question answering over CSVs  \\nOverview of the SplittersExample HTML DocumentUsing HTMLHeaderTextSplitterUsing HTMLSectionSplitterUsing HTMLSemanticPreservingSplitter  \\nHTMLHeaderTextSplitterHTMLSectionSplitterHTMLSemanticPreservingSplitterChoosing the Right Splitter  \\nHow to split from a URL or HTML file:How to constrain chunk sizes:Limitations  \\nHow to split HTML strings:How to constrain chunk sizes:  \\nPreserving Tables and ListsUsing a Custom HandlerUsing a custom handler to analyze an image with an LLM  \\nCommunity  \\nTwitter  \\nGitHub  \\nOrganizationPythonJS/TS  \\nMore  \\nHomepageBlogYouTube  \\nCopyright ¬© 2025 LangChain, Inc.')]\n"
     ]
    }
   ],
   "source": [
    "#method 2\n",
    "\n",
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "url=\"https://python.langchain.com/docs/how_to/split_html/#choosing-the-right-splitter\"\n",
    "\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on)\n",
    "html_header_splits = html_splitter.split_text_from_url(url)\n",
    "print(html_header_splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'openapi': '3.1.0', 'info': {'title': 'LangSmith', 'version': '0.1.0'}, 'paths': {'/api/v1/sessions/{session_id}': {'get': {'tags': ['tracer-sessions'], 'summary': 'Read Tracer Session', 'description': 'Get a specific session.'}}}}\n",
      "{'paths': {'/api/v1/sessions/{session_id}': {'get': {'operationId': 'read_tracer_session_api_v1_sessions__session_id__get', 'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}}}}\n",
      "{'paths': {'/api/v1/sessions/{session_id}': {'get': {'parameters': [{'name': 'session_id', 'in': 'path', 'required': True, 'schema': {'type': 'string', 'format': 'uuid', 'title': 'Session Id'}}, {'name': 'include_stats', 'in': 'query', 'required': False, 'schema': {'type': 'boolean', 'default': False, 'title': 'Include Stats'}}, {'name': 'accept', 'in': 'header', 'required': False, 'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Accept'}}]}}}}\n",
      "page_content='{\"openapi\": \"3.1.0\", \"info\": {\"title\": \"LangSmith\", \"version\": \"0.1.0\"}, \"paths\": {\"/api/v1/sessions/{session_id}\": {\"get\": {\"tags\": [\"tracer-sessions\"], \"summary\": \"Read Tracer Session\", \"description\": \"Get a specific session.\"}}}}'\n",
      "page_content='{\"paths\": {\"/api/v1/sessions/{session_id}\": {\"get\": {\"operationId\": \"read_tracer_session_api_v1_sessions__session_id__get\", \"security\": [{\"API Key\": []}, {\"Tenant ID\": []}, {\"Bearer Auth\": []}]}}}}'\n",
      "page_content='{\"paths\": {\"/api/v1/sessions/{session_id}\": {\"get\": {\"parameters\": [{\"name\": \"session_id\", \"in\": \"path\", \"required\": true, \"schema\": {\"type\": \"string\", \"format\": \"uuid\", \"title\": \"Session Id\"}}, {\"name\": \"include_stats\", \"in\": \"query\", \"required\": false, \"schema\": {\"type\": \"boolean\", \"default\": false, \"title\": \"Include Stats\"}}, {\"name\": \"accept\", \"in\": \"header\", \"required\": false, \"schema\": {\"anyOf\": [{\"type\": \"string\"}, {\"type\": \"null\"}], \"title\": \"Accept\"}}]}}}}'\n"
     ]
    }
   ],
   "source": [
    "#https://python.langchain.com/docs/how_to/recursive_json_splitter/\n",
    "import json\n",
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "import requests\n",
    "\n",
    "# This is a large nested json object and will be loaded as a python dict\n",
    "json_data = requests.get(\"https://api.smith.langchain.com/openapi.json\").json()\n",
    "\n",
    "\n",
    "splitter = RecursiveJsonSplitter(max_chunk_size=300)\n",
    "\n",
    "# Recursively split json data - If you need to access/manipulate the smaller json chunks\n",
    "json_chunks = splitter.split_json(json_data=json_data)\n",
    "\n",
    "#printing top 3 chunks\n",
    "for chunk in json_chunks[:3]:\n",
    "    print(chunk)\n",
    "\n",
    "# The splitter can also output documents\n",
    "docs = splitter.create_documents(texts=[json_data])\n",
    "\n",
    "for doc in docs[:3]:\n",
    "    print(doc)\n",
    "\n",
    "#Or use .split_text to obtain string content directly:\n",
    "texts = splitter.split_text(json_data=json_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.001864203019067645\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "#https://platform.openai.com/docs/guides/embeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "text = \"Hi this is shanmukh\"\n",
    "result = embeddings.embed_query(text)\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.embeddings.Embeddings object at 0x000001E006FCDD00> async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x000001E0068FE670> model='text-embedding-3-small' dimensions=199 deployment='text-embedding-ada-002' openai_api_version=None openai_api_base=None openai_api_type=None openai_proxy=None embedding_ctx_length=8191 openai_api_key=SecretStr('**********') openai_organization=None allowed_special=None disallowed_special=None chunk_size=1000 max_retries=2 request_timeout=None headers=None tiktoken_enabled=True tiktoken_model_name=None show_progress_bar=False model_kwargs={} skip_empty=False default_headers=None default_query=None retry_min_seconds=4 retry_max_seconds=20 http_client=None http_async_client=None check_embedding_ctx_length=True\n",
      "[-0.00402143644168973, -0.11767087131738663, 0.051633257418870926, -0.00761921564117074, -0.03791734576225281, -0.1177767887711525, 0.08880920708179474, 0.11724721640348434, -0.08690274506807327, -0.05719376355409622, 0.0032684514299035072, -0.08367235958576202, -0.04085646942257881, 0.0028017661534249783, 0.030132640153169632, 0.10263103246688843, -0.010578198358416557, -0.01563560962677002, 0.030953476205468178, 0.0014008830767124891, 0.046681761741638184, -0.041147734969854355, 0.10421974956989288, 0.024148477241396904, 0.005004454404115677, -0.049223706126213074, 0.021699208766222, -0.08817371726036072, -0.00342566822655499, -0.2133115530014038, 0.05269240215420723, -0.05841177701950073, 0.0023913481272757053, 0.02751125954091549, -0.0048290337435901165, -0.030794605612754822, 0.031324177980422974, 0.04117421433329582, -0.07525216788053513, 0.013298873789608479, 0.03291289135813713, -0.0585176944732666, 0.004514600150287151, 0.14192526042461395, 0.04361024498939514, -0.052957188338041306, -0.21670082211494446, -0.060265280306339264, 0.017184607684612274, 0.031774312257766724, 0.010730450041592121, 0.029550110921263695, 0.015768002718687057, 0.1474328190088272, 0.04464291036128998, -0.11968324333429337, 0.08271912485361099, 0.06736154109239578, -0.009624969214200974, 0.03291289135813713, -0.07398119568824768, -0.12497896701097488, -0.01856149360537529, 0.017052214592695236, -0.10533184558153152, -0.07684087753295898, -0.05846473574638367, 0.06476663798093796, -0.05608166381716728, -0.09452857822179794, 0.06105963885784149, 0.03638158738613129, -0.04504008963704109, 0.03217149153351784, -0.013338591903448105, 0.03534892201423645, 0.010333271697163582, -0.021394703537225723, 0.09071566164493561, 0.04583444818854332, -0.07752932608127594, 0.08732639998197556, 0.008214984089136124, -0.0547577328979969, -0.14849194884300232, 0.06857956200838089, -0.20091956853866577, 0.1054377630352974, -0.07218065112829208, 0.003839396173134446, -0.036063846200704575, 0.06868547201156616, -0.08992130309343338, 0.03918831795454025, 0.020322320982813835, 0.03005320392549038, -0.053963374346494675, 0.029841376468539238, 0.09733530879020691, -0.02364538423717022, 0.038288045674562454, 0.03256867080926895, 0.10549072176218033, 0.05624053254723549, -0.006917532533407211, -0.0896565169095993, 0.024426503106951714, -0.05872952193021774, 0.008910046890377998, 0.1063380315899849, -0.2165949046611786, -0.1123751550912857, 0.05666419118642807, 0.09526998549699783, 0.02253328450024128, 0.10713239014148712, 0.16046027839183807, -0.02180512249469757, -0.09384013712406158, -0.002626345492899418, -0.08790893107652664, 0.04726428911089897, 0.014986883848905563, -0.09754714369773865, -0.13366393744945526, -0.05221578851342201, 0.06810294091701508, -0.05422816053032875, -0.15421132743358612, -0.01347760483622551, 0.05740559101104736, -0.07514625042676926, -0.010756928473711014, -0.07202177494764328, -0.017568547278642654, -0.03590497374534607, -0.04890596494078636, 0.06969165802001953, -0.08822667598724365, -0.034289781004190445, -0.0030963406898081303, -0.030847562476992607, -0.0018501917365938425, -0.04559613764286041, 0.09669982641935349, -0.09569364041090012, 0.08743231743574142, 0.0773174986243248, -0.0481380857527256, -0.035507794469594955, 0.014430833980441093, 0.09648799896240234, 0.014669140800833702, 0.029099974781274796, 0.054069288074970245, -0.09034496545791626, 0.007970056496560574, -0.009207931347191334, 0.035746101289987564, 0.09929472953081131, 0.048985399305820465, -0.01926317811012268, 0.007188938092440367, -0.002950708381831646, -0.027802523225545883, 0.09881811589002609, -0.08955060690641403, 0.03148304671049118, -0.0849962905049324, -0.016919821500778198, -0.026690423488616943, -0.012458178214728832, -0.0017326930537819862, -0.07154516130685806, -0.029576590284705162, 0.02793491631746292, 0.10051274299621582, -0.09786488115787506, -0.0594179667532444, 0.0612185113132, 0.0020471264142543077, 0.11618807166814804, 0.01198156364262104, 0.053274933248758316, 0.0035977789666503668, 0.052612967789173126, 0.09060975164175034, -0.017224324867129326, -0.04639049619436264, 0.056399405002593994, 0.009260888211429119, 0.17698292434215546, -0.009711024351418018, 0.018402623012661934, -0.007791326381266117, -0.07890620827674866, 0.10930363833904266, -0.021818362176418304, -0.019276415929198265]\n",
      "199\n",
      "-0.00402143644168973\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "#https://platform.openai.com/docs/guides/embeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\",dimensions=199)\n",
    "print(embeddings)\n",
    "\n",
    "\n",
    "text = \"Hi this is shanmukh\"\n",
    "result = embeddings.embed_query(text)\n",
    "print(result)\n",
    "print(len(result))\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Building an exciting new project with LangChain - come check it out! [{'source': 'tweet'}]\n",
      "* Building an exciting new project with LangChain - come check it out! [{'source': 'tweet'}]\n",
      "* [SIM=0.893664] The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees. [{'source': 'news'}]\n",
      "* I had chocolate chip pancakes and scrambled eggs for breakfast this morning. [{'source': 'tweet'}]\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from uuid import uuid4\n",
    "from langchain_core.documents import Document\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
    ")\n",
    "\n",
    "document_1 = Document(\n",
    "    page_content=\"I had chocolate chip pancakes and scrambled eggs for breakfast this morning.\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=1,\n",
    ")\n",
    "\n",
    "document_2 = Document(\n",
    "    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    "    id=2,\n",
    ")\n",
    "\n",
    "document_3 = Document(\n",
    "    page_content=\"Building an exciting new project with LangChain - come check it out!\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=3,\n",
    ")\n",
    "\n",
    "document_4 = Document(\n",
    "    page_content=\"Robbers broke into the city bank and stole $1 million in cash.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    "    id=4,\n",
    ")\n",
    "\n",
    "document_5 = Document(\n",
    "    page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=5,\n",
    ")\n",
    "\n",
    "document_6 = Document(\n",
    "    page_content=\"Is the new iPhone worth the price? Read this review to find out.\",\n",
    "    metadata={\"source\": \"website\"},\n",
    "    id=6,\n",
    ")\n",
    "\n",
    "document_7 = Document(\n",
    "    page_content=\"The top 10 soccer players in the world right now.\",\n",
    "    metadata={\"source\": \"website\"},\n",
    "    id=7,\n",
    ")\n",
    "\n",
    "document_8 = Document(\n",
    "    page_content=\"LangGraph is the best framework for building stateful, agentic applications!\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=8,\n",
    ")\n",
    "\n",
    "document_9 = Document(\n",
    "    page_content=\"The stock market is down 500 points today due to fears of a recession.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    "    id=9,\n",
    ")\n",
    "\n",
    "document_10 = Document(\n",
    "    page_content=\"I have a bad feeling I am going to get deleted :(\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=10,\n",
    ")\n",
    "\n",
    "documents = [\n",
    "    document_1,\n",
    "    document_2,\n",
    "    document_3,\n",
    "    document_4,\n",
    "    document_5,\n",
    "    document_6,\n",
    "    document_7,\n",
    "    document_8,\n",
    "    document_9,\n",
    "    document_10,\n",
    "]\n",
    "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "\n",
    "vector_store.add_documents(documents=documents, ids=uuids)\n",
    "\n",
    "\n",
    "results = vector_store.similarity_search(\n",
    "    \"LangChain provides abstractions to make working with LLMs easy\",\n",
    "    k=2,\n",
    "    filter={\"source\": \"tweet\"},\n",
    ")\n",
    "for res in results:\n",
    "    print(f\"* {res.page_content} [{res.metadata}]\")\n",
    "\n",
    "\n",
    "\n",
    "#k=1 specifies the number of results that the similarity search should return.\n",
    "results = vector_store.similarity_search_with_score(\n",
    "    \"Will it be hot tomorrow?\", k=1, filter={\"source\": \"news\"}\n",
    ")\n",
    "for res, score in results:\n",
    "    print(f\"* [SIM={score:3f}] {res.page_content} [{res.metadata}]\")\n",
    "\n",
    "#Search by vector\n",
    "results = vector_store.similarity_search_by_vector(\n",
    "    embedding=embeddings.embed_query(\"I love green eggs and ham!\"), k=1\n",
    ")\n",
    "for doc in results:\n",
    "    print(f\"* {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* APJ was born in india [{'source': 'D://GenAI//kalam.txt'}]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "file_path = \"D://GenAI//kalam.txt\"\n",
    "\n",
    "loader = TextLoader(file_path)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "final_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection1\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db1\",  # Where to save data locally, remove if not necessary\n",
    ")\n",
    "\n",
    "vector_store.add_documents(final_docs)\n",
    "\n",
    "results = vector_store.similarity_search(\n",
    "    \"Where is APJ Born\",\n",
    "    k=1,\n",
    ")\n",
    "for res in results:\n",
    "    print(f\"* {res.page_content} [{res.metadata}]\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
