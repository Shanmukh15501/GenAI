{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document loaders\n",
    "- Document Loaders are responsible for loading documents from a variety of sources.\n",
    "- Reference https://python.langchain.com/docs/how_to/#document-loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'D://GenAI//kalam.pdf', 'page': 0}, page_content=\"Avul Pakir Jainulabdeen Abdul Kalam BR (/ËˆÉ™bdÊŠl kÉ™ËˆlÉ‘Ëm/ â“˜; 15 October 1931 â€“ 27 July \\n2015) was an Indian aerospace scientist and statesman who served as the 11th president of \\nIndia from 2002 to 2007. Born and raised in a Muslim family in Rameswaram, Tamil Nadu, \\nhe studied physics and aerospace engineering. He spent the next four decades as a \\nscientist and science administrator, mainly at the Defence Research and Development \\nOrganisation (DRDO) and Indian Space Research Organisation (ISRO) and was intimately \\ninvolved in India's civilian space programme and military missile development efforts.[2] He \\nthus came to be known as the Missile Man of India for his work on the development \\nof ballistic missile and launch vehicle technology.[3][4][5] He also played a pivotal \\norganisational, technical, and political role in India's Pokhran-II nuclear tests in 1998, the first \\nsince the original nuclear test by India in 1974.[6] \")]\n"
     ]
    }
   ],
   "source": [
    "#How to load PDFs\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"D://GenAI//kalam.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(file_path)\n",
    "pages = []\n",
    "for page in loader.load():\n",
    "    pages.append(page)\n",
    "\n",
    "print(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'D://GenAI//kalam.txt'}, page_content='The Defence Research and Development Organisation.\\nDefence Research and Development in Ministry of Defence of the Government of India.The Defence Research and Development Organisation.\\nDefence Research and Development in Ministry of Defence of the Government of India.The Defence Research and Development Organisation.\\nDefence Research and Development in Ministry of Defence of the Government of India.\\nThe Defence Research and Development Organisation.\\nDefence Research and Development in Ministry of Defence of the Government of India.\\nAPJ was born in india')]\n"
     ]
    }
   ],
   "source": [
    "#How to load txt file\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "file_path = \"D://GenAI//kalam.txt\"\n",
    "\n",
    "loader = TextLoader(file_path)\n",
    "pages = []\n",
    "\n",
    "for page in loader.load():\n",
    "    pages.append(page)\n",
    "\n",
    "print(pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Architecture | Guides | Tutorials | How-to guides | Conceptual guide | Integrations | API reference | Ecosystem | ğŸ¦œğŸ› ï¸ LangSmith | ğŸ¦œğŸ•¸ï¸ LangGraph | Additional resources | Versions | Security | Contributing' metadata={'source': 'https://python.langchain.com/docs/introduction/'}\n"
     ]
    }
   ],
   "source": [
    "#How to load web pages\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "page_url = \"https://python.langchain.com/docs/introduction/\"\n",
    "\n",
    "loader = WebBaseLoader(web_paths=[page_url],\n",
    "                       bs_kwargs={\n",
    "        \"parse_only\": bs4.SoupStrainer(class_=\"table-of-contents__link toc-highlight\"),\n",
    "    },\n",
    "    bs_get_text_kwargs={\"separator\": \" | \", \"strip\": True},)\n",
    "docs = []\n",
    "for doc in loader.load():\n",
    "    docs.append(doc)\n",
    "\n",
    "assert len(docs) == 1\n",
    "doc = docs[0]\n",
    "\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large language models (LLMs) have demonstrated impressive reasoning\n",
      "abilities, but they still strugg\n",
      "{'Entry ID': 'http://arxiv.org/abs/2410.13080v1', 'Published': datetime.date(2024, 10, 16), 'Title': 'Graph-constrained Reasoning: Faithful Reasoning on Knowledge Graphs with Large Language Models', 'Authors': 'Linhao Luo, Zicheng Zhao, Chen Gong, Gholamreza Haffari, Shirui Pan'}\n"
     ]
    }
   ],
   "source": [
    "#https://python.langchain.com/docs/integrations/providers/arxiv/#installation-and-setup\n",
    "#ArxivLoader is a tool used to fetch and load research papers from the arXiv database, which is a popular repository for academic papers in fields like physics, computer science, and mathematics. It allows users to retrieve papers in a structured format, enabling them to process and analyze the content programmatically.\n",
    "#for more data source providers go through this link https://python.langchain.com/docs/integrations/providers/all/\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "loader = ArxivLoader(\n",
    "    query=\"reasoning\"\n",
    ")\n",
    "\n",
    "docs = loader.get_summaries_as_docs()\n",
    "print(docs[0].page_content[:100])\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon Reeve Musk (; born June 28, 1971) is a businessman known for his key roles in the space company\n"
     ]
    }
   ],
   "source": [
    "#load from wikipedia\n",
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "\n",
    "retriever = WikipediaRetriever()\n",
    "docs = retriever.invoke(\"Elon Musk\")\n",
    "print(docs[0].page_content[:100])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation\n",
    "- How to recursively split by characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={}, page_content='Hundreds of Elon Muskâ€™s private text messages about Twitter were made public Thursday in a Delaware'), Document(metadata={}, page_content='in a Delaware court filing, part of a stack of evidence in Twitterâ€™s forthcoming legal case against'), Document(metadata={}, page_content='legal case against the Tesla CEO.'), Document(metadata={}, page_content='Muskâ€™s texts offer a rare insight into the private conversations of the man at the center of a'), Document(metadata={}, page_content='at the center of a business deal that captivated the world. The messages were mostly sent in March'), Document(metadata={}, page_content='sent in March and April this year, a period when Musk began regularly criticizing Twitter publicly,'), Document(metadata={}, page_content='Twitter publicly, announced he had acquired a minority stake in the company, agreed to join its'), Document(metadata={}, page_content='agreed to join its board, then backed out and made a $44 billion hostile takeover bid before'), Document(metadata={}, page_content='takeover bid before attempting to pull out of the deal.'), Document(metadata={}, page_content='Twitter is now suing Musk in an attempt to force him to go through with his agreement, finalized in'), Document(metadata={}, page_content='finalized in April, to buy Twitter. In his attempt to get out of the deal, Musk argues that Twitter'), Document(metadata={}, page_content='argues that Twitter misled him over the number of fake accounts on its platform. The trial is'), Document(metadata={}, page_content='The trial is scheduled to begin on Oct. 17.'), Document(metadata={}, page_content='Read More: Whether or Not He Buys Twitter, Elon Musk Has Thrown the Company Into Turmoil'), Document(metadata={}, page_content='The texts reveal how Jack Dorsey, Twitterâ€™s co-founder, cheered Muskâ€™s effort to buy and reform the'), Document(metadata={}, page_content='buy and reform the platform from behind the scenes, and attempted to heal Muskâ€™s deteriorating'), Document(metadata={}, page_content='deteriorating relationship with Parag Agrawal, Dorseyâ€™s successor as Twitter CEO.'), Document(metadata={}, page_content='They also show how, at the same time as he was soliciting financing for his Twitter deal, Musk was'), Document(metadata={}, page_content='deal, Musk was contemplating a â€œPlan Bâ€ to turn the platform into a blockchain-based social'), Document(metadata={}, page_content='social network. The messages suggest Musk soured on that plan after concluding it would be'), Document(metadata={}, page_content='it would be unfeasible.'), Document(metadata={}, page_content='The texts reveal Muskâ€™s contact list is a whoâ€™s who of business titans and media personalities,'), Document(metadata={}, page_content='personalities, many of whom clambered to offer him their two cents as his plans for Twitter became'), Document(metadata={}, page_content='for Twitter became public knowledge.'), Document(metadata={}, page_content='Here are the big takeaways from Elon Muskâ€™s Twitter text messages.'), Document(metadata={}, page_content='Jack Dorseyâ€™s behind-the-scenes role'), Document(metadata={}, page_content='The texts reveal how Dorsey had pushed for months at Twitter to grant Musk a seat on the companyâ€™s'), Document(metadata={}, page_content='on the companyâ€™s board. On March 26, after Musk sent a series of tweets criticizing Twitterâ€™s'), Document(metadata={}, page_content='Twitterâ€™s approach to free speech and asking whether a new platform was needed, Dorsey sent Musk a'), Document(metadata={}, page_content='Dorsey sent Musk a series of messages urging Musk to â€œhelpâ€ turn the company around.'), Document(metadata={}, page_content='â€œYou care so much, get its importance, and could def help in immeasurable ways,â€ Dorsey wrote to'), Document(metadata={}, page_content='Dorsey wrote to Musk.'), Document(metadata={}, page_content='Dorsey told Musk that in 2021, while he was still CEO, he had tried to convince Twitterâ€™s board to'), Document(metadata={}, page_content='Twitterâ€™s board to give Musk a seat, but decided to quit after the directors declined. The board'), Document(metadata={}, page_content='declined. The board considered giving Musk a seat too risky, Dorseyâ€™s texts say. â€œThatâ€™s about the'), Document(metadata={}, page_content='â€œThatâ€™s about the time I decided I needed to work to leave [Twitter], as hard as it was for me,â€'), Document(metadata={}, page_content='as it was for me,â€ Dorsey told Musk in the messages. (Dorsey resigned as Twitter CEO in November'), Document(metadata={}, page_content='CEO in November 2021, and stepped down from its board in May 2022.)'), Document(metadata={}, page_content='Meanwhile, Muskâ€™s tweets about the future of Twitter appeared to have spurred the companyâ€™s board'), Document(metadata={}, page_content='the companyâ€™s board to reevaluate their opposition to the billionaire taking a seat. Days later,'), Document(metadata={}, page_content='a seat. Days later, according to the texts, Musk met Agrawal and Bret Taylor, chair of the Twitter'), Document(metadata={}, page_content='of the Twitter board, for dinner at a quirky AirBnB in San Jose, California, surrounded by'), Document(metadata={}, page_content='surrounded by tractors, donkeys, and abandoned trucks.'), Document(metadata={}, page_content='â€œGreat dinner :)â€ Musk texted a group chat afterward.'), Document(metadata={}, page_content='â€œReally great,â€ Taylor replied. â€œThe donkeys and dystopian surveillance helicopters really added to'), Document(metadata={}, page_content='really added to the ambiance.â€'), Document(metadata={}, page_content='â€œMemorable for multiple reasons. Really enjoyed it,â€ Agrawal said.'), Document(metadata={}, page_content='Two days later, Dorsey texted Musk. â€œI heard good things are happening,â€ he said.'), Document(metadata={}, page_content='Read More: Elon Musk Is Convinced Heâ€™s the Future. We Need to Look Beyond Him'), Document(metadata={}, page_content='The next day, April 4, Musk announced publicly he had been buying Twitter stock since January and'), Document(metadata={}, page_content='since January and had amassed an almost 10% stake in the company. The day after that, Agrawal'), Document(metadata={}, page_content='after that, Agrawal tweeted out a statement saying Musk had agreed to join the Twitter board.'), Document(metadata={}, page_content='After the news was announced, Dorsey criticized the board in a message to Musk. â€œThank you for'), Document(metadata={}, page_content='â€œThank you for joining!â€ he wrote. â€œParag is an incredible engineer. The board is terrible. Always'), Document(metadata={}, page_content='is terrible. Always here to talk through anything you want.â€'), Document(metadata={}, page_content='â€œI couldnâ€™t be happier youâ€™re doing this,â€ Dorsey added. â€œIâ€™ve wanted it for a long time. Got very'), Document(metadata={}, page_content='long time. Got very emotional when I learned it was finally possible.â€'), Document(metadata={}, page_content='Muskâ€™s relationship with Parag Agrawal sours'), Document(metadata={}, page_content='The texts provide a behind-the-scenes look at how Muskâ€™s conversations with the Twitter CEO quickly'), Document(metadata={}, page_content='Twitter CEO quickly deteriorated.'), Document(metadata={}, page_content='The pair had a cordial exchange where they bonded over a shared love of coding and engineering. â€œI'), Document(metadata={}, page_content='and engineering. â€œI have a ton of ideas, but [let me know] if Iâ€™m pushing too hard,â€ Musk wrote to'), Document(metadata={}, page_content='Musk wrote to Agrawal on April 7. â€œI just want Twitter to be maximum amazing.â€'), Document(metadata={}, page_content='But two days later, after more tweets from Musk criticizing Twitter publicly, Agrawal sent him a'), Document(metadata={}, page_content='Agrawal sent him a terse message. â€œYou are free to tweet â€˜is Twitter dying?â€™ or anything else about'), Document(metadata={}, page_content='anything else about Twitter â€“ but itâ€™s my responsibility to tell you that itâ€™s not helping me make'), Document(metadata={}, page_content='not helping me make Twitter better in the current context,â€ Agrawal wrote. â€œNext time we speak, Iâ€™d'), Document(metadata={}, page_content='time we speak, Iâ€™d like to [â€¦] provide you perspective on the level of internal distraction right'), Document(metadata={}, page_content='distraction right now and how itâ€™s hurting our ability to do work.â€'), Document(metadata={}, page_content='â€œWhat did you get done this week?â€ Musk wrote back.'), Document(metadata={}, page_content='â€œIâ€™m not joining the board,â€ Musk added in a followup message. â€œThis is a waste of time. Will make'), Document(metadata={}, page_content='of time. Will make an offer to take Twitter private.â€'), Document(metadata={}, page_content='As Musk began soliciting bankers and other tech billionaires for cash to finance his takeover, the'), Document(metadata={}, page_content='his takeover, the court document shows Dorsey attempting to repair the damage between Musk and'), Document(metadata={}, page_content='between Musk and Agrawal.'), Document(metadata={}, page_content='â€œI just want to make this amazing and feel bound to it,â€ Dorsey wrote to Musk on April 26, after'), Document(metadata={}, page_content='on April 26, after getting him to agree to join a group call with Agrawal. â€œI wonâ€™t let this fail'), Document(metadata={}, page_content='wonâ€™t let this fail and will do whatever it takes. Itâ€™s just too critical to humanity.â€'), Document(metadata={}, page_content='Read More: The Twitter Whistleblower Needs You to Trust Him'), Document(metadata={}, page_content='The subsequent messages suggest that the attempt failed. â€œYou and I are in complete agreement,â€'), Document(metadata={}, page_content='agreement,â€ Musk wrote to Dorsey after the call. â€œParag is just moving far too slowly and trying to'), Document(metadata={}, page_content='and trying to please people who will not be happy no matter what he does.â€'), Document(metadata={}, page_content='â€œAt least it became clear that you canâ€™t work together,â€ Dorsey replied. â€œThat was clarifying.â€'), Document(metadata={}, page_content='How Musk soured on blockchain'), Document(metadata={}, page_content='Another big takeaway from the text messages is that Musk was weighing an ill-fated plan to turn'), Document(metadata={}, page_content='plan to turn Twitter into a blockchain-based platform, even after he had decided to mount a hostile'), Document(metadata={}, page_content='to mount a hostile takeover of the company.'), Document(metadata={}, page_content='â€œI have an idea for a blockchain social media system that does both payments and short text'), Document(metadata={}, page_content='and short text messages/links like Twitter,â€ Musk texted his brother, Kimbal Musk, on April 9. â€œYou'), Document(metadata={}, page_content='on April 9. â€œYou have to pay a tiny amount to register your message on the chain, which will cut'), Document(metadata={}, page_content='which will cut out the vast majority of spam and bots. There is no throat to choke, so free speech'), Document(metadata={}, page_content='so free speech is guaranteed.â€'), Document(metadata={}, page_content='Musk said he had a â€œPlan-Bâ€ for Twitter based on the blockchain. â€œMy Plan-B is a blockchain-based'), Document(metadata={}, page_content='a blockchain-based version of Twitter, where the â€œtweetsâ€ are embedded in the transaction as'), Document(metadata={}, page_content='the transaction as comments,â€ he wrote in an April 14 message to Steve Davis, the CEO of Muskâ€™s'), Document(metadata={}, page_content='the CEO of Muskâ€™s tunnel-drilling Boring Company. â€œSo youâ€™d have to pay maybe 0.1 Doge per comment'), Document(metadata={}, page_content='Doge per comment or repost of that comment.â€ Dogecoin, or Doge, is a spoof cryptocurrency, named'), Document(metadata={}, page_content='named after a meme about a dog. Musk repeatedly tweeted about investing in the digital currency in'), Document(metadata={}, page_content='digital currency in 2021, which briefly sent its value surging before it crashed.'), Document(metadata={}, page_content='But just 11 days later, Musk appeared to have changed his mind. Musk received a text from an'), Document(metadata={}, page_content='a text from an intermediary who said he represented the billionaire CEO of the cryptocurrency'), Document(metadata={}, page_content='the cryptocurrency exchange FTX, Sam Bankman-Fried. The intermediary said Bankman-Fried could offer'), Document(metadata={}, page_content='could offer Musk up to $5 billion in financing for his Twitter takeover if Musk agreed to let him'), Document(metadata={}, page_content='agreed to let him â€œdo the engineering for social media blockchain integration.â€'), Document(metadata={}, page_content='â€œBlockchain Twitter isnâ€™t possible,â€ Musk shot back. â€œThe bandwidth and latency requirements cannot'), Document(metadata={}, page_content='requirements cannot be supported by a peer to peer network, unless those â€˜peersâ€™ are absolutely'), Document(metadata={}, page_content='are absolutely gigantic, thus defeating the purpose of a decentralized network.â€'), Document(metadata={}, page_content='Musk still agreed to meet with Bankman-Fried, but on one condition: â€œSo long as I donâ€™t have to'), Document(metadata={}, page_content='as I donâ€™t have to have a laborious blockchain debate.â€ (Bankman-Fried did not end up contributing'), Document(metadata={}, page_content='end up contributing money to Muskâ€™s Twitter bid.)'), Document(metadata={}, page_content='Muskâ€™s inbox blows up with big-name messages'), Document(metadata={}, page_content='As Muskâ€™s plans for Twitter began to dominate the news cycle, his inbox began to blow up with'), Document(metadata={}, page_content='to blow up with messages from public figuresâ€”including interview requests from high-profile'), Document(metadata={}, page_content='from high-profile journalists, outreach from fixers claiming to represent politicians, and informal'), Document(metadata={}, page_content='and informal commitments from some of Silicon Valleyâ€™s biggest investors to help fund the Twitter'), Document(metadata={}, page_content='fund the Twitter takeover.'), Document(metadata={}, page_content='Gayle King, co-host of CBS This Morning, texted Musk on April 14. â€œELON! You buying Twitter or'), Document(metadata={}, page_content='buying Twitter or offering to buy Twitter â€¦ Wow!â€ her message reads. â€œNow donâ€™t you think we should'), Document(metadata={}, page_content='you think we should sit down together face to face? This is, as the kids of today say, a â€˜gangsta'), Document(metadata={}, page_content='say, a â€˜gangsta moveâ€™.â€ The message went on: â€œI donâ€™t know how shareholders turn this down. Like I'), Document(metadata={}, page_content='this down. Like I said, you are not like the other kids in the class.â€'), Document(metadata={}, page_content='Read More: A Complete Timeline of Elon Muskâ€™s Business Endeavors'), Document(metadata={}, page_content='The court filing shows Musk soliciting billions of dollars in financing directly, via informal'), Document(metadata={}, page_content='via informal messages with tech CEOs. On April 20, Musk texted Oracle CEO Larry Ellison to ask if'), Document(metadata={}, page_content='Ellison to ask if he would be interested in contributing funding toward his Twitter takeover. An'), Document(metadata={}, page_content='takeover. An hour later, Ellison had offered him a billion dollars. (Ellison followed through,'), Document(metadata={}, page_content='followed through, according to Bloomberg.)'), Document(metadata={}, page_content='Other heavyweights from the business world whose messages appear in the filing include Microsoft'), Document(metadata={}, page_content='include Microsoft CEO Satya Nadella, LinkedIn founder Reid Hoffman, media investor James Murdoch,'), Document(metadata={}, page_content='James Murdoch, and Salesforce co-CEO Marc Benioff. (Benioff is the owner and co-chair of TIME).'), Document(metadata={}, page_content='Muskâ€™s inbox also filled with messages from people on the overt political right, who appeared'), Document(metadata={}, page_content='right, who appeared enthused by his public statements criticizing Twitter for being too left-wing.'), Document(metadata={}, page_content='â€œAre you going to liberate Twitter from the censorship happy mob?â€ podcast host Joe Rogan asked in'), Document(metadata={}, page_content='Joe Rogan asked in a message dated April 4.'), Document(metadata={}, page_content='â€œI will provide advice, which they may or may not choose to follow,â€ Musk replied.'), Document(metadata={}, page_content='And Ron DeSantis, the Republican Governor of Florida tipped for a run at the presidency in 2024,'), Document(metadata={}, page_content='presidency in 2024, appeared to reach out through an intermediary. â€œGovernor DeSantis just called'), Document(metadata={}, page_content='just called me just now with ideas how to help you,â€ wrote Joe Lonsdale, a venture capitalist, in a'), Document(metadata={}, page_content='capitalist, in a message to Musk on April 16. â€œLet me know if you or somebody on your side wants to'), Document(metadata={}, page_content='your side wants to chat [with] him.â€')]\n",
      "140\n",
      "<class 'list'>\n",
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "source": [
    "#https://python.langchain.com/docs/how_to/recursive_text_splitter/\n",
    "#Let's go through the parameters set above for RecursiveCharacterTextSplitter:\n",
    "\n",
    "#chunk_size: The maximum size of a chunk, where size is determined by the length_function.\n",
    "#chunk_overlap: Target overlap between chunks. Overlapping chunks helps to mitigate loss of information when context is divided between chunks.\n",
    "#length_function: Function determining the chunk size.\n",
    "#is_separator_regex: Whether the separator list (defaulting to [\"\\n\\n\", \"\\n\", \" \", \"\"]) should be interpreted as regex.\n",
    "\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load example document\n",
    "with open(\"elon.txt\", encoding=\"utf-8\") as f:\n",
    "    state_of_the_union = f.read()\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "texts = text_splitter.create_documents([state_of_the_union])\n",
    "print(texts)\n",
    "print(len(texts))\n",
    "print(type(texts))\n",
    "print(type(texts[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Hundreds of Elon Muskâ€™s private text messages about Twitter were made public Thursday in a Delaware court filing, part of a stack of evidence in Twitterâ€™s forthcoming legal case against the Tesla CEO.\n",
      "\n",
      "Muskâ€™s texts offer a rare insight into the private conversations of the man at the center of a business deal that captivated the world. The messages were mostly sent in March and April this year, a period when Musk began regularly criticizing Twitter publicly, announced he had acquired a minority stake in the company, agreed to join its board, then backed out and made a $44 billion hostile takeover bid before attempting to pull out of the deal.\n",
      "\n",
      "Twitter is now suing Musk in an attempt to force him to go through with his agreement, finalized in April, to buy Twitter. In his attempt to get out of the deal, Musk argues that Twitter misled him over the number of fake accounts on its platform. The trial is scheduled to begin on Oct. 17.\n",
      "\n",
      "Read More: Whether or Not He Buys Twitter, Elon Musk Has Thrown the Company Into Turmoil\n",
      "\n",
      "The texts reveal how Jack Dorsey, Twitterâ€™s co-founder, cheered Muskâ€™s effort to buy and reform the platform from behind the scenes, and attempted to heal Muskâ€™s deteriorating relationship with Parag Agrawal, Dorseyâ€™s successor as Twitter CEO.\n",
      "\n",
      "They also show how, at the same time as he was soliciting financing for his Twitter deal, Musk was contemplating a â€œPlan Bâ€ to turn the platform into a blockchain-based social network. The messages suggest Musk soured on that plan after concluding it would be unfeasible.\n",
      "\n",
      "\n",
      "The texts reveal Muskâ€™s contact list is a whoâ€™s who of business titans and media personalities, many of whom clambered to offer him their two cents as his plans for Twitter became public knowledge.\n",
      "\n",
      "Here are the big takeaways from Elon Muskâ€™s Twitter text messages.\n",
      "\n",
      "Jack Dorseyâ€™s behind-the-scenes role\n",
      "The texts reveal how Dorsey had pushed for months at Twitter to grant Musk a seat on the companyâ€™s board. On March 26, after Musk sent a series of tweets criticizing Twitterâ€™s approach to free speech and asking whether a new platform was needed, Dorsey sent Musk a series of messages urging Musk to â€œhelpâ€ turn the company around.\n",
      "\n",
      "â€œYou care so much, get its importance, and could def help in immeasurable ways,â€ Dorsey wrote to Musk.\n",
      "\n",
      "Dorsey told Musk that in 2021, while he was still CEO, he had tried to convince Twitterâ€™s board to give Musk a seat, but decided to quit after the directors declined. The board considered giving Musk a seat too risky, Dorseyâ€™s texts say. â€œThatâ€™s about the time I decided I needed to work to leave [Twitter], as hard as it was for me,â€ Dorsey told Musk in the messages. (Dorsey resigned as Twitter CEO in November 2021, and stepped down from its board in May 2022.)\n",
      "\n",
      "\n",
      "Meanwhile, Muskâ€™s tweets about the future of Twitter appeared to have spurred the companyâ€™s board to reevaluate their opposition to the billionaire taking a seat. Days later, according to the texts, Musk met Agrawal and Bret Taylor, chair of the Twitter board, for dinner at a quirky AirBnB in San Jose, California, surrounded by tractors, donkeys, and abandoned trucks.\n",
      "\n",
      "â€œGreat dinner :)â€ Musk texted a group chat afterward.\n",
      "\n",
      "â€œReally great,â€ Taylor replied. â€œThe donkeys and dystopian surveillance helicopters really added to the ambiance.â€\n",
      "\n",
      "â€œMemorable for multiple reasons. Really enjoyed it,â€ Agrawal said.\n",
      "\n",
      "Two days later, Dorsey texted Musk. â€œI heard good things are happening,â€ he said.\n",
      "\n",
      "Read More: Elon Musk Is Convinced Heâ€™s the Future. We Need to Look Beyond Him\n",
      "\n",
      "The next day, April 4, Musk announced publicly he had been buying Twitter stock since January and had amassed an almost 10% stake in the company. The day after that, Agrawal tweeted out a statement saying Musk had agreed to join the Twitter board.\n",
      "\n",
      "After the news was announced, Dorsey criticized the board in a message to Musk. â€œThank you for joining!â€ he wrote. â€œParag is an incredible engineer. The board is terrible. Always here to talk through anything you want.â€\n",
      "\n",
      "\n",
      "â€œI couldnâ€™t be happier youâ€™re doing this,â€ Dorsey added. â€œIâ€™ve wanted it for a long time. Got very emotional when I learned it was finally possible.â€\n",
      "\n",
      "Muskâ€™s relationship with Parag Agrawal sours\n",
      "The texts provide a behind-the-scenes look at how Muskâ€™s conversations with the Twitter CEO quickly deteriorated.\n",
      "\n",
      "The pair had a cordial exchange where they bonded over a shared love of coding and engineering. â€œI have a ton of ideas, but [let me know] if Iâ€™m pushing too hard,â€ Musk wrote to Agrawal on April 7. â€œI just want Twitter to be maximum amazing.â€\n",
      "\n",
      "But two days later, after more tweets from Musk criticizing Twitter publicly, Agrawal sent him a terse message. â€œYou are free to tweet â€˜is Twitter dying?â€™ or anything else about Twitter â€“ but itâ€™s my responsibility to tell you that itâ€™s not helping me make Twitter better in the current context,â€ Agrawal wrote. â€œNext time we speak, Iâ€™d like to [â€¦] provide you perspective on the level of internal distraction right now and how itâ€™s hurting our ability to do work.â€\n",
      "\n",
      "â€œWhat did you get done this week?â€ Musk wrote back.\n",
      "\n",
      "â€œIâ€™m not joining the board,â€ Musk added in a followup message. â€œThis is a waste of time. Will make an offer to take Twitter private.â€\n",
      "\n",
      "\n",
      "As Musk began soliciting bankers and other tech billionaires for cash to finance his takeover, the court document shows Dorsey attempting to repair the damage between Musk and Agrawal.\n",
      "\n",
      "â€œI just want to make this amazing and feel bound to it,â€ Dorsey wrote to Musk on April 26, after getting him to agree to join a group call with Agrawal. â€œI wonâ€™t let this fail and will do whatever it takes. Itâ€™s just too critical to humanity.â€\n",
      "\n",
      "Read More: The Twitter Whistleblower Needs You to Trust Him\n",
      "\n",
      "The subsequent messages suggest that the attempt failed. â€œYou and I are in complete agreement,â€ Musk wrote to Dorsey after the call. â€œParag is just moving far too slowly and trying to please people who will not be happy no matter what he does.â€\n",
      "\n",
      "â€œAt least it became clear that you canâ€™t work together,â€ Dorsey replied. â€œThat was clarifying.â€\n",
      "\n",
      "How Musk soured on blockchain\n",
      "Another big takeaway from the text messages is that Musk was weighing an ill-fated plan to turn Twitter into a blockchain-based platform, even after he had decided to mount a hostile takeover of the company.\n",
      "\n",
      "â€œI have an idea for a blockchain social media system that does both payments and short text messages/links like Twitter,â€ Musk texted his brother, Kimbal Musk, on April 9. â€œYou have to pay a tiny amount to register your message on the chain, which will cut out the vast majority of spam and bots. There is no throat to choke, so free speech is guaranteed.â€\n",
      "\n",
      "\n",
      "Musk said he had a â€œPlan-Bâ€ for Twitter based on the blockchain. â€œMy Plan-B is a blockchain-based version of Twitter, where the â€œtweetsâ€ are embedded in the transaction as comments,â€ he wrote in an April 14 message to Steve Davis, the CEO of Muskâ€™s tunnel-drilling Boring Company. â€œSo youâ€™d have to pay maybe 0.1 Doge per comment or repost of that comment.â€ Dogecoin, or Doge, is a spoof cryptocurrency, named after a meme about a dog. Musk repeatedly tweeted about investing in the digital currency in 2021, which briefly sent its value surging before it crashed.\n",
      "\n",
      "But just 11 days later, Musk appeared to have changed his mind. Musk received a text from an intermediary who said he represented the billionaire CEO of the cryptocurrency exchange FTX, Sam Bankman-Fried. The intermediary said Bankman-Fried could offer Musk up to $5 billion in financing for his Twitter takeover if Musk agreed to let him â€œdo the engineering for social media blockchain integration.â€\n",
      "\n",
      "â€œBlockchain Twitter isnâ€™t possible,â€ Musk shot back. â€œThe bandwidth and latency requirements cannot be supported by a peer to peer network, unless those â€˜peersâ€™ are absolutely gigantic, thus defeating the purpose of a decentralized network.â€\n",
      "\n",
      "Musk still agreed to meet with Bankman-Fried, but on one condition: â€œSo long as I donâ€™t have to have a laborious blockchain debate.â€ (Bankman-Fried did not end up contributing money to Muskâ€™s Twitter bid.)\n",
      "\n",
      "\n",
      "Muskâ€™s inbox blows up with big-name messages\n",
      "As Muskâ€™s plans for Twitter began to dominate the news cycle, his inbox began to blow up with messages from public figuresâ€”including interview requests from high-profile journalists, outreach from fixers claiming to represent politicians, and informal commitments from some of Silicon Valleyâ€™s biggest investors to help fund the Twitter takeover.\n",
      "\n",
      "Gayle King, co-host of CBS This Morning, texted Musk on April 14. â€œELON! You buying Twitter or offering to buy Twitter â€¦ Wow!â€ her message reads. â€œNow donâ€™t you think we should sit down together face to face? This is, as the kids of today say, a â€˜gangsta moveâ€™.â€ The message went on: â€œI donâ€™t know how shareholders turn this down. Like I said, you are not like the other kids in the class.â€\n",
      "\n",
      "Read More: A Complete Timeline of Elon Muskâ€™s Business Endeavors\n",
      "\n",
      "The court filing shows Musk soliciting billions of dollars in financing directly, via informal messages with tech CEOs. On April 20, Musk texted Oracle CEO Larry Ellison to ask if he would be interested in contributing funding toward his Twitter takeover. An hour later, Ellison had offered him a billion dollars. (Ellison followed through, according to Bloomberg.)\n",
      "\n",
      "Other heavyweights from the business world whose messages appear in the filing include Microsoft CEO Satya Nadella, LinkedIn founder Reid Hoffman, media investor James Murdoch, and Salesforce co-CEO Marc Benioff. (Benioff is the owner and co-chair of TIME).\n",
      "\n",
      "\n",
      "Muskâ€™s inbox also filled with messages from people on the overt political right, who appeared enthused by his public statements criticizing Twitter for being too left-wing.\n",
      "\n",
      "â€œAre you going to liberate Twitter from the censorship happy mob?â€ podcast host Joe Rogan asked in a message dated April 4.\n",
      "\n",
      "â€œI will provide advice, which they may or may not choose to follow,â€ Musk replied.\n",
      "\n",
      "And Ron DeSantis, the Republican Governor of Florida tipped for a run at the presidency in 2024, appeared to reach out through an intermediary. â€œGovernor DeSantis just called me just now with ideas how to help you,â€ wrote Joe Lonsdale, a venture capitalist, in a message to Musk on April 16. â€œLet me know if you or somebody on your side wants to chat [with] him.â€'\n"
     ]
    }
   ],
   "source": [
    "#https://python.langchain.com/docs/how_to/character_text_splitter/\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# Load example document\n",
    "with open(\"elon.txt\", encoding=\"utf-8\") as f:\n",
    "    state_of_the_union = f.read()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\t\",\n",
    "    chunk_size=10,\n",
    "    chunk_overlap=5,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "texts = text_splitter.create_documents([state_of_the_union])\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header-1': 'Main Title', 'Header-2': 'Section 1: Introduction'}, page_content='This section introduces the topic. Below is a list:'),\n",
       " Document(metadata={'Header-1': 'Main Title', 'Header-2': 'Section 1: Introduction', 'Header-3': 'Subsection 1.1: Details'}, page_content=\"This subsection provides additional details. Here's a table:\")]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://python.langchain.com/docs/how_to/split_html/#overview-of-the-splitters\n",
    "#How to: split HTML\n",
    "\n",
    "#Choosing the Right Splitter\n",
    "    #Use HTMLHeaderTextSplitter when:\n",
    "        #You need to split an HTML document based on its header hierarchy and maintain metadata about the headers.\n",
    "    \n",
    "    #Use HTMLSectionSplitter when:\n",
    "        #You need to split the document into larger, more general sections, possibly based on custom tags or font sizes.\n",
    "    \n",
    "    #Use HTMLSemanticPreservingSplitter when: \n",
    "        #You need to split the document into chunks while preserving semantic elements like tables and lists, ensuring that they are not split and that their context is maintained.\n",
    "\n",
    "html_string = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "  <html lang='en'>\n",
    "  <head>\n",
    "    <meta charset='UTF-8'>\n",
    "    <meta name='viewport' content='width=device-width, initial-scale=1.0'>\n",
    "    <title>Fancy Example HTML Page</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <h1>Main Title</h1>\n",
    "    \n",
    "    <h2>Section 1: Introduction</h2>\n",
    "    <p>This section introduces the topic. Below is a list:</p>\n",
    "    \n",
    "    <h3>Subsection 1.1: Details</h3>\n",
    "    <p>This subsection provides additional details. Here's a table:</p>\n",
    "    \n",
    "    <h2>Section 2: Media Content</h2>\n",
    "    \n",
    "\n",
    "    <h2>Section 3: Code Example</h2>\n",
    "  \n",
    "    <h2>Conclusion</h2>\n",
    "  </body>\n",
    "  </html>\n",
    " \"\"\"\n",
    "\n",
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header-1\"),\n",
    "    (\"h2\", \"Header-2\"),\n",
    "    (\"h3\", \"Header-3\"),\n",
    "]\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on)\n",
    "html_header_splits = html_splitter.split_text(html_string)\n",
    "html_header_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={}, page_content='Skip to main content  \\nIntegrationsAPI Reference  \\nMore  \\nContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TS  \\nğŸ’¬  \\nv0.3  \\nv0.3v0.2v0.1  \\nSearch  \\nIntroductionSecurity Policy  \\nTutorials  \\nBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize Text  \\nHow-to guides  \\nHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector stores  \\nConceptual guide  \\nAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?  \\nEcosystem  \\nğŸ¦œğŸ› ï¸ LangSmithğŸ¦œğŸ•¸ï¸ LangGraph  \\nVersions  \\nv0.3Pydantic compatibilityRelease policy  \\nv0.2  \\nMigrating from v0.0 chains  \\nHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChain  \\nUpgrading to LangGraph memory  \\nHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory Agent  \\nHow-to guidesHow to split HTML  \\nOn this page  \\nHow to split HTML Overview of the Splitters\\u200b HTMLHeaderTextSplitter\\u200b HTMLSectionSplitter\\u200b HTMLSemanticPreservingSplitter\\u200b Choosing the Right Splitter\\u200b FeatureHTMLHeaderTextSplitterHTMLSectionSplitterHTMLSemanticPreservingSplitterSplits based on headersYesYesYesPreserves semantic elements (tables, lists)NoNoYesAdds metadata for headersYesYesYesCustom handlers for HTML tagsNoNoYesPreserves media (images, videos)NoNoYesConsiders font sizesNoYesNoUses XSLT transformationsNoYesNo Example HTML Document\\u200b Using HTMLHeaderTextSplitter\\u200b How to split from a URL or HTML file:\\u200b How to constrain chunk sizes:\\u200b Limitations\\u200b Using HTMLSectionSplitter\\u200b How to split HTML strings:\\u200b How to constrain chunk sizes:\\u200b Using HTMLSemanticPreservingSplitter\\u200b Preserving Tables and Lists\\u200b Explanation\\u200b Using a Custom Handler\\u200b Explanation\\u200b Using a custom handler to analyze an image with an LLM\\u200b Explanation:\\u200b'), Document(metadata={'Header 1': 'How to split HTML'}, page_content='Splitting HTML documents into manageable chunks is essential for various text processing tasks such as natural language processing, search indexing, and more. In this guide, we will explore three different text splitters provided by LangChain that you can use to split HTML content effectively:  \\nHTMLHeaderTextSplitter HTMLSectionSplitter HTMLSemanticPreservingSplitter  \\nEach of these splitters has unique features and use cases. This guide will help you understand the differences between them, why you might choose one over the others, and how to use them effectively.  \\n%pip install -qU langchain-text-splitters'), Document(metadata={'Header 1': 'How to split HTML', 'Header 2': 'Overview of the Splitters\\u200b', 'Header 3': 'HTMLHeaderTextSplitter\\u200b'}, page_content='info  \\nUseful when you want to preserve the hierarchical structure of a document based on its headings.  \\nDescription: Splits HTML text based on header tags (e.g., <h1>, <h2>, <h3>, etc.), and adds metadata for each header relevant to any given chunk.  \\nCapabilities:  \\nSplits text at the HTML element level. Preserves context-rich information encoded in document structures. Can return chunks element by element or combine elements with the same metadata.'), Document(metadata={'Header 1': 'How to split HTML', 'Header 2': 'Overview of the Splitters\\u200b', 'Header 3': 'HTMLSectionSplitter\\u200b'}, page_content='info  \\nUseful when you want to split HTML documents into larger sections, such as <section>, <div>, or custom-defined sections.  \\nDescription: Similar to HTMLHeaderTextSplitter but focuses on splitting HTML into sections based on specified tags.  \\nCapabilities:  \\nUses XSLT transformations to detect and split sections. Internally uses RecursiveCharacterTextSplitter for large sections. Considers font sizes to determine sections.'), Document(metadata={'Header 1': 'How to split HTML', 'Header 2': 'Overview of the Splitters\\u200b', 'Header 3': 'HTMLSemanticPreservingSplitter\\u200b'}, page_content='info  \\nIdeal when you need to ensure that structured elements are not split across chunks, preserving contextual relevancy.  \\nDescription: Splits HTML content into manageable chunks while preserving the semantic structure of important elements like tables, lists, and other HTML components.  \\nCapabilities:  \\nPreserves tables, lists, and other specified HTML elements. Allows custom handlers for specific HTML tags. Ensures that the semantic meaning of the document is maintained. Built in normalization & stopword removal'), Document(metadata={'Header 1': 'How to split HTML', 'Header 2': 'Overview of the Splitters\\u200b', 'Header 3': 'Choosing the Right Splitter\\u200b'}, page_content='Use HTMLHeaderTextSplitter when: You need to split an HTML document based on its header hierarchy and maintain metadata about the headers. Use HTMLSectionSplitter when: You need to split the document into larger, more general sections, possibly based on custom tags or font sizes. Use HTMLSemanticPreservingSplitter when: You need to split the document into chunks while preserving semantic elements like tables and lists, ensuring that they are not split and that their context is maintained.'), Document(metadata={'Header 1': 'How to split HTML', 'Header 2': 'Example HTML Document\\u200b'}, page_content='Let\\'s use the following HTML document as an example:  \\nhtml_string = \"\"\"<!DOCTYPE html> <html lang=\\'en\\'> <head> <meta charset=\\'UTF-8\\'> <meta name=\\'viewport\\' content=\\'width=device-width, initial-scale=1.0\\'> <title>Fancy Example HTML Page</title> </head> <body> <h1>Main Title</h1> <p>This is an introductory paragraph with some basic content.</p> <h2>Section 1: Introduction</h2> <p>This section introduces the topic. Below is a list:</p> <ul> <li>First item</li> <li>Second item</li> <li>Third item with <strong>bold text</strong> and <a href=\\'#\\'>a link</a></li> </ul> <h3>Subsection 1.1: Details</h3> <p>This subsection provides additional details. Here\\'s a table:</p> <table border=\\'1\\'> <thead> <tr> <th>Header 1</th> <th>Header 2</th> <th>Header 3</th> </tr> </thead> <tbody> <tr> <td>Row 1, Cell 1</td> <td>Row 1, Cell 2</td> <td>Row 1, Cell 3</td> </tr> <tr> <td>Row 2, Cell 1</td> <td>Row 2, Cell 2</td> <td>Row 2, Cell 3</td> </tr> </tbody> </table> <h2>Section 2: Media Content</h2> <p>This section contains an image and a video:</p> <img src=\\'example_image_link.mp4\\' alt=\\'Example Image\\'> <video controls width=\\'250\\' src=\\'example_video_link.mp4\\' type=\\'video/mp4\\'> Your browser does not support the video tag. </video> <h2>Section 3: Code Example</h2> <p>This section contains a code block:</p> <pre><code data-lang=\"html\"> &lt;div&gt; &lt;p&gt;This is a paragraph inside a div.&lt;/p&gt; &lt;/div&gt; </code></pre> <h2>Conclusion</h2> <p>This is the conclusion of the document.</p> </body> </html>\"\"\"'), Document(metadata={'Header 1': 'How to split HTML', 'Header 2': 'Using HTMLHeaderTextSplitter\\u200b'}, page_content='HTMLHeaderTextSplitter is a \"structure-aware\" text splitter that splits text at the HTML element level and adds metadata for each header \"relevant\" to any given chunk. It can return chunks element by element or combine elements with the same metadata, with the objectives of (a) keeping related text grouped (more or less) semantically and (b) preserving context-rich information encoded in document structures. It can be used with other text splitters as part of a chunking pipeline.  \\nIt is analogous to the MarkdownHeaderTextSplitter for markdown files.  \\nTo specify what headers to split on, specify headers_to_split_on when instantiating HTMLHeaderTextSplitter as shown below.  \\nfrom langchain_text_splitters import HTMLHeaderTextSplitterheaders_to_split_on = [ (\"h1\", \"Header 1\"), (\"h2\", \"Header 2\"), (\"h3\", \"Header 3\"),]html_splitter = HTMLHeaderTextSplitter(headers_to_split_on)html_header_splits = html_splitter.split_text(html_string)html_header_splits  \\nAPI Reference:HTMLHeaderTextSplitter  \\n[Document(metadata={\\'Header 1\\': \\'Main Title\\'}, page_content=\\'This is an introductory paragraph with some basic content.\\'), Document(metadata={\\'Header 1\\': \\'Main Title\\', \\'Header 2\\': \\'Section 1: Introduction\\'}, page_content=\\'This section introduces the topic. Below is a list: \\\\nFirst item Second item Third item with bold text and a link\\'), Document(metadata={\\'Header 1\\': \\'Main Title\\', \\'Header 2\\': \\'Section 1: Introduction\\', \\'Header 3\\': \\'Subsection 1.1: Details\\'}, page_content=\"This subsection provides additional details. Here\\'s a table:\"), Document(metadata={\\'Header 1\\': \\'Main Title\\', \\'Header 2\\': \\'Section 2: Media Content\\'}, page_content=\\'This section contains an image and a video:\\'), Document(metadata={\\'Header 1\\': \\'Main Title\\', \\'Header 2\\': \\'Section 3: Code Example\\'}, page_content=\\'This section contains a code block:\\'), Document(metadata={\\'Header 1\\': \\'Main Title\\', \\'Header 2\\': \\'Conclusion\\'}, page_content=\\'This is the conclusion of the document.\\')]  \\nTo return each element together with their associated headers, specify return_each_element=True when instantiating HTMLHeaderTextSplitter:  \\nhtml_splitter = HTMLHeaderTextSplitter( headers_to_split_on, return_each_element=True,)html_header_splits_elements = html_splitter.split_text(html_string)  \\nComparing with the above, where elements are aggregated by their headers:  \\nfor element in html_header_splits[:2]: print(element)  \\npage_content=\\'This is an introductory paragraph with some basic content.\\' metadata={\\'Header 1\\': \\'Main Title\\'}page_content=\\'This section introduces the topic. Below is a list: First item Second item Third item with bold text and a link\\' metadata={\\'Header 1\\': \\'Main Title\\', \\'Header 2\\': \\'Section 1: Introduction\\'}  \\nNow each element is returned as a distinct Document:  \\nfor element in html_header_splits_elements[:3]: print(element)  \\npage_content=\\'This is an introductory paragraph with some basic content.\\' metadata={\\'Header 1\\': \\'Main Title\\'}page_content=\\'This section introduces the topic. Below is a list:\\' metadata={\\'Header 1\\': \\'Main Title\\', \\'Header 2\\': \\'Section 1: Introduction\\'}page_content=\\'First item Second item Third item with bold text and a link\\' metadata={\\'Header 1\\': \\'Main Title\\', \\'Header 2\\': \\'Section 1: Introduction\\'}'), Document(metadata={'Header 1': 'How to split HTML', 'Header 2': 'Using HTMLHeaderTextSplitter\\u200b', 'Header 3': 'How to split from a URL or HTML file:\\u200b'}, page_content='To read directly from a URL, pass the URL string into the split_text_from_url method.  \\nSimilarly, a local HTML file can be passed to the split_text_from_file method.  \\nurl = \"https://plato.stanford.edu/entries/goedel/\"headers_to_split_on = [ (\"h1\", \"Header 1\"), (\"h2\", \"Header 2\"), (\"h3\", \"Header 3\"), (\"h4\", \"Header 4\"),]html_splitter = HTMLHeaderTextSplitter(headers_to_split_on)# for local file use html_splitter.split_text_from_file(<path_to_file>)html_header_splits = html_splitter.split_text_from_url(url)'), Document(metadata={'Header 1': 'How to split HTML', 'Header 2': 'Using HTMLHeaderTextSplitter\\u200b', 'Header 3': 'How to constrain chunk sizes:\\u200b'}, page_content=\"HTMLHeaderTextSplitter, which splits based on HTML headers, can be composed with another splitter which constrains splits based on character lengths, such as RecursiveCharacterTextSplitter.  \\nThis can be done using the .split_documents method of the second splitter:  \\nfrom langchain_text_splitters import RecursiveCharacterTextSplitterchunk_size = 500chunk_overlap = 30text_splitter = RecursiveCharacterTextSplitter( chunk_size=chunk_size, chunk_overlap=chunk_overlap)# Splitsplits = text_splitter.split_documents(html_header_splits)splits[80:85]  \\nAPI Reference:RecursiveCharacterTextSplitter  \\n[Document(metadata={'Header 1': 'Kurt GÃ¶del', 'Header 2': '2. GÃ¶delâ€™s Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}, page_content='We see that GÃ¶del first tried to reduce the consistency problem for analysis to that of arithmetic. This seemed to require a truth definition for arithmetic, which in turn led to paradoxes, such as the Liar paradox (â€œThis sentence is falseâ€) and Berryâ€™s paradox (â€œThe least number not defined by an expression consisting of just fourteen English wordsâ€). GÃ¶del then noticed that such paradoxes would not necessarily arise if truth were replaced by provability. But this means that arithmetic truth'), Document(metadata={'Header 1': 'Kurt GÃ¶del', 'Header 2': '2. GÃ¶delâ€™s Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}, page_content='means that arithmetic truth and arithmetic provability are not co-extensive â€” whence the First Incompleteness Theorem.'), Document(metadata={'Header 1': 'Kurt GÃ¶del', 'Header 2': '2. GÃ¶delâ€™s Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}, page_content='This account of GÃ¶delâ€™s discovery was told to Hao Wang very much after the fact; but in GÃ¶delâ€™s contemporary correspondence with Bernays and Zermelo, essentially the same description of his path to the theorems is given. (See GÃ¶del 2003a and GÃ¶del 2003b respectively.) From those accounts we see that the undefinability of truth in arithmetic, a result credited to Tarski, was likely obtained in some form by GÃ¶del by 1931. But he neither publicized nor published the result; the biases logicians'), Document(metadata={'Header 1': 'Kurt GÃ¶del', 'Header 2': '2. GÃ¶delâ€™s Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}, page_content='result; the biases logicians had expressed at the time concerning the notion of truth, biases which came vehemently to the fore when Tarski announced his results on the undefinability of truth in formal systems 1935, may have served as a deterrent to GÃ¶delâ€™s publication of that theorem.'), Document(metadata={'Header 1': 'Kurt GÃ¶del', 'Header 2': '2. GÃ¶delâ€™s Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.2 The proof of the First Incompleteness Theorem'}, page_content='We now describe the proof of the two theorems, formulating GÃ¶delâ€™s results in Peano arithmetic. GÃ¶del himself used a system related to that defined in Principia Mathematica, but containing Peano arithmetic. In our presentation of the First and Second Incompleteness Theorems we refer to Peano arithmetic as P, following GÃ¶delâ€™s notation.')]\"), Document(metadata={'Header 1': 'How to split HTML', 'Header 2': 'Using HTMLHeaderTextSplitter\\u200b', 'Header 3': 'Limitations\\u200b'}, page_content='There can be quite a bit of structural variation from one HTML document to another, and while HTMLHeaderTextSplitter will attempt to attach all \"relevant\" headers to any given chunk, it can sometimes miss certain headers. For example, the algorithm assumes an informational hierarchy in which headers are always at nodes \"above\" associated text, i.e. prior siblings, ancestors, and combinations thereof. In the following news article (as of the writing of this document), the document is structured such that the text of the top-level headline, while tagged \"h1\", is in a distinct subtree from the text elements that we\\'d expect it to be \"above\"â€”so we can observe that the \"h1\" element and its associated text do not show up in the chunk metadata (but, where applicable, we do see \"h2\" and its associated text):  \\nurl = \"https://www.cnn.com/2023/09/25/weather/el-nino-winter-us-climate/index.html\"headers_to_split_on = [ (\"h1\", \"Header 1\"), (\"h2\", \"Header 2\"),]html_splitter = HTMLHeaderTextSplitter(headers_to_split_on)html_header_splits = html_splitter.split_text_from_url(url)print(html_header_splits[1].page_content[:500])  \\nNo two El NiÃ±o winters are the same, but many have temperature and precipitation trends in common. Average conditions during an El NiÃ±o winter across the continental US. One of the major reasons is the position of the jet stream, which often shifts south during an El NiÃ±o winter. This shift typically brings wetter and cooler weather to the South while the North becomes drier and warmer, according to NOAA. Because the jet stream is essentially a river of air that storms flow through, they c'), Document(metadata={'Header 1': 'How to split HTML', 'Header 2': 'Using HTMLSectionSplitter\\u200b'}, page_content='Similar in concept to the HTMLHeaderTextSplitter, the HTMLSectionSplitter is a \"structure-aware\" text splitter that splits text at the element level and adds metadata for each header \"relevant\" to any given chunk. It lets you split HTML by sections.  \\nIt can return chunks element by element or combine elements with the same metadata, with the objectives of (a) keeping related text grouped (more or less) semantically and (b) preserving context-rich information encoded in document structures.  \\nUse xslt_path to provide an absolute path to transform the HTML so that it can detect sections based on provided tags. The default is to use the converting_to_header.xslt file in the data_connection/document_transformers directory. This is for converting the html to a format/layout that is easier to detect sections. For example, span based on their font size can be converted to header tags to be detected as a section.'), Document(metadata={'Header 1': 'How to split HTML', 'Header 2': 'Using HTMLSectionSplitter\\u200b', 'Header 3': 'How to split HTML strings:\\u200b'}, page_content='from langchain_text_splitters import HTMLSectionSplitterheaders_to_split_on = [ (\"h1\", \"Header 1\"), (\"h2\", \"Header 2\"),]html_splitter = HTMLSectionSplitter(headers_to_split_on)html_header_splits = html_splitter.split_text(html_string)html_header_splits  \\nAPI Reference:HTMLSectionSplitter  \\n[Document(metadata={\\'Header 1\\': \\'Main Title\\'}, page_content=\\'Main Title \\\\n This is an introductory paragraph with some basic content.\\'), Document(metadata={\\'Header 2\\': \\'Section 1: Introduction\\'}, page_content=\"Section 1: Introduction \\\\n This section introduces the topic. Below is a list: \\\\n \\\\n First item \\\\n Second item \\\\n Third item with bold text and a link \\\\n \\\\n \\\\n Subsection 1.1: Details \\\\n This subsection provides additional details. Here\\'s a table: \\\\n \\\\n \\\\n \\\\n Header 1 \\\\n Header 2 \\\\n Header 3 \\\\n \\\\n \\\\n \\\\n \\\\n Row 1, Cell 1 \\\\n Row 1, Cell 2 \\\\n Row 1, Cell 3 \\\\n \\\\n \\\\n Row 2, Cell 1 \\\\n Row 2, Cell 2 \\\\n Row 2, Cell 3\"), Document(metadata={\\'Header 2\\': \\'Section 2: Media Content\\'}, page_content=\\'Section 2: Media Content \\\\n This section contains an image and a video: \\\\n \\\\n \\\\n Your browser does not support the video tag.\\'), Document(metadata={\\'Header 2\\': \\'Section 3: Code Example\\'}, page_content=\\'Section 3: Code Example \\\\n This section contains a code block: \\\\n \\\\n <div>\\\\n <p>This is a paragraph inside a div.</p>\\\\n </div>\\'), Document(metadata={\\'Header 2\\': \\'Conclusion\\'}, page_content=\\'Conclusion \\\\n This is the conclusion of the document.\\')]'), Document(metadata={'Header 1': 'How to split HTML', 'Header 2': 'Using HTMLSectionSplitter\\u200b', 'Header 3': 'How to constrain chunk sizes:\\u200b'}, page_content='HTMLSectionSplitter can be used with other text splitters as part of a chunking pipeline. Internally, it uses the RecursiveCharacterTextSplitter when the section size is larger than the chunk size. It also considers the font size of the text to determine whether it is a section or not based on the determined font size threshold.  \\nfrom langchain_text_splitters import RecursiveCharacterTextSplitterheaders_to_split_on = [ (\"h1\", \"Header 1\"), (\"h2\", \"Header 2\"), (\"h3\", \"Header 3\"),]html_splitter = HTMLSectionSplitter(headers_to_split_on)html_header_splits = html_splitter.split_text(html_string)chunk_size = 50chunk_overlap = 5text_splitter = RecursiveCharacterTextSplitter( chunk_size=chunk_size, chunk_overlap=chunk_overlap)# Splitsplits = text_splitter.split_documents(html_header_splits)splits  \\nAPI Reference:RecursiveCharacterTextSplitter  \\n[Document(metadata={\\'Header 1\\': \\'Main Title\\'}, page_content=\\'Main Title\\'), Document(metadata={\\'Header 1\\': \\'Main Title\\'}, page_content=\\'This is an introductory paragraph with some\\'), Document(metadata={\\'Header 1\\': \\'Main Title\\'}, page_content=\\'some basic content.\\'), Document(metadata={\\'Header 2\\': \\'Section 1: Introduction\\'}, page_content=\\'Section 1: Introduction\\'), Document(metadata={\\'Header 2\\': \\'Section 1: Introduction\\'}, page_content=\\'This section introduces the topic. Below is a\\'), Document(metadata={\\'Header 2\\': \\'Section 1: Introduction\\'}, page_content=\\'is a list:\\'), Document(metadata={\\'Header 2\\': \\'Section 1: Introduction\\'}, page_content=\\'First item \\\\n Second item\\'), Document(metadata={\\'Header 2\\': \\'Section 1: Introduction\\'}, page_content=\\'Third item with bold text and a link\\'), Document(metadata={\\'Header 3\\': \\'Subsection 1.1: Details\\'}, page_content=\\'Subsection 1.1: Details\\'), Document(metadata={\\'Header 3\\': \\'Subsection 1.1: Details\\'}, page_content=\\'This subsection provides additional details.\\'), Document(metadata={\\'Header 3\\': \\'Subsection 1.1: Details\\'}, page_content=\"Here\\'s a table:\"), Document(metadata={\\'Header 3\\': \\'Subsection 1.1: Details\\'}, page_content=\\'Header 1 \\\\n Header 2 \\\\n Header 3\\'), Document(metadata={\\'Header 3\\': \\'Subsection 1.1: Details\\'}, page_content=\\'Row 1, Cell 1 \\\\n Row 1, Cell 2\\'), Document(metadata={\\'Header 3\\': \\'Subsection 1.1: Details\\'}, page_content=\\'Row 1, Cell 3 \\\\n \\\\n \\\\n Row 2, Cell 1\\'), Document(metadata={\\'Header 3\\': \\'Subsection 1.1: Details\\'}, page_content=\\'Row 2, Cell 2 \\\\n Row 2, Cell 3\\'), Document(metadata={\\'Header 2\\': \\'Section 2: Media Content\\'}, page_content=\\'Section 2: Media Content\\'), Document(metadata={\\'Header 2\\': \\'Section 2: Media Content\\'}, page_content=\\'This section contains an image and a video:\\'), Document(metadata={\\'Header 2\\': \\'Section 2: Media Content\\'}, page_content=\\'Your browser does not support the video\\'), Document(metadata={\\'Header 2\\': \\'Section 2: Media Content\\'}, page_content=\\'tag.\\'), Document(metadata={\\'Header 2\\': \\'Section 3: Code Example\\'}, page_content=\\'Section 3: Code Example\\'), Document(metadata={\\'Header 2\\': \\'Section 3: Code Example\\'}, page_content=\\'This section contains a code block: \\\\n \\\\n <div>\\'), Document(metadata={\\'Header 2\\': \\'Section 3: Code Example\\'}, page_content=\\'<p>This is a paragraph inside a div.</p>\\'), Document(metadata={\\'Header 2\\': \\'Section 3: Code Example\\'}, page_content=\\'</div>\\'), Document(metadata={\\'Header 2\\': \\'Conclusion\\'}, page_content=\\'Conclusion\\'), Document(metadata={\\'Header 2\\': \\'Conclusion\\'}, page_content=\\'This is the conclusion of the document.\\')]'), Document(metadata={'Header 1': 'How to split HTML', 'Header 2': 'Using HTMLSemanticPreservingSplitter\\u200b'}, page_content='The HTMLSemanticPreservingSplitter is designed to split HTML content into manageable chunks while preserving the semantic structure of important elements like tables, lists, and other HTML components. This ensures that such elements are not split across chunks, causing loss of contextual relevancy such as table headers, list headers etc.  \\nThis splitter is designed at its heart, to create contextually relevant chunks. General Recursive splitting with HTMLHeaderTextSplitter can cause tables, lists and other structered elements to be split in the middle, losing signifcant context and creating bad chunks.  \\nThe HTMLSemanticPreservingSplitter is essential for splitting HTML content that includes structured elements like tables and lists, especially when it\\'s critical to preserve these elements intact. Additionally, its ability to define custom handlers for specific HTML tags makes it a versatile tool for processing complex HTML documents.  \\nIMPORTANT: max_chunk_size is not a definite maximum size of a chunk, the calculation of max size, occurs when the preserved content is not apart of the chunk, to ensure it is not split. When we add the preserved data back in to the chunk, there is a chance the chunk size will exceed the max_chunk_size. This is crucial to ensure we maintain the structure of the original document  \\ninfo  \\nNotes:  \\nWe have defined a custom handler to re-format the contents of code blocks We defined a deny list for specific html elements, to decompose them and their contents pre-processing We have intentionally set a small chunk size to demonstrate the non-splitting of elements  \\n# BeautifulSoup is required to use the custom handlersfrom bs4 import Tagfrom langchain_text_splitters import HTMLSemanticPreservingSplitterheaders_to_split_on = [ (\"h1\", \"Header 1\"), (\"h2\", \"Header 2\"),]def code_handler(element: Tag) -> str: data_lang = element.get(\"data-lang\") code_format = f\"<code:{data_lang}>{element.get_text()}</code>\" return code_formatsplitter = HTMLSemanticPreservingSplitter( headers_to_split_on=headers_to_split_on, separators=[\"\\\\n\\\\n\", \"\\\\n\", \". \", \"! \", \"? \"], max_chunk_size=50, preserve_images=True, preserve_videos=True, elements_to_preserve=[\"table\", \"ul\", \"ol\", \"code\"], denylist_tags=[\"script\", \"style\", \"head\"], custom_handlers={\"code\": code_handler},)documents = splitter.split_text(html_string)documents  \\nAPI Reference:HTMLSemanticPreservingSplitter  \\n[Document(metadata={\\'Header 1\\': \\'Main Title\\'}, page_content=\\'This is an introductory paragraph with some basic content.\\'), Document(metadata={\\'Header 2\\': \\'Section 1: Introduction\\'}, page_content=\\'This section introduces the topic\\'), Document(metadata={\\'Header 2\\': \\'Section 1: Introduction\\'}, page_content=\\'. Below is a list: First item Second item Third item with bold text and a link Subsection 1.1: Details This subsection provides additional details\\'), Document(metadata={\\'Header 2\\': \\'Section 1: Introduction\\'}, page_content=\". Here\\'s a table: Header 1 Header 2 Header 3 Row 1, Cell 1 Row 1, Cell 2 Row 1, Cell 3 Row 2, Cell 1 Row 2, Cell 2 Row 2, Cell 3\"), Document(metadata={\\'Header 2\\': \\'Section 2: Media Content\\'}, page_content=\\'This section contains an image and a video: ![image:example_image_link.mp4](example_image_link.mp4) ![video:example_video_link.mp4](example_video_link.mp4)\\'), Document(metadata={\\'Header 2\\': \\'Section 3: Code Example\\'}, page_content=\\'This section contains a code block: <code:html> <div> <p>This is a paragraph inside a div.</p> </div> </code>\\'), Document(metadata={\\'Header 2\\': \\'Conclusion\\'}, page_content=\\'This is the conclusion of the document.\\')]'), Document(metadata={'Header 1': 'How to split HTML', 'Header 2': 'Using HTMLSemanticPreservingSplitter\\u200b', 'Header 3': 'Preserving Tables and Lists\\u200b'}, page_content='In this example, we will demonstrate how the HTMLSemanticPreservingSplitter can preserve a table and a large list within an HTML document. The chunk size will be set to 50 characters to illustrate how the splitter ensures that these elements are not split, even when they exceed the maximum defined chunk size.  \\nfrom langchain_text_splitters import HTMLSemanticPreservingSplitterhtml_string = \"\"\"<!DOCTYPE html><html> <body> <div> <h1>Section 1</h1> <p>This section contains an important table and list that should not be split across chunks.</p> <table> <tr> <th>Item</th> <th>Quantity</th> <th>Price</th> </tr> <tr> <td>Apples</td> <td>10</td> <td>$1.00</td> </tr> <tr> <td>Oranges</td> <td>5</td> <td>$0.50</td> </tr> <tr> <td>Bananas</td> <td>50</td> <td>$1.50</td> </tr> </table> <h2>Subsection 1.1</h2> <p>Additional text in subsection 1.1 that is separated from the table and list.</p> <p>Here is a detailed list:</p> <ul> <li>Item 1: Description of item 1, which is quite detailed and important.</li> <li>Item 2: Description of item 2, which also contains significant information.</li> <li>Item 3: Description of item 3, another item that we don\\'t want to split across chunks.</li> </ul> </div> </body></html>\"\"\"headers_to_split_on = [(\"h1\", \"Header 1\"), (\"h2\", \"Header 2\")]splitter = HTMLSemanticPreservingSplitter( headers_to_split_on=headers_to_split_on, max_chunk_size=50, elements_to_preserve=[\"table\", \"ul\"],)documents = splitter.split_text(html_string)print(documents)  \\nAPI Reference:HTMLSemanticPreservingSplitter  \\n[Document(metadata={\\'Header 1\\': \\'Section 1\\'}, page_content=\\'This section contains an important table and list\\'), Document(metadata={\\'Header 1\\': \\'Section 1\\'}, page_content=\\'that should not be split across chunks.\\'), Document(metadata={\\'Header 1\\': \\'Section 1\\'}, page_content=\\'Item Quantity Price Apples 10 $1.00 Oranges 5 $0.50 Bananas 50 $1.50\\'), Document(metadata={\\'Header 2\\': \\'Subsection 1.1\\'}, page_content=\\'Additional text in subsection 1.1 that is\\'), Document(metadata={\\'Header 2\\': \\'Subsection 1.1\\'}, page_content=\\'separated from the table and list. Here is a\\'), Document(metadata={\\'Header 2\\': \\'Subsection 1.1\\'}, page_content=\"detailed list: Item 1: Description of item 1, which is quite detailed and important. Item 2: Description of item 2, which also contains significant information. Item 3: Description of item 3, another item that we don\\'t want to split across chunks.\")]  \\nIn this example, the HTMLSemanticPreservingSplitter ensures that the entire table and the unordered list (<ul>) are preserved within their respective chunks. Even though the chunk size is set to 50 characters, the splitter recognizes that these elements should not be split and keeps them intact.  \\nThis is particularly important when dealing with data tables or lists, where splitting the content could lead to loss of context or confusion. The resulting Document objects retain the full structure of these elements, ensuring that the contextual relevance of the information is maintained.'), Document(metadata={'Header 1': 'How to split HTML', 'Header 2': 'Using HTMLSemanticPreservingSplitter\\u200b', 'Header 3': 'Using a Custom Handler\\u200b'}, page_content='The HTMLSemanticPreservingSplitter allows you to define custom handlers for specific HTML elements. Some platforms, have custom HTML tags that are not natively parsed by BeautifulSoup, when this occurs, you can utilize custom handlers to add the formatting logic easily.  \\nThis can be particularly useful for elements that require special processing, such as <iframe> tags or specific \\'data-\\' elements. In this example, we\\'ll create a custom handler for iframe tags that converts them into Markdown-like links.  \\ndef custom_iframe_extractor(iframe_tag): iframe_src = iframe_tag.get(\"src\", \"\") return f\"[iframe:{iframe_src}]({iframe_src})\"splitter = HTMLSemanticPreservingSplitter( headers_to_split_on=headers_to_split_on, max_chunk_size=50, separators=[\"\\\\n\\\\n\", \"\\\\n\", \". \"], elements_to_preserve=[\"table\", \"ul\", \"ol\"], custom_handlers={\"iframe\": custom_iframe_extractor},)html_string = \"\"\"<!DOCTYPE html><html> <body> <div> <h1>Section with Iframe</h1> <iframe src=\"https://example.com/embed\"></iframe> <p>Some text after the iframe.</p> <ul> <li>Item 1: Description of item 1, which is quite detailed and important.</li> <li>Item 2: Description of item 2, which also contains significant information.</li> <li>Item 3: Description of item 3, another item that we don\\'t want to split across chunks.</li> </ul> </div> </body></html>\"\"\"documents = splitter.split_text(html_string)print(documents)  \\n[Document(metadata={\\'Header 1\\': \\'Section with Iframe\\'}, page_content=\\'[iframe:https://example.com/embed](https://example.com/embed) Some text after the iframe\\'), Document(metadata={\\'Header 1\\': \\'Section with Iframe\\'}, page_content=\". Item 1: Description of item 1, which is quite detailed and important. Item 2: Description of item 2, which also contains significant information. Item 3: Description of item 3, another item that we don\\'t want to split across chunks.\")]  \\nIn this example, we defined a custom handler for iframe tags that converts them into Markdown-like links. When the splitter processes the HTML content, it uses this custom handler to transform the iframe tags while preserving other elements like tables and lists. The resulting Document objects show how the iframe is handled according to the custom logic you provided.  \\nImportant: When presvering items such as links, you should be mindful not to include . in your seperators, or leave seperators blank. RecursiveCharacterTextSplitter splits on full stop, which will cut links in half. Ensure you provide a seperator list with . instead.'), Document(metadata={'Header 1': 'How to split HTML', 'Header 2': 'Using HTMLSemanticPreservingSplitter\\u200b', 'Header 3': 'Using a custom handler to analyze an image with an LLM\\u200b'}, page_content='With custom handler\\'s, we can also override the default processing for any element. A great example of this, is inserting semantic analysis of an image within a document, directly in the chunking flow.  \\nSince our function is called when the tag is discovered, we can override the <img> tag and turn off preserve_images to insert any content we would like to embed in our chunks.  \\n\"\"\"This example assumes you have helper methods `load_image_from_url` and an LLM agent `llm` that can process image data.\"\"\"from langchain.agents import AgentExecutor# This example needs to be replaced with your own agentllm = AgentExecutor(...)# This method is a placeholder for loading image data from a URL and is not implemented heredef load_image_from_url(image_url: str) -> bytes: # Assuming this method fetches the image data from the URL return b\"image_data\"html_string = \"\"\"<!DOCTYPE html><html> <body> <div> <h1>Section with Image and Link</h1> <p> <img src=\"https://example.com/image.jpg\" alt=\"An example image\" /> Some text after the image. </p> <ul> <li>Item 1: Description of item 1, which is quite detailed and important.</li> <li>Item 2: Description of item 2, which also contains significant information.</li> <li>Item 3: Description of item 3, another item that we don\\'t want to split across chunks.</li> </ul> </div> </body></html>\"\"\"def custom_image_handler(img_tag) -> str: img_src = img_tag.get(\"src\", \"\") img_alt = img_tag.get(\"alt\", \"No alt text provided\") image_data = load_image_from_url(img_src) semantic_meaning = llm.invoke(image_data) markdown_text = f\"[Image Alt Text: {img_alt} | Image Source: {img_src} | Image Semantic Meaning: {semantic_meaning}]\" return markdown_textsplitter = HTMLSemanticPreservingSplitter( headers_to_split_on=headers_to_split_on, max_chunk_size=50, separators=[\"\\\\n\\\\n\", \"\\\\n\", \". \"], elements_to_preserve=[\"ul\"], preserve_images=False, custom_handlers={\"img\": custom_image_handler},)documents = splitter.split_text(html_string)print(documents)  \\nAPI Reference:AgentExecutor  \\n[Document(metadata={\\'Header 1\\': \\'Section with Image and Link\\'}, page_content=\\'[Image Alt Text: An example image | Image Source: https://example.com/image.jpg | Image Semantic Meaning: semantic-meaning] Some text after the image\\'), Document(metadata={\\'Header 1\\': \\'Section with Image and Link\\'}, page_content=\". Item 1: Description of item 1, which is quite detailed and important. Item 2: Description of item 2, which also contains significant information. Item 3: Description of item 3, another item that we don\\'t want to split across chunks.\")]  \\nWith our custom handler written to extract the specific fields from a <img> element in HTML, we can further process the data with our agent, and insert the result directly into our chunk. It is important to ensure preserve_images is set to False otherwise the default processing of <img> fields will take place.'), Document(metadata={}, page_content='Edit this page  \\nWas this page helpful?  \\nPrevious  \\nHow to split text by tokens  \\nNext  \\nHow to do question answering over CSVs  \\nOverview of the SplittersExample HTML DocumentUsing HTMLHeaderTextSplitterUsing HTMLSectionSplitterUsing HTMLSemanticPreservingSplitter  \\nHTMLHeaderTextSplitterHTMLSectionSplitterHTMLSemanticPreservingSplitterChoosing the Right Splitter  \\nHow to split from a URL or HTML file:How to constrain chunk sizes:Limitations  \\nHow to split HTML strings:How to constrain chunk sizes:  \\nPreserving Tables and ListsUsing a Custom HandlerUsing a custom handler to analyze an image with an LLM  \\nCommunity  \\nTwitter  \\nGitHub  \\nOrganizationPythonJS/TS  \\nMore  \\nHomepageBlogYouTube  \\nCopyright Â© 2025 LangChain, Inc.')]\n"
     ]
    }
   ],
   "source": [
    "#method 2\n",
    "\n",
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "url=\"https://python.langchain.com/docs/how_to/split_html/#choosing-the-right-splitter\"\n",
    "\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on)\n",
    "html_header_splits = html_splitter.split_text_from_url(url)\n",
    "print(html_header_splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'openapi': '3.1.0', 'info': {'title': 'LangSmith', 'version': '0.1.0'}, 'paths': {'/api/v1/sessions/{session_id}': {'get': {'tags': ['tracer-sessions'], 'summary': 'Read Tracer Session', 'description': 'Get a specific session.'}}}}\n",
      "{'paths': {'/api/v1/sessions/{session_id}': {'get': {'operationId': 'read_tracer_session_api_v1_sessions__session_id__get', 'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}}}}\n",
      "{'paths': {'/api/v1/sessions/{session_id}': {'get': {'parameters': [{'name': 'session_id', 'in': 'path', 'required': True, 'schema': {'type': 'string', 'format': 'uuid', 'title': 'Session Id'}}, {'name': 'include_stats', 'in': 'query', 'required': False, 'schema': {'type': 'boolean', 'default': False, 'title': 'Include Stats'}}, {'name': 'accept', 'in': 'header', 'required': False, 'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Accept'}}]}}}}\n",
      "page_content='{\"openapi\": \"3.1.0\", \"info\": {\"title\": \"LangSmith\", \"version\": \"0.1.0\"}, \"paths\": {\"/api/v1/sessions/{session_id}\": {\"get\": {\"tags\": [\"tracer-sessions\"], \"summary\": \"Read Tracer Session\", \"description\": \"Get a specific session.\"}}}}'\n",
      "page_content='{\"paths\": {\"/api/v1/sessions/{session_id}\": {\"get\": {\"operationId\": \"read_tracer_session_api_v1_sessions__session_id__get\", \"security\": [{\"API Key\": []}, {\"Tenant ID\": []}, {\"Bearer Auth\": []}]}}}}'\n",
      "page_content='{\"paths\": {\"/api/v1/sessions/{session_id}\": {\"get\": {\"parameters\": [{\"name\": \"session_id\", \"in\": \"path\", \"required\": true, \"schema\": {\"type\": \"string\", \"format\": \"uuid\", \"title\": \"Session Id\"}}, {\"name\": \"include_stats\", \"in\": \"query\", \"required\": false, \"schema\": {\"type\": \"boolean\", \"default\": false, \"title\": \"Include Stats\"}}, {\"name\": \"accept\", \"in\": \"header\", \"required\": false, \"schema\": {\"anyOf\": [{\"type\": \"string\"}, {\"type\": \"null\"}], \"title\": \"Accept\"}}]}}}}'\n",
      "{\"openapi\": \"3.1.0\", \"info\": {\"title\": \"LangSmith\", \"version\": \"0.1.0\"}, \"paths\": {\"/api/v1/sessions/{session_id}\": {\"get\": {\"tags\": [\"tracer-sessions\"], \"summary\": \"Read Tracer Session\", \"description\": \"Get a specific session.\"}}}}\n",
      "{\"paths\": {\"/api/v1/sessions/{session_id}\": {\"get\": {\"operationId\": \"read_tracer_session_api_v1_sessions__session_id__get\", \"security\": [{\"API Key\": []}, {\"Tenant ID\": []}, {\"Bearer Auth\": []}]}}}}\n"
     ]
    }
   ],
   "source": [
    "#https://python.langchain.com/docs/how_to/recursive_json_splitter/\n",
    "import json\n",
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "import requests\n",
    "\n",
    "# This is a large nested json object and will be loaded as a python dict\n",
    "json_data = requests.get(\"https://api.smith.langchain.com/openapi.json\").json()\n",
    "\n",
    "\n",
    "splitter = RecursiveJsonSplitter(max_chunk_size=300)\n",
    "\n",
    "# Recursively split json data - If you need to access/manipulate the smaller json chunks\n",
    "json_chunks = splitter.split_json(json_data=json_data)\n",
    "\n",
    "#printing top 3 chunks\n",
    "for chunk in json_chunks[:3]:\n",
    "    print(chunk)\n",
    "\n",
    "# The splitter can also output documents\n",
    "docs = splitter.create_documents(texts=[json_data])\n",
    "\n",
    "for doc in docs[:3]:\n",
    "    print(doc)\n",
    "\n",
    "#Or use .split_text to obtain string content directly:\n",
    "texts = splitter.split_text(json_data=json_data)\n",
    "\n",
    "print(texts[0])\n",
    "print(texts[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert text to vectors\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.embeddings.Embeddings object at 0x000002A3C51F8B50> async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x000002A3C621DCD0> model='text-embedding-3-small' dimensions=None deployment='text-embedding-ada-002' openai_api_version=None openai_api_base=None openai_api_type=None openai_proxy=None embedding_ctx_length=8191 openai_api_key=SecretStr('**********') openai_organization=None allowed_special=None disallowed_special=None chunk_size=1000 max_retries=2 request_timeout=None headers=None tiktoken_enabled=True tiktoken_model_name=None show_progress_bar=False model_kwargs={} skip_empty=False default_headers=None default_query=None retry_min_seconds=4 retry_max_seconds=20 http_client=None http_async_client=None check_embedding_ctx_length=True\n",
      "[-0.0018429403426125646, -0.0545056127011776, 0.023987380787730217, -0.0035938103683292866, -0.0175915639847517, -0.054554715752601624, 0.04114928096532822, 0.054260093718767166, -0.040240854024887085, -0.026589900255203247, 0.0014884697739034891, -0.03871862590312958, -0.0189542043954134, 0.001308165374211967, 0.01399468444287777, 0.04758192598819733, -0.004907346796244383, -0.00720603484660387, 0.014338413253426552, 0.0006368195172399282, 0.0216426569968462, -0.01912606880068779, 0.048318490386009216, 0.011177333071827888, 0.002333981916308403, -0.022772053256630898, 0.010041800327599049, -0.04085465893149376, -0.0015621259808540344, -0.0988466665148735, 0.024453869089484215, -0.027080941945314407, 0.0011623874306678772, 0.012767080217599869, -0.0022004800848662853, -0.01428930927067995, 0.014473450370132923, 0.01912606880068779, -0.03493760526180267, 0.006184054538607597, 0.015246840193867683, -0.02717914991080761, 0.002100737066939473, 0.06579957157373428, 0.02019408345222473, -0.024552078917622566, -0.10046710073947906, -0.027964817360043526, 0.0080162538215518, 0.014780350960791111, 0.005002486053854227, 0.013675507158041, 0.007291967049241066, 0.06830388307571411, 0.020721953362226486, -0.055536799132823944, 0.038374897092580795, 0.031132034957408905, -0.004450064152479172, 0.015246840193867683, -0.034299254417419434, -0.05789380148053169, -0.00862391758710146, 0.007881216704845428, -0.04883408173918724, -0.03555141016840935, -0.02710549347102642, 0.030051743611693382, -0.026000650599598885, -0.043899115175008774, 0.028283994644880295, 0.016805896535515785, -0.020856989547610283, 0.014964491128921509, -0.006223951932042837, 0.0163639597594738, 0.004772310145199299, -0.009906763210892677, 0.04205770790576935, 0.02124982327222824, -0.03596879541873932, 0.040486376732587814, 0.0037564679514616728, -0.025411400943994522, -0.06869671493768692, 0.03181949257850647, -0.09319968521595001, 0.0488586351275444, -0.033489033579826355, 0.0017969051841646433, -0.016732241958379745, 0.03184404596686363, -0.0416894294321537, 0.018193090334534645, 0.009372755885124207, 0.013933304697275162, -0.025030843913555145, 0.01379826758056879, 0.045126721262931824, -0.010980917140841484, 0.01773887686431408, 0.015136356465518475, 0.048883188515901566, 0.026098858565092087, -0.0032347363885492086, -0.041542116552591324, 0.011343060061335564, -0.027203703299164772, 0.004137025214731693, 0.04925146698951721, -0.10036889463663101, -0.05209951102733612, 0.026319827884435654, 0.0441446378827095, 0.010465322993695736, 0.04971795901656151, 0.07439279556274414, -0.010029523633420467, -0.043506283313035965, -0.0012375782243907452, -0.04070734605193138, 0.021925006061792374, 0.006892995908856392, -0.045200373977422714, -0.061969444155693054, -0.024220624938607216, 0.03157397359609604, -0.02514132857322693, -0.0714956521987915, -0.006236227694898844, 0.026639005169272423, -0.03486395254731178, -0.0049165538512170315, -0.0333908274769783, -0.008151290006935596, -0.016658585518598557, -0.022661568596959114, 0.03228598088026047, -0.04085465893149376, -0.01588519476354122, -0.0014155807439237833, -0.014264757744967937, -0.0008432104368694127, -0.021127063781023026, 0.04483209550380707, -0.044390156865119934, 0.040486376732587814, 0.03582148253917694, -0.02230556309223175, -0.016474444419145584, 0.006708855275064707, 0.04475843906402588, 0.006788649596273899, 0.013491366989910603, 0.02506767213344574, -0.041885845363140106, 0.0036981566809117794, -0.00433651078492403, 0.016560377553105354, 0.04598604142665863, 0.02271067164838314, -0.008924680761992931, 0.003403531853109598, -0.0013396227732300758, -0.012865289114415646, 0.04581417888402939, -0.041542116552591324, 0.014547105878591537, -0.03938153386116028, -0.007825975306332111, -0.012361971661448479, -0.005782014224678278, -0.0008033132762648165, -0.03321896120905876, -0.013675507158041, 0.012963497079908848, 0.04662439599633217, -0.045298583805561066, -0.027522878721356392, 0.028382202610373497, 0.0009022888843901455, 0.053916364908218384, 0.005511941388249397, 0.024760771542787552, 0.001730921445414424, 0.024392489343881607, 0.04200860485434532, -0.007991701364517212, -0.02148306742310524, 0.026074307039380074, 0.004238302353769541, 0.082053042948246, -0.004514513537287712, 0.00850115716457367, -0.0035784654319286346, -0.036582596600055695, 0.050675489008426666, -0.010103180073201656, -0.008936956524848938, -0.029437942430377007, -0.0037196397315710783, -0.026000650599598885, 0.012294452637434006, 0.0010089369025081396, -0.046280667185783386, -0.025165880098938942, -0.024723943322896957, 0.02115161530673504, 0.010262768715620041, -0.005539562553167343, 0.039602503180503845, 0.02075878158211708, 0.044070981442928314, -0.03037092089653015, -0.0204150527715683, -0.00879578199237585, -0.015897469595074654, 0.01001110952347517, 0.008973784744739532, -0.006807063706219196, -0.014264757744967937, 0.0452740304172039, 0.06452286243438721, 0.04338352009654045, 0.022440599277615547, 0.0015122545883059502, -0.0011915430659428239, 0.013724612072110176, 0.017849361523985863, -0.03820303454995155, -0.02207231894135475, 0.01074153371155262, -0.006242366041988134, -0.011318507604300976, 0.007107826415449381, 0.013577299192547798, 0.01644989289343357, 0.019997667521238327, -0.026982733979821205, -0.022845707833766937, -0.0060152593068778515, -0.00143476203083992, 0.05308159440755844, 0.02139713615179062, 0.009869934991002083, -0.019334761425852776, -0.019924011081457138, -0.01448572613298893, -0.00032589046168141067, 0.03371000289916992, 0.0008976853569038212, -0.004572824575006962, -0.02393827587366104, -0.020476432517170906, -0.035109471529722214, -0.0255587138235569, -0.015774710103869438, 0.004815276246517897, 0.04893229156732559, 0.021348031237721443, 0.0028403685428202152, -0.024699389934539795, 0.03493760526180267, -0.05499665439128876, -0.01196299958974123, -0.003603017423301935, -0.0019380797166377306, 0.06005438417196274, 0.037564679980278015, -0.023312197998166084, -0.02133575640618801, 0.003547775326296687, -0.005944672040641308, -0.02938883751630783, -0.015516913495957851, 0.009581448510289192, -0.0004419374163262546, -0.0073901754803955555, -0.039602503180503845, -0.01904013566672802, -0.022158250212669373, -0.0010143077233806252, 0.019666215404868126, 0.024687115103006363, -0.04303979128599167, 0.0009107286459766328, 0.0605454258620739, 0.008722125552594662, -0.0038331930991262197, 0.00045958420378156006, 0.030174503102898598, -0.05858125910162926, -0.0017953707138076425, -0.0071016885340213776, -0.00023497105576097965, 0.003741122782230377, -0.027277357876300812, 0.030493680387735367, 0.026737213134765625, 0.01824219338595867, 0.02506767213344574, 0.054161883890628815, -0.017149627208709717, 0.0025887098163366318, -0.020316844806075096, 0.00826177466660738, 0.04173853248357773, -0.017468802630901337, 0.0013925632229074836, 0.016621757298707962, -0.03208956494927406, -0.010606497526168823, 0.016400787979364395, -0.023975104093551636, 0.0038454693276435137, -0.02392599917948246, 0.029339732602238655, 0.03771199285984039, -0.0027360222302377224, -0.02646714076399803, 0.045863281935453415, -0.05568411201238632, 0.0049196225591003895, -0.034642983227968216, -0.009685794822871685, 0.0020547020249068737, 0.020856989547610283, 0.015553741715848446, -0.027817504480481148, 0.013601851649582386, 0.047827448695898056, -0.06972790509462357, -0.0061564333736896515, 0.018733235076069832, -0.010477599687874317, 0.032506950199604034, -0.013086257502436638, 0.03150031715631485, -0.030812857672572136, 0.006745683494955301, 0.013012601062655449, -0.04466022923588753, -0.0031488039530813694, 0.0007565108826383948, 0.02115161530673504, 0.013417710550129414, 0.01411744486540556, 0.033243514597415924, -0.03727005422115326, 0.03645983710885048, -0.008574813604354858, -0.004833690356463194, -0.006800925824791193, 0.004689447116106749, 0.001936545129865408, -0.09118641912937164, 0.033489033579826355, -0.032752472907304764, -0.028308546170592308, -0.0267863180488348, -0.031868595629930496, -0.03083740919828415, -0.004483823198825121, 0.02742467075586319, 0.03761378303170204, 0.02352089062333107, -0.0416894294321537, -0.02784205600619316, 0.03019905649125576, 0.003891504369676113, -0.017714323475956917, -0.02010815218091011, 0.012306729331612587, 0.007598868105560541, -0.02311578206717968, -0.0040787141770124435, -0.010373253375291824, -0.010146146640181541, 0.01233741920441389, 0.04365359619259834, 0.0771426260471344, 0.06113467365503311, 0.006561542861163616, -0.020169531926512718, 0.015971126034855843, 0.028283994644880295, -0.0012490869266912341, -0.01799667254090309, -0.014313861727714539, 0.030960170552134514, 0.043088898062705994, -0.02019408345222473, -0.008108323439955711, 0.03238419070839882, 0.04402187466621399, -0.007138516753911972, 0.07478562742471695, 0.00445927120745182, -0.015148632228374481, -0.00143476203083992, 0.012392661534249783, -0.018119433894753456, 0.013123085722327232, 0.0005175117985345423, -0.01290211733430624, 0.0545056127011776, -0.023398131132125854, 0.01571333035826683, 0.025190431624650955, -0.02703183703124523, -0.02368047833442688, 0.005775876343250275, 0.021593552082777023, 0.001681817346252501, 0.07674979418516159, -0.012828460894525051, -0.0035661894362419844, 0.02833309769630432, 0.009612138383090496, -0.027130046859383583, 0.019469797611236572, -0.026909077540040016, -0.061085570603609085, -0.0055457004345953465, -0.03729460760951042, 0.02938883751630783, 0.03302254527807236, -0.044242843985557556, 0.021127063781023026, -0.07350891828536987, 0.0023723444901406765, 0.011085263453423977, 0.016007954254746437, 0.06697806715965271, -0.012183968909084797, -0.03150031715631485, -0.028455859050154686, 0.029020555317401886, -0.022268734872341156, -0.006113467272371054, -0.014620762318372726, 0.021176166832447052, 0.0026945904828608036, -0.06088915467262268, -0.009814693592488766, -0.04006899148225784, -0.014424345456063747, 0.031451210379600525, -0.03417649120092392, 0.028357649222016335, 0.04011809453368187, -0.03977436572313309, -0.012988049536943436, -0.02067285031080246, -0.015922022983431816, -0.019248828291893005, -0.0033390827011317015, -0.0008455122006125748, 0.050086237490177155, 0.030665544793009758, 0.025116775184869766, -0.007875079289078712, 0.040314510464668274, 0.0096428282558918, 0.02066057361662388, -0.019015584141016006, -0.06177302822470665, -0.0071016885340213776, 0.006208606529980898, -0.04760647937655449, 0.0023431889712810516, 0.01379826758056879, -0.005263351835310459, 0.012386523187160492, -0.02440476603806019, -0.01960483379662037, -0.01033642515540123, -0.05175578221678734, -0.012681148014962673, 0.00418919837102294, 0.01602023094892502, -0.004575893748551607, 0.00899219885468483, 0.04802386462688446, 0.029167868196964264, 0.04615790769457817, 0.012429489754140377, 0.04961974918842316, -0.04355538636445999, -0.006245434749871492, 0.05190309137105942, 0.05077369883656502, 0.04124749079346657, -0.012134864926338196, -0.005168212577700615, -0.0315985232591629, -0.008267912082374096, -0.026025202125310898, 0.016719965264201164, -0.014141997322440147, -0.02457663044333458, 0.005689944140613079, 0.004594307392835617, 0.043751802295446396, 0.022440599277615547, 0.016658585518598557, 0.01611843891441822, -0.018450886011123657, 0.049595195800065994, 0.011570166796445847, -0.002237308071926236, 0.019518902525305748, -0.03199135884642601, -0.007359485607594252, -0.014989043585956097, 0.0049165538512170315, 0.025509608909487724, 0.009084268473088741, 0.00855026114732027, -0.0028326960746198893, 0.024527525529265404, 0.007979425601661205, -0.019494349136948586, -0.012036656029522419, -0.02498173899948597, 0.013687783852219582, 0.023901447653770447, -0.002568761119619012, -0.022686120122671127, -0.00365212163887918, 0.015443257056176662, -0.018094882369041443, 0.050675489008426666, 0.018598198890686035, 0.012466317974030972, -0.015210011973977089, 0.011895481497049332, -0.01404378842562437, 0.010723120532929897, -0.0038025029934942722, 0.04817117750644684, -0.011011607013642788, -0.021053407341241837, 0.023128056898713112, -0.02703183703124523, 0.011576305143535137, 0.03427470102906227, -0.021016579121351242, -0.0012774752685800195, 0.036582596600055695, 0.02938883751630783, 0.01428930927067995, 0.015406428836286068, -0.04011809453368187, -0.006543128751218319, -0.03636162728071213, -0.02929062955081463, 0.003790226997807622, -0.010532841086387634, 0.014313861727714539, -0.004468478262424469, 0.009102682583034039, 0.0066290609538555145, -0.00891854241490364, -0.012742528691887856, 0.0187209602445364, -0.04672260582447052, -0.039037805050611496, 0.013233570381999016, 0.01557829324156046, -0.04335897043347359, -0.003922194242477417, -0.023557718843221664, 0.009249995462596416, 0.026393484324216843, -0.022600186988711357, -0.0367790125310421, 0.023091228678822517, 0.02636893093585968, -0.0017217145068570971, 0.013282674364745617, -0.014056065119802952, 0.01534504909068346, 0.0007661015843041241, -0.04085465893149376, 0.034962158650159836, -0.02514132857322693, 0.017468802630901337, 0.008102186024188995, 0.011735892854630947, 0.003336013527587056, -0.054702028632164, 0.054161883890628815, 0.04539679363369942, 0.023005297407507896, 0.0023355165030807257, -0.02995353564620018, -0.02359454706311226, 0.02124982327222824, -0.007746180519461632, 0.03947973996400833, 0.014915387146174908, -0.01766522042453289, -0.04949698969721794, 0.029609806835651398, -0.000311696290737018, -0.007365623489022255, 0.014817179180681705, 0.006187123712152243, -0.004268992692232132, -0.003185632172971964, 0.0021099441219121218, 0.006647475063800812, -0.022526532411575317, 0.00377488206140697, 0.0006153364665806293, 0.00903516449034214, 0.020488709211349487, 0.009532344527542591, 0.02009587548673153, -0.0043058209121227264, -0.010084765963256359, 0.007911907508969307, 0.03181949257850647, 0.02353316731750965, 0.020365947857499123, -0.017775705084204674, -0.00903516449034214, -0.017051417380571365, -0.022182801738381386, 0.030420023947954178, 0.016388511285185814, 0.029266076162457466, -0.012717976234853268, -0.032605160027742386, -0.004692515823990107, 0.026810869574546814, 0.031451210379600525, -0.001486168010160327, -0.004563617520034313, -0.0013472952414304018, -0.026835421100258827, 0.017063694074749947, -0.010244354605674744, 0.062460485845804214, 0.002065443666651845, -0.005023968871682882, 0.0037104326765984297, -0.015246840193867683, -0.036828115582466125, 0.01685500144958496, -0.011416716501116753, 0.023299921303987503, -0.0036950877401977777, 0.02636893093585968, -0.0036766736302524805, 0.0051313843578100204, -0.0009291426977142692, -0.014141997322440147, -0.006886858027428389, 0.001689489814452827, 0.07414727658033371, 0.009667380712926388, -0.023398131132125854, -0.006469472777098417, 0.02408558875322342, -0.022403771057724953, -0.00445927120745182, -0.007881216704845428, 0.008900128304958344, 0.01411744486540556, 0.03189314901828766, -0.010354839265346527, -0.06304973363876343, -0.004799931310117245, 0.009513930417597294, -0.0037257778458297253, -0.02573057822883129, -0.013172189705073833, 0.006868443917483091, 0.03434835746884346, 0.005600942764431238, -0.019322484731674194, 0.025362296029925346, -0.015566017478704453, 0.038546763360500336, 0.027891160920262337, -0.016560377553105354, -0.005674599204212427, -0.005987638141959906, -0.019555730745196342, 0.004060300067067146, 0.028701379895210266, -0.019334761425852776, 0.0064142304472625256, 0.012693424709141254, 0.005152867175638676, -0.023177161812782288, 0.030248159542679787, -0.07660248130559921, -0.020451880991458893, -0.00705872243270278, 0.003514016279950738, -0.017726600170135498, 0.014277033507823944, 0.0016940933419391513, -0.0023800169583410025, 0.015774710103869438, 0.030567336827516556, -0.026712661609053612, -0.0034771880600601435, 0.0043088896200060844, -0.004109404049813747, 0.0027559706941246986, -0.01342998631298542, -0.00632829824462533, -0.027473775669932365, 0.013601851649582386, 0.024146968498826027, -0.018033500760793686, -0.0163639597594738, 0.03776109591126442, -0.05386725813150406, 0.009575310163199902, -0.0060797082260251045, 0.0005923188873566687, 0.0017539390828460455, -0.021470792591571808, 0.03395552560687065, -0.01167451310902834, -0.004397890996187925, 0.00915792491286993, 0.02246515080332756, 0.014277033507823944, 0.018119433894753456, 0.04493030160665512, -0.003200977109372616, -0.006610646843910217, -0.0023800169583410025, -0.05558590590953827, 0.03179493919014931, -0.0178248081356287, 0.009826969355344772, 0.029487045481801033, -0.00267617660574615, 0.007334933150559664, 0.012263762764632702, 0.04539679363369942, -0.05047907307744026, 0.008709849789738655, -0.02230556309223175, 0.00789349339902401, -0.014964491128921509, 0.0029615943785756826, -0.018021225929260254, 0.006727269385010004, 0.024797597900032997, 0.0027544363401830196, 0.011594719253480434, 0.02408558875322342, 0.012061208486557007, 0.055045757442712784, -0.009323651902377605, -0.003590741427615285, 0.021274374797940254, 0.01921200193464756, 0.007979425601661205, -0.041861291974782944, 0.01130623184144497, 0.05263965576887131, 0.058483049273490906, -0.01588519476354122, -0.013638678938150406, -0.017223283648490906, 0.03704908490180969, 0.007408589590340853, -0.030395472422242165, -0.024466145783662796, 0.0029431802686303854, 0.01588519476354122, 0.011386026628315449, 0.014448897913098335, -0.004621928557753563, -0.014510277658700943, -0.019960839301347733, -0.02798936888575554, -0.035625066608190536, 0.007629558444023132, 0.003357496578246355, -0.022772053256630898, -0.03832579404115677, 0.0066290609538555145, -0.008599365130066872, 0.00718148285523057, -0.006831615697592497, 0.0032285982742905617, -0.009311375208199024, -0.02742467075586319, 0.032752472907304764, 0.016400787979364395, -0.008525708690285683, 0.04311344772577286, 0.006647475063800812, 0.056126050651073456, -0.02206004224717617, 0.03906235471367836, -0.023422682657837868, 0.00720603484660387, -0.013037153519690037, -0.01350364275276661, 0.0021406342275440693, 0.01302487775683403, -0.011539476923644543, 0.039848022162914276, 0.005070004146546125, -0.0008708315435796976, 0.035035815089941025, 0.005343146156519651, -0.03378365933895111, 0.029487045481801033, 0.04141935706138611, -0.039823468774557114, -0.012668872252106667, 0.014743522740900517, 0.023422682657837868, 0.04046182334423065, 0.011441268026828766, -0.0020485639106482267, 0.008409086614847183, -0.052050404250621796, 0.030714649707078934, -0.03847310692071915, 0.004450064152479172, -0.017628392204642296, 0.016990037634968758, 0.03481484577059746, -0.010692429728806019, -0.012988049536943436, -0.016744516789913177, -0.009900625795125961, 0.01168065145611763, 0.00904130283743143, -0.022919364273548126, 0.034053731709718704, 0.010287320241332054, 0.01929793320596218, -0.014571658335626125, -0.009415721520781517, -0.02155672386288643, -0.007021894212812185, 0.003501740051433444, 0.011257127858698368, 0.0015774710336700082, -0.02654079720377922, 0.04164032265543938, 0.02084471471607685, 0.023410405963659286, 0.02457663044333458, 0.030812857672572136, -0.07910679280757904, -0.013368606567382812, 0.029904430732131004, 0.038939595222473145, 0.015762433409690857, -0.01595885120332241, -0.035846032202243805, 0.006245434749871492, -0.04669805243611336, 0.03994623199105263, -0.0008178910939022899, 0.043899115175008774, 0.020893817767500877, -0.029118765145540237, -0.01013386994600296, 0.034986712038517, 0.03076375462114811, 0.018180813640356064, -0.009777865372598171, 0.01969076693058014, 0.03493760526180267, 0.0351831279695034, 0.005343146156519651, 0.00445927120745182, -0.029855327680706978, 0.02172858826816082, 0.00494417455047369, 0.01314763817936182, -0.0016864208737388253, -0.00964896660298109, 0.002780522918328643, 0.04380090534687042, -0.015173184685409069, -0.009090406820178032, -0.053425323218107224, -0.031524866819381714, 0.01570105366408825, -0.010078627616167068, 0.017628392204642296, 0.00026527754380367696, 0.02149534411728382, 0.02480987459421158, -0.04296613484621048, -0.020132703706622124, 0.011079125106334686, 0.007875079289078712, -0.00837839674204588, 0.030297264456748962, 0.004204543307423592, -0.023066677153110504, 0.017211006954312325, -0.005017830990254879, -0.006751821376383305, -0.0003761455009225756, 0.008507294580340385, -0.020169531926512718, -0.031451210379600525, 0.012889840640127659, 0.019347038120031357, 0.025288639590144157, -0.015811538323760033, -0.0038669523783028126, -0.03282612934708595, 0.04370269924402237, 0.017382871359586716, -0.016977762803435326, -0.004520651418715715, -0.03329261764883995, 0.054751135408878326, 0.03368544951081276, 0.032997991889715195, 0.021421687677502632, -0.010176836512982845, -0.016179818660020828, -0.0039436775259673595, 0.006475610658526421, 0.007733904756605625, -0.0016311786603182554, 0.023005297407507896, 0.008758953772485256, -0.027080941945314407, 0.006776373367756605, 0.02441704086959362, 0.014878558926284313, -0.018340403214097023, -0.0020961337722837925, -0.01961711049079895, -0.007457693573087454, -0.035354990512132645, 0.03827669098973274, 0.0027068667113780975, -0.01571333035826683, 0.03407828509807587, 0.013700059615075588, 0.004603514447808266, -0.004572824575006962, 0.0051313843578100204, -0.006868443917483091, -0.025902442634105682, 0.02318943850696087, 0.028504962101578712, -0.005220385733991861, -0.027817504480481148, 0.008120600134134293, 0.0033881866838783026, 0.024797597900032997, -0.010612635873258114, 0.006432644557207823, -0.02906966023147106, 0.024711666628718376, 0.02490808255970478, -9.700947703095153e-05, 0.006098122335970402, -0.021053407341241837, 0.0011669909581542015, 0.009716484695672989, -0.022722948342561722, 0.04272061586380005, 0.02155672386288643, 0.0032101841643452644, -0.0008086840971373022, 0.009912901557981968, 0.014989043585956097, 0.02668810822069645, 0.010195250622928143, -0.02418379671871662, -0.03832579404115677, -0.00017407040286343545, -0.008065357804298401, 0.003197908168658614, 0.017075970768928528, -0.041468460112810135, -0.027940263971686363, 0.019580282270908356, -0.011324645951390266, 0.010569669306278229, 0.0033881866838783026, -0.02538684941828251, 0.0032101841643452644, -0.010305734351277351, 0.019334761425852776, 0.001715576509013772, 0.021212995052337646, 0.022686120122671127, -0.027817504480481148, 0.0004323467437643558, -0.02703183703124523, 0.03835034742951393, 0.012951221317052841, -0.029560701921582222, -0.014927663840353489, -0.016376236453652382, -0.02604975551366806, -0.002616330748423934, 0.0037012258544564247, -0.02034139633178711, 0.012754804454743862, 0.03083740919828415, 0.06742000579833984, 0.006739545613527298, -0.04524948075413704, 0.042352333664894104, -0.019273381680250168, 0.03906235471367836, -0.05818842351436615, 0.00469865370541811, -0.011379888281226158, 0.0018306643469259143, -0.006850029807537794, 0.0008278653840534389, 0.04421829432249069, -0.01111595332622528, -0.015983402729034424, -0.02042732946574688, -0.023373577743768692, -0.0065063005313277245, 0.0109870545566082, -0.019997667521238327, -0.013638678938150406, -0.006868443917483091, -0.00023593012883793563, 0.006457196548581123, -0.006438782438635826, 0.008660745806992054, -0.01411744486540556, 0.0062791937962174416, 0.01314763817936182, -0.01106684934347868, -0.009612138383090496, -0.017309214919805527, -0.010367115028202534, -0.03196680545806885, -0.02050098590552807, 0.023975104093551636, -0.007028032559901476, -0.006653612945228815, 0.012570664286613464, 0.0021544448100030422, -0.0010442305356264114, -0.01374916359782219, 0.03061644174158573, -0.009311375208199024, 0.0041493009775877, -0.01155789103358984, 0.0015820745611563325, -0.010268907062709332, -0.017296938225626945, 0.016130715608596802, 0.02457663044333458, 0.019678490236401558, 0.025828786194324493, -0.005834187380969524, 0.013073981739580631, -0.002300222869962454, -0.0016910244012251496, -0.018941927701234818, -0.028382202610373497, 0.015750158578157425, -0.016007954254746437, 0.00455747963860631, -0.010465322993695736, -0.0028587826527655125, 0.008157428354024887, 0.033734556287527084, -0.038865938782691956, -0.029315181076526642, -0.004600445739924908, -0.0009091941174119711, 0.007273552939295769, 0.01302487775683403, -0.022735225036740303, 0.030395472422242165, -0.015848366543650627, 0.02301757223904133, -0.01954345405101776, 0.0331944078207016, 0.009673519060015678, -0.011177333071827888, 0.023815516382455826, -0.009292961098253727, 0.0023508614394813776, -0.027375567704439163, 0.02774384804069996, -0.016732241958379745, -0.0020961337722837925, 0.010766086168587208, -0.00830474030226469, 0.008083771914243698, 0.034716639667749405, -0.026270722970366478, 0.00939730741083622, 0.02749832719564438, -0.004671033006161451, 0.0038331930991262197, -0.003397393738850951, 0.0017263180343434215, 0.006543128751218319, -0.02239149436354637, -0.04063368961215019, -0.004063368774950504, -0.0007994770421646535, 0.02107795886695385, 0.0005106065073050559, 0.015332772396504879, 0.01192003395408392, -0.028627723455429077, 0.0011800342472270131, 0.001009704195894301, -0.011637684889137745, -0.0025702957063913345, 0.012644319795072079, 0.04711543768644333, -0.0019258036045357585, 0.004480754025280476, 0.008611641824245453, -0.01734604313969612, -0.005226523615419865, -0.005386112257838249, 0.030149951577186584, -0.013565023429691792, 0.004713999107480049, 0.015443257056176662, -0.037147294729948044, -0.02148306742310524, 0.013638678938150406, 0.0004120145458728075, 0.026909077540040016, -0.008402948267757893, 0.09683339297771454, -0.01106684934347868, -0.020009944215416908, 0.0030076296534389257, 0.027449224144220352, -0.04583872854709625, -0.01700231432914734, 0.031942252069711685, -0.009943591430783272, 0.022882536053657532, 0.010901122353971004, -0.007654110435396433, -0.04011809453368187, 0.02165493369102478, -0.005100694019347429, 0.025681473314762115, -0.005944672040641308, -0.02489580772817135, -0.03908690810203552, -0.002745229285210371, -0.009047441184520721, 0.001223000348545611, -0.0066413371823728085, -0.005340076982975006, -0.029167868196964264, -0.039111461490392685, -0.019813526421785355, 0.043751802295446396, 0.0077277664095163345, 0.010385529138147831, 0.02139713615179062, -0.014350689947605133, 0.04556865617632866, 0.02570602484047413, -0.007482245564460754, 0.01586064323782921, -0.010588083416223526, 0.005395319312810898, 0.025681473314762115, -0.006512438878417015, 0.032605160027742386, -0.02393827587366104, 0.04053547978401184, -0.003342151641845703, 0.009900625795125961, 0.01886827126145363, 0.0009667380945757031, -0.013663231395184994, 0.009305237792432308, -0.004535996355116367, 0.0004035747842863202, 0.003606086364015937, -0.017849361523985863, 0.019997667521238327, 0.004652618896216154, -0.005214247386902571, 0.03722095116972923, 0.022256458178162575, 0.012963497079908848, 0.004772310145199299, 0.006500162649899721, -0.028283994644880295, 0.050258103758096695, 0.010489875450730324, 0.040314510464668274, 0.035134024918079376, 0.000899219885468483, 0.0027989367954432964, -2.5115527932939585e-06, 0.03402917832136154, 0.009366617538034916, 0.01562739722430706, 0.01265659648925066, 0.006843891926109791, 0.00837839674204588, 0.00555797666311264, -0.02115161530673504, 0.024380212649703026, 0.026589900255203247, 0.025092223659157753, 0.004947243724018335, -0.03002719208598137, 0.04144390672445297, 0.027645640075206757, -0.02710549347102642, -0.001772353076376021, -0.03312075510621071, 0.013098533265292645, 0.0021667208056896925, 0.0020608401391655207, 0.01766522042453289, 0.023557718843221664, 0.0533762164413929, -0.0011861722450703382, -0.022096870467066765, 0.015430981293320656, -0.04583872854709625, 0.004124748986214399, 0.003467981005087495, 0.039602503180503845, 0.0040940591134130955, -0.01798439770936966, -0.0029094212222844362, 0.00571449613198638, -0.012570664286613464, -0.01037939079105854, 0.014105169102549553, -0.005745186470448971, -0.014350689947605133, -0.026000650599598885, 0.02987987920641899, 0.01660948060452938, -0.01879461482167244, 0.017039142549037933, -0.0035784654319286346, 0.010845880024135113, -0.01944524608552456, 0.032997991889715195, -0.01290211733430624, 0.03884138911962509, 0.03896414861083031, 0.0387677326798439, -0.0021160822361707687, 0.02343495935201645, 0.0014217187417671084, -0.018340403214097023, 0.004609652794897556, 0.007095550652593374, 0.05975975841283798, -0.009882211685180664, 0.0048428974114358425, 0.03402917832136154, 0.017812533304095268, 0.011183471418917179, 0.02075878158211708, -0.002446000697091222, 0.008630055002868176, -0.011367612518370152, -0.027351014316082, 0.020230911672115326, -0.019150620326399803, 0.025853337720036507, -0.009962005540728569, -0.0015237632906064391, 0.003185632172971964, 0.038448553532361984, 0.017677495256066322, 0.015836089849472046, 0.02840675413608551, 0.0379084087908268, -0.039185117930173874, 0.010146146640181541, 0.006518576759845018, -0.02067285031080246, -0.0006939798477105796, -0.015676502138376236, -0.003329875646159053, -0.020967474207282066, 0.019494349136948586, -0.015566017478704453, -0.03164763003587723, -0.005137522239238024, -0.017702048644423485, -0.024294281378388405, 0.036999981850385666, -0.021274374797940254, -0.01675679348409176, -0.0041554393246769905, 0.015443257056176662, 0.016216646879911423, 0.02995353564620018, -0.01886827126145363, -0.05038086324930191, 0.04085465893149376, 0.011760445311665535, -0.021949557587504387, -0.01724783517420292, 0.026810869574546814, -0.03189314901828766, -0.0080162538215518, 0.003200977109372616, -0.010858156718313694, 0.013614127412438393, 0.037466470152139664, -0.029904430732131004, -0.002424517646431923, -0.006469472777098417, -0.02514132857322693, 0.009544620290398598, -0.018598198890686035, -0.0479993112385273, 0.001853681867942214, 0.029094211757183075, -0.0031702870037406683, -0.02587789110839367, -0.008513432927429676, -0.03896414861083031, 0.009900625795125961, 0.004167715087532997, 0.01473124697804451, 0.0397007092833519, 0.02749832719564438, -0.004437787923961878, -0.0038025029934942722, 0.025264088064432144, 0.005002486053854227, -0.0020562366116791964, -0.028996003791689873, -0.015553741715848446, -0.030911065638065338, 0.051313843578100204, 0.006530852522701025, -0.008832610212266445, 0.012865289114415646, -0.0034833261743187904, 0.04038816690444946, -0.0267863180488348, 0.014682142995297909, -0.019985390827059746, 0.007709352299571037, -0.011386026628315449, 0.02002221904695034, 0.04645253345370293, -0.014547105878591537, 0.010232078842818737, 0.03427470102906227, 0.02109023556113243, -0.03977436572313309, -0.019027860835194588, 0.022268734872341156, 0.05794290453195572, -0.021544449031352997, -0.02646714076399803, -0.014620762318372726, 0.042278677225112915, 0.035772379487752914, -0.00891240406781435, -0.01306170504540205, -0.038301240652799606, -0.030346369370818138, -0.05990707129240036, -0.010127732530236244, -0.012582940049469471, -0.004938036669045687, 0.004833690356463194, 0.045544106513261795, 0.04942333325743675, 0.0036152934189885855, -0.005821911618113518, -0.0297080148011446, -0.005337007809430361, 0.02636893093585968, -0.008808057755231857, -0.03616521134972572, -0.03871862590312958, -0.011606995016336441, 0.0010365580674260855, -0.04907960444688797, 0.018193090334534645, -0.03179493919014931, 0.0046464805491268635, -0.03920966759324074, 0.03280157595872879, -0.007537487894296646, 0.010342562571167946, -0.017567012459039688, -0.020292293280363083, 0.01969076693058014, -0.010262768715620041, -0.03061644174158573, 0.002554950537160039, -0.040339063853025436, 0.021200720220804214, -0.024048760533332825, -0.0509210079908371, 0.003158011008054018, -0.035944242030382156, -0.009839245118200779, 0.004778448026627302, -0.008875575847923756, -0.0039037803653627634, 0.04930057376623154, -0.01350364275276661, 0.031696733087301254, 0.005929326638579369, -0.0077400426380336285, 0.007611144334077835, 0.017873913049697876, 0.009857659228146076, -0.022366942837834358, -0.008145151659846306, -0.00299228448420763, 0.0050423829816281796, 0.022010937333106995, 0.0009414187516085804, -0.0025211914908140898, 0.015394153073430061, 0.006622923072427511, 0.015369600616395473, -0.0031702870037406683, -0.029020555317401886, 0.0026807801332324743, 0.04758192598819733, 0.008040805347263813, 0.01619209535419941, 0.028504962101578712, -0.00365212163887918, -0.0012705700937658548, -0.012263762764632702, 0.024699389934539795, -0.0017738876631483436, 0.02668810822069645, -0.004977933596819639, -0.01069856807589531, 0.010403943248093128, 0.0034096697345376015, -0.0011907757725566626, 0.011877067387104034, 0.0013327174820005894, 0.029683461412787437, 0.03957794979214668, 0.03719639778137207, 0.012632044032216072, 0.0246625617146492, 0.029855327680706978, 0.006223951932042837, -0.0351831279695034, -0.03314530476927757, 0.015934297814965248, -0.021912729367613792, -0.018991032615303993, -0.026565348729491234, 0.029560701921582222, 0.040977418422698975, 0.0036398456431925297, -0.014817179180681705, 0.01473124697804451, 0.04323621094226837, 0.011030021123588085, -0.023164885118603706, -0.031303901225328445, -0.01710052229464054, 0.023091228678822517, 0.03208956494927406, -0.0008194256224669516, -0.01847543939948082, 0.008777367882430553, 0.039111461490392685, -0.0062300898134708405, -0.001654196297749877, -0.019420694559812546, 0.00972262304276228, -0.000748838356230408, 0.011275541968643665, 0.007985563017427921, 0.011619270779192448, 0.03255605697631836, -0.02710549347102642, -0.011821825988590717, 0.025509608909487724, -0.014755798503756523, 0.011705202981829643, -0.0021237547043710947, -0.004318096674978733, -0.003679742803797126, -0.002355464966967702, -0.056764405220746994, 0.026319827884435654, -0.02239149436354637, -0.004456202033907175, 0.011717479676008224, 0.015811538323760033, 0.0019948564004153013, -0.03378365933895111, 0.037392813712358475, 0.009974281303584576, 0.0017401285003870726, -0.00041431630961596966, 0.04834304004907608, -0.029560701921582222, 0.015762433409690857, 0.03132845088839531, 0.022931640967726707, -0.006800925824791193, -0.010367115028202534, -0.015087251551449299, -0.022772053256630898, 0.016584929078817368, -0.0002506997261662036]\n",
      "1536\n",
      "-0.0018429403426125646\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "#https://platform.openai.com/docs/guides/embeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "print(embeddings)\n",
    "\n",
    "text = \"Hi this is shanmukh\"\n",
    "result = embeddings.embed_query(text)\n",
    "print(result)\n",
    "print(len(result))\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.embeddings.Embeddings object at 0x000002A3C51F8820> async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x000002A3C661A790> model='text-embedding-3-small' dimensions=199 deployment='text-embedding-ada-002' openai_api_version=None openai_api_base=None openai_api_type=None openai_proxy=None embedding_ctx_length=8191 openai_api_key=SecretStr('**********') openai_organization=None allowed_special=None disallowed_special=None chunk_size=1000 max_retries=2 request_timeout=None headers=None tiktoken_enabled=True tiktoken_model_name=None show_progress_bar=False model_kwargs={} skip_empty=False default_headers=None default_query=None retry_min_seconds=4 retry_max_seconds=20 http_client=None http_async_client=None check_embedding_ctx_length=True\n",
      "[-0.00402143644168973, -0.11767087131738663, 0.051633257418870926, -0.00761921564117074, -0.03791734576225281, -0.1177767887711525, 0.08880920708179474, 0.11724721640348434, -0.08690274506807327, -0.05719376355409622, 0.0032684514299035072, -0.08367235958576202, -0.04085646942257881, 0.0028017661534249783, 0.030132640153169632, 0.10263103246688843, -0.010578198358416557, -0.01563560962677002, 0.030953476205468178, 0.0014008830767124891, 0.046681761741638184, -0.041147734969854355, 0.10421974956989288, 0.024148477241396904, 0.005004454404115677, -0.049223706126213074, 0.021699208766222, -0.08817371726036072, -0.00342566822655499, -0.2133115530014038, 0.05269240215420723, -0.05841177701950073, 0.0023913481272757053, 0.02751125954091549, -0.0048290337435901165, -0.030794605612754822, 0.031324177980422974, 0.04117421433329582, -0.07525216788053513, 0.013298873789608479, 0.03291289135813713, -0.0585176944732666, 0.004514600150287151, 0.14192526042461395, 0.04361024498939514, -0.052957188338041306, -0.21670082211494446, -0.060265280306339264, 0.017184607684612274, 0.031774312257766724, 0.010730450041592121, 0.029550110921263695, 0.015768002718687057, 0.1474328190088272, 0.04464291036128998, -0.11968324333429337, 0.08271912485361099, 0.06736154109239578, -0.009624969214200974, 0.03291289135813713, -0.07398119568824768, -0.12497896701097488, -0.01856149360537529, 0.017052214592695236, -0.10533184558153152, -0.07684087753295898, -0.05846473574638367, 0.06476663798093796, -0.05608166381716728, -0.09452857822179794, 0.06105963885784149, 0.03638158738613129, -0.04504008963704109, 0.03217149153351784, -0.013338591903448105, 0.03534892201423645, 0.010333271697163582, -0.021394703537225723, 0.09071566164493561, 0.04583444818854332, -0.07752932608127594, 0.08732639998197556, 0.008214984089136124, -0.0547577328979969, -0.14849194884300232, 0.06857956200838089, -0.20091956853866577, 0.1054377630352974, -0.07218065112829208, 0.003839396173134446, -0.036063846200704575, 0.06868547201156616, -0.08992130309343338, 0.03918831795454025, 0.020322320982813835, 0.03005320392549038, -0.053963374346494675, 0.029841376468539238, 0.09733530879020691, -0.02364538423717022, 0.038288045674562454, 0.03256867080926895, 0.10549072176218033, 0.05624053254723549, -0.006917532533407211, -0.0896565169095993, 0.024426503106951714, -0.05872952193021774, 0.008910046890377998, 0.1063380315899849, -0.2165949046611786, -0.1123751550912857, 0.05666419118642807, 0.09526998549699783, 0.02253328450024128, 0.10713239014148712, 0.16046027839183807, -0.02180512249469757, -0.09384013712406158, -0.002626345492899418, -0.08790893107652664, 0.04726428911089897, 0.014986883848905563, -0.09754714369773865, -0.13366393744945526, -0.05221578851342201, 0.06810294091701508, -0.05422816053032875, -0.15421132743358612, -0.01347760483622551, 0.05740559101104736, -0.07514625042676926, -0.010756928473711014, -0.07202177494764328, -0.017568547278642654, -0.03590497374534607, -0.04890596494078636, 0.06969165802001953, -0.08822667598724365, -0.034289781004190445, -0.0030963406898081303, -0.030847562476992607, -0.0018501917365938425, -0.04559613764286041, 0.09669982641935349, -0.09569364041090012, 0.08743231743574142, 0.0773174986243248, -0.0481380857527256, -0.035507794469594955, 0.014430833980441093, 0.09648799896240234, 0.014669140800833702, 0.029099974781274796, 0.054069288074970245, -0.09034496545791626, 0.007970056496560574, -0.009207931347191334, 0.035746101289987564, 0.09929472953081131, 0.048985399305820465, -0.01926317811012268, 0.007188938092440367, -0.002950708381831646, -0.027802523225545883, 0.09881811589002609, -0.08955060690641403, 0.03148304671049118, -0.0849962905049324, -0.016919821500778198, -0.026690423488616943, -0.012458178214728832, -0.0017326930537819862, -0.07154516130685806, -0.029576590284705162, 0.02793491631746292, 0.10051274299621582, -0.09786488115787506, -0.0594179667532444, 0.0612185113132, 0.0020471264142543077, 0.11618807166814804, 0.01198156364262104, 0.053274933248758316, 0.0035977789666503668, 0.052612967789173126, 0.09060975164175034, -0.017224324867129326, -0.04639049619436264, 0.056399405002593994, 0.009260888211429119, 0.17698292434215546, -0.009711024351418018, 0.018402623012661934, -0.007791326381266117, -0.07890620827674866, 0.10930363833904266, -0.021818362176418304, -0.019276415929198265]\n",
      "199\n",
      "-0.00402143644168973\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "#https://platform.openai.com/docs/guides/embeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\",dimensions=199)\n",
    "print(embeddings)\n",
    "\n",
    "\n",
    "text = \"Hi this is shanmukh\"\n",
    "result = embeddings.embed_query(text)\n",
    "print(result)\n",
    "print(len(result))\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Building an exciting new project with LangChain - come check it out! [{'source': 'tweet'}]\n",
      "* Building an exciting new project with LangChain - come check it out! [{'source': 'tweet'}]\n",
      "* [SIM=0.893629] The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees. [{'source': 'news'}]\n",
      "* I had chocolate chip pancakes and scrambled eggs for breakfast this morning. [{'source': 'tweet'}]\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from uuid import uuid4\n",
    "from langchain_core.documents import Document\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
    ")\n",
    "\n",
    "document_1 = Document(\n",
    "    page_content=\"I had chocolate chip pancakes and scrambled eggs for breakfast this morning.\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=1,\n",
    ")\n",
    "\n",
    "document_2 = Document(\n",
    "    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    "    id=2,\n",
    ")\n",
    "\n",
    "document_3 = Document(\n",
    "    page_content=\"Building an exciting new project with LangChain - come check it out!\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=3,\n",
    ")\n",
    "\n",
    "document_4 = Document(\n",
    "    page_content=\"Robbers broke into the city bank and stole $1 million in cash.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    "    id=4,\n",
    ")\n",
    "\n",
    "document_5 = Document(\n",
    "    page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=5,\n",
    ")\n",
    "\n",
    "document_6 = Document(\n",
    "    page_content=\"Is the new iPhone worth the price? Read this review to find out.\",\n",
    "    metadata={\"source\": \"website\"},\n",
    "    id=6,\n",
    ")\n",
    "\n",
    "document_7 = Document(\n",
    "    page_content=\"The top 10 soccer players in the world right now.\",\n",
    "    metadata={\"source\": \"website\"},\n",
    "    id=7,\n",
    ")\n",
    "\n",
    "document_8 = Document(\n",
    "    page_content=\"LangGraph is the best framework for building stateful, agentic applications!\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=8,\n",
    ")\n",
    "\n",
    "document_9 = Document(\n",
    "    page_content=\"The stock market is down 500 points today due to fears of a recession.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    "    id=9,\n",
    ")\n",
    "\n",
    "document_10 = Document(\n",
    "    page_content=\"I have a bad feeling I am going to get deleted :(\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=10,\n",
    ")\n",
    "\n",
    "documents = [\n",
    "    document_1,\n",
    "    document_2,\n",
    "    document_3,\n",
    "    document_4,\n",
    "    document_5,\n",
    "    document_6,\n",
    "    document_7,\n",
    "    document_8,\n",
    "    document_9,\n",
    "    document_10,\n",
    "]\n",
    "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "\n",
    "vector_store.add_documents(documents=documents, ids=uuids)\n",
    "\n",
    "\n",
    "results = vector_store.similarity_search(\n",
    "    \"LangChain provides abstractions to make working with LLMs easy\",\n",
    "    k=2,\n",
    "    filter={\"source\": \"tweet\"},\n",
    ")\n",
    "for res in results:\n",
    "    print(f\"* {res.page_content} [{res.metadata}]\")\n",
    "\n",
    "\n",
    "\n",
    "#k=1 specifies the number of results that the similarity search should return.\n",
    "results = vector_store.similarity_search_with_score(\n",
    "    \"Will it be hot tomorrow?\", k=1, filter={\"source\": \"news\"}\n",
    ")\n",
    "for res, score in results:\n",
    "    print(f\"* [SIM={score:3f}] {res.page_content} [{res.metadata}]\")\n",
    "\n",
    "#Search by vector\n",
    "results = vector_store.similarity_search_by_vector(\n",
    "    embedding=embeddings.embed_query(\"I love green eggs and ham!\"), k=1\n",
    ")\n",
    "for doc in results:\n",
    "    print(f\"* {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* APJ was born in india [{'source': 'D://GenAI//kalam.txt'}]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "file_path = \"D://GenAI//kalam.txt\"\n",
    "\n",
    "loader = TextLoader(file_path)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "final_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection1\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db1\",  # Where to save data locally, remove if not necessary\n",
    ")\n",
    "\n",
    "vector_store.add_documents(final_docs)\n",
    "\n",
    "results = vector_store.similarity_search(\n",
    "    \"Where is APJ Born\",\n",
    "    k=1,\n",
    ")\n",
    "for res in results:\n",
    "    print(f\"* {res.page_content} [{res.metadata}]\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
