{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document loaders\n",
    "- Document Loaders are responsible for loading documents from a variety of sources.\n",
    "- Reference https://python.langchain.com/docs/how_to/#document-loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert text to vectors\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "os.environ['HF_TOKEN']=os.getenv('HF_TOKEN')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'D://GenAI//kalam.pdf', 'page': 0}, page_content=\"Avul Pakir Jainulabdeen Abdul Kalam BR (/Àà…ôbd äl k…ôÀàl…ëÀêm/ ‚ìò; 15 October 1931 ‚Äì 27 July \\n2015) was an Indian aerospace scientist and statesman who served as the 11th president of \\nIndia from 2002 to 2007. Born and raised in a Muslim family in Rameswaram, Tamil Nadu, \\nhe studied physics and aerospace engineering. He spent the next four decades as a \\nscientist and science administrator, mainly at the Defence Research and Development \\nOrganisation (DRDO) and Indian Space Research Organisation (ISRO) and was intimately \\ninvolved in India's civilian space programme and military missile development efforts.[2] He \\nthus came to be known as the Missile Man of India for his work on the development \\nof ballistic missile and launch vehicle technology.[3][4][5] He also played a pivotal \\norganisational, technical, and political role in India's Pokhran-II nuclear tests in 1998, the first \\nsince the original nuclear test by India in 1974.[6] \")]\n"
     ]
    }
   ],
   "source": [
    "#How to load PDFs\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"D://GenAI//kalam.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(file_path)\n",
    "pages = []\n",
    "for page in loader.load():\n",
    "    pages.append(page)\n",
    "\n",
    "print(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'D://GenAI//kalam.txt'}, page_content='The Defence Research and Development Organisation.\\nDefence Research and Development in Ministry of Defence of the Government of India.The Defence Research and Development Organisation.\\nDefence Research and Development in Ministry of Defence of the Government of India.The Defence Research and Development Organisation.\\nDefence Research and Development in Ministry of Defence of the Government of India.\\nThe Defence Research and Development Organisation.\\nDefence Research and Development in Ministry of Defence of the Government of India.\\nAPJ was born in india')]\n"
     ]
    }
   ],
   "source": [
    "#How to load txt file\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "file_path = \"D://GenAI//kalam.txt\"\n",
    "\n",
    "loader = TextLoader(file_path)\n",
    "pages = []\n",
    "\n",
    "for page in loader.load():\n",
    "    pages.append(page)\n",
    "\n",
    "print(pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Architecture | Guides | Tutorials | How-to guides | Conceptual guide | Integrations | API reference | Ecosystem | ü¶úüõ†Ô∏è LangSmith | ü¶úüï∏Ô∏è LangGraph | Additional resources | Versions | Security | Contributing' metadata={'source': 'https://python.langchain.com/docs/introduction/'}\n"
     ]
    }
   ],
   "source": [
    "#How to load web pages\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "page_url = \"https://python.langchain.com/docs/introduction/\"\n",
    "\n",
    "loader = WebBaseLoader(web_paths=[page_url],\n",
    "                       bs_kwargs={\n",
    "        \"parse_only\": bs4.SoupStrainer(class_=\"table-of-contents__link toc-highlight\"),\n",
    "    },\n",
    "    bs_get_text_kwargs={\"separator\": \" | \", \"strip\": True},)\n",
    "docs = []\n",
    "for doc in loader.load():\n",
    "    docs.append(doc)\n",
    "\n",
    "assert len(docs) == 1\n",
    "doc = docs[0]\n",
    "\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large language models (LLMs) have demonstrated impressive reasoning\n",
      "abilities, but they still strugg\n",
      "{'Entry ID': 'http://arxiv.org/abs/2410.13080v1', 'Published': datetime.date(2024, 10, 16), 'Title': 'Graph-constrained Reasoning: Faithful Reasoning on Knowledge Graphs with Large Language Models', 'Authors': 'Linhao Luo, Zicheng Zhao, Chen Gong, Gholamreza Haffari, Shirui Pan'}\n"
     ]
    }
   ],
   "source": [
    "#https://python.langchain.com/docs/integrations/providers/arxiv/#installation-and-setup\n",
    "#ArxivLoader is a tool used to fetch and load research papers from the arXiv database, which is a popular repository for academic papers in fields like physics, computer science, and mathematics. It allows users to retrieve papers in a structured format, enabling them to process and analyze the content programmatically.\n",
    "#for more data source providers go through this link https://python.langchain.com/docs/integrations/providers/all/\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "loader = ArxivLoader(\n",
    "    query=\"reasoning\"\n",
    ")\n",
    "\n",
    "docs = loader.get_summaries_as_docs()\n",
    "print(docs[0].page_content[:100])\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon Reeve Musk (; born June 28, 1971) is a businessman known for his key roles in the space company\n"
     ]
    }
   ],
   "source": [
    "#load from wikipedia\n",
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "\n",
    "retriever = WikipediaRetriever()\n",
    "docs = retriever.invoke(\"Elon Musk\")\n",
    "print(docs[0].page_content[:100])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation\n",
    "- How to recursively split by characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://python.langchain.com/docs/how_to/recursive_text_splitter/\n",
    "#Let's go through the parameters set above for RecursiveCharacterTextSplitter:\n",
    "\n",
    "#chunk_size: The maximum size of a chunk, where size is determined by the length_function.\n",
    "#chunk_overlap: Target overlap between chunks. Overlapping chunks helps to mitigate loss of information when context is divided between chunks.\n",
    "#length_function: Function determining the chunk size.\n",
    "#is_separator_regex: Whether the separator list (defaulting to [\"\\n\\n\", \"\\n\", \" \", \"\"]) should be interpreted as regex.\n",
    "\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load example document\n",
    "with open(\"elon.txt\", encoding=\"utf-8\") as f:\n",
    "    state_of_the_union = f.read()\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "texts = text_splitter.create_documents([state_of_the_union])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://python.langchain.com/docs/how_to/character_text_splitter/\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# Load example document\n",
    "with open(\"elon.txt\", encoding=\"utf-8\") as f:\n",
    "    state_of_the_union = f.read()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\t\",\n",
    "    chunk_size=10,\n",
    "    chunk_overlap=5,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "texts = text_splitter.create_documents([state_of_the_union])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://python.langchain.com/docs/how_to/split_html/#overview-of-the-splitters\n",
    "#How to: split HTML\n",
    "\n",
    "#Choosing the Right Splitter\n",
    "    #Use HTMLHeaderTextSplitter when:\n",
    "        #You need to split an HTML document based on its header hierarchy and maintain metadata about the headers.\n",
    "    \n",
    "    #Use HTMLSectionSplitter when:\n",
    "        #You need to split the document into larger, more general sections, possibly based on custom tags or font sizes.\n",
    "    \n",
    "    #Use HTMLSemanticPreservingSplitter when: \n",
    "        #You need to split the document into chunks while preserving semantic elements like tables and lists, ensuring that they are not split and that their context is maintained.\n",
    "\n",
    "html_string = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "  <html lang='en'>\n",
    "  <head>\n",
    "    <meta charset='UTF-8'>\n",
    "    <meta name='viewport' content='width=device-width, initial-scale=1.0'>\n",
    "    <title>Fancy Example HTML Page</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <h1>Main Title</h1>\n",
    "    \n",
    "    <h2>Section 1: Introduction</h2>\n",
    "    <p>This section introduces the topic. Below is a list:</p>\n",
    "    \n",
    "    <h3>Subsection 1.1: Details</h3>\n",
    "    <p>This subsection provides additional details. Here's a table:</p>\n",
    "    \n",
    "    <h2>Section 2: Media Content</h2>\n",
    "    \n",
    "\n",
    "    <h2>Section 3: Code Example</h2>\n",
    "  \n",
    "    <h2>Conclusion</h2>\n",
    "  </body>\n",
    "  </html>\n",
    " \"\"\"\n",
    "\n",
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header-1\"),\n",
    "    (\"h2\", \"Header-2\"),\n",
    "    (\"h3\", \"Header-3\"),\n",
    "]\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on)\n",
    "html_header_splits = html_splitter.split_text(html_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method 2\n",
    "\n",
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "url=\"https://python.langchain.com/docs/how_to/split_html/#choosing-the-right-splitter\"\n",
    "\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on)\n",
    "html_header_splits = html_splitter.split_text_from_url(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'openapi': '3.1.0', 'info': {'title': 'LangSmith', 'version': '0.1.0'}, 'paths': {'/api/v1/sessions/{session_id}': {'get': {'tags': ['tracer-sessions'], 'summary': 'Read Tracer Session', 'description': 'Get a specific session.'}}}}\n",
      "{'paths': {'/api/v1/sessions/{session_id}': {'get': {'operationId': 'read_tracer_session_api_v1_sessions__session_id__get', 'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}}}}\n",
      "{'paths': {'/api/v1/sessions/{session_id}': {'get': {'parameters': [{'name': 'session_id', 'in': 'path', 'required': True, 'schema': {'type': 'string', 'format': 'uuid', 'title': 'Session Id'}}, {'name': 'include_stats', 'in': 'query', 'required': False, 'schema': {'type': 'boolean', 'default': False, 'title': 'Include Stats'}}, {'name': 'accept', 'in': 'header', 'required': False, 'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Accept'}}]}}}}\n",
      "page_content='{\"openapi\": \"3.1.0\", \"info\": {\"title\": \"LangSmith\", \"version\": \"0.1.0\"}, \"paths\": {\"/api/v1/sessions/{session_id}\": {\"get\": {\"tags\": [\"tracer-sessions\"], \"summary\": \"Read Tracer Session\", \"description\": \"Get a specific session.\"}}}}'\n",
      "page_content='{\"paths\": {\"/api/v1/sessions/{session_id}\": {\"get\": {\"operationId\": \"read_tracer_session_api_v1_sessions__session_id__get\", \"security\": [{\"API Key\": []}, {\"Tenant ID\": []}, {\"Bearer Auth\": []}]}}}}'\n",
      "page_content='{\"paths\": {\"/api/v1/sessions/{session_id}\": {\"get\": {\"parameters\": [{\"name\": \"session_id\", \"in\": \"path\", \"required\": true, \"schema\": {\"type\": \"string\", \"format\": \"uuid\", \"title\": \"Session Id\"}}, {\"name\": \"include_stats\", \"in\": \"query\", \"required\": false, \"schema\": {\"type\": \"boolean\", \"default\": false, \"title\": \"Include Stats\"}}, {\"name\": \"accept\", \"in\": \"header\", \"required\": false, \"schema\": {\"anyOf\": [{\"type\": \"string\"}, {\"type\": \"null\"}], \"title\": \"Accept\"}}]}}}}'\n"
     ]
    }
   ],
   "source": [
    "#https://python.langchain.com/docs/how_to/recursive_json_splitter/\n",
    "import json\n",
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "import requests\n",
    "\n",
    "# This is a large nested json object and will be loaded as a python dict\n",
    "json_data = requests.get(\"https://api.smith.langchain.com/openapi.json\").json()\n",
    "\n",
    "\n",
    "splitter = RecursiveJsonSplitter(max_chunk_size=300)\n",
    "\n",
    "# Recursively split json data - If you need to access/manipulate the smaller json chunks\n",
    "json_chunks = splitter.split_json(json_data=json_data)\n",
    "\n",
    "#printing top 3 chunks\n",
    "for chunk in json_chunks[:3]:\n",
    "    print(chunk)\n",
    "\n",
    "# The splitter can also output documents\n",
    "docs = splitter.create_documents(texts=[json_data])\n",
    "\n",
    "for doc in docs[:3]:\n",
    "    print(doc)\n",
    "\n",
    "#Or use .split_text to obtain string content directly:\n",
    "texts = splitter.split_text(json_data=json_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.001864203019067645\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "#https://platform.openai.com/docs/guides/embeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "text = \"Hi this is shanmukh\"\n",
    "result = embeddings.embed_query(text)\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.embeddings.Embeddings object at 0x000001D50195F9A0> async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x000001D5428311C0> model='text-embedding-3-small' dimensions=199 deployment='text-embedding-ada-002' openai_api_version=None openai_api_base=None openai_api_type=None openai_proxy=None embedding_ctx_length=8191 openai_api_key=SecretStr('**********') openai_organization=None allowed_special=None disallowed_special=None chunk_size=1000 max_retries=2 request_timeout=None headers=None tiktoken_enabled=True tiktoken_model_name=None show_progress_bar=False model_kwargs={} skip_empty=False default_headers=None default_query=None retry_min_seconds=4 retry_max_seconds=20 http_client=None http_async_client=None check_embedding_ctx_length=True\n",
      "[-0.00402143644168973, -0.11767087131738663, 0.051633257418870926, -0.00761921564117074, -0.03791734576225281, -0.1177767887711525, 0.08880920708179474, 0.11724721640348434, -0.08690274506807327, -0.05719376355409622, 0.0032684514299035072, -0.08367235958576202, -0.04085646942257881, 0.0028017661534249783, 0.030132640153169632, 0.10263103246688843, -0.010578198358416557, -0.01563560962677002, 0.030953476205468178, 0.0014008830767124891, 0.046681761741638184, -0.041147734969854355, 0.10421974956989288, 0.024148477241396904, 0.005004454404115677, -0.049223706126213074, 0.021699208766222, -0.08817371726036072, -0.00342566822655499, -0.2133115530014038, 0.05269240215420723, -0.05841177701950073, 0.0023913481272757053, 0.02751125954091549, -0.0048290337435901165, -0.030794605612754822, 0.031324177980422974, 0.04117421433329582, -0.07525216788053513, 0.013298873789608479, 0.03291289135813713, -0.0585176944732666, 0.004514600150287151, 0.14192526042461395, 0.04361024498939514, -0.052957188338041306, -0.21670082211494446, -0.060265280306339264, 0.017184607684612274, 0.031774312257766724, 0.010730450041592121, 0.029550110921263695, 0.015768002718687057, 0.1474328190088272, 0.04464291036128998, -0.11968324333429337, 0.08271912485361099, 0.06736154109239578, -0.009624969214200974, 0.03291289135813713, -0.07398119568824768, -0.12497896701097488, -0.01856149360537529, 0.017052214592695236, -0.10533184558153152, -0.07684087753295898, -0.05846473574638367, 0.06476663798093796, -0.05608166381716728, -0.09452857822179794, 0.06105963885784149, 0.03638158738613129, -0.04504008963704109, 0.03217149153351784, -0.013338591903448105, 0.03534892201423645, 0.010333271697163582, -0.021394703537225723, 0.09071566164493561, 0.04583444818854332, -0.07752932608127594, 0.08732639998197556, 0.008214984089136124, -0.0547577328979969, -0.14849194884300232, 0.06857956200838089, -0.20091956853866577, 0.1054377630352974, -0.07218065112829208, 0.003839396173134446, -0.036063846200704575, 0.06868547201156616, -0.08992130309343338, 0.03918831795454025, 0.020322320982813835, 0.03005320392549038, -0.053963374346494675, 0.029841376468539238, 0.09733530879020691, -0.02364538423717022, 0.038288045674562454, 0.03256867080926895, 0.10549072176218033, 0.05624053254723549, -0.006917532533407211, -0.0896565169095993, 0.024426503106951714, -0.05872952193021774, 0.008910046890377998, 0.1063380315899849, -0.2165949046611786, -0.1123751550912857, 0.05666419118642807, 0.09526998549699783, 0.02253328450024128, 0.10713239014148712, 0.16046027839183807, -0.02180512249469757, -0.09384013712406158, -0.002626345492899418, -0.08790893107652664, 0.04726428911089897, 0.014986883848905563, -0.09754714369773865, -0.13366393744945526, -0.05221578851342201, 0.06810294091701508, -0.05422816053032875, -0.15421132743358612, -0.01347760483622551, 0.05740559101104736, -0.07514625042676926, -0.010756928473711014, -0.07202177494764328, -0.017568547278642654, -0.03590497374534607, -0.04890596494078636, 0.06969165802001953, -0.08822667598724365, -0.034289781004190445, -0.0030963406898081303, -0.030847562476992607, -0.0018501917365938425, -0.04559613764286041, 0.09669982641935349, -0.09569364041090012, 0.08743231743574142, 0.0773174986243248, -0.0481380857527256, -0.035507794469594955, 0.014430833980441093, 0.09648799896240234, 0.014669140800833702, 0.029099974781274796, 0.054069288074970245, -0.09034496545791626, 0.007970056496560574, -0.009207931347191334, 0.035746101289987564, 0.09929472953081131, 0.048985399305820465, -0.01926317811012268, 0.007188938092440367, -0.002950708381831646, -0.027802523225545883, 0.09881811589002609, -0.08955060690641403, 0.03148304671049118, -0.0849962905049324, -0.016919821500778198, -0.026690423488616943, -0.012458178214728832, -0.0017326930537819862, -0.07154516130685806, -0.029576590284705162, 0.02793491631746292, 0.10051274299621582, -0.09786488115787506, -0.0594179667532444, 0.0612185113132, 0.0020471264142543077, 0.11618807166814804, 0.01198156364262104, 0.053274933248758316, 0.0035977789666503668, 0.052612967789173126, 0.09060975164175034, -0.017224324867129326, -0.04639049619436264, 0.056399405002593994, 0.009260888211429119, 0.17698292434215546, -0.009711024351418018, 0.018402623012661934, -0.007791326381266117, -0.07890620827674866, 0.10930363833904266, -0.021818362176418304, -0.019276415929198265]\n",
      "199\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "#https://platform.openai.com/docs/guides/embeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\",dimensions=199)\n",
    "print(embeddings)\n",
    "\n",
    "\n",
    "text = \"Hi this is shanmukh\"\n",
    "result = embeddings.embed_query(text)\n",
    "print(result)\n",
    "print(len(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Building an exciting new project with LangChain - come check it out! [{'source': 'tweet'}]\n",
      "* Building an exciting new project with LangChain - come check it out! [{'source': 'tweet'}]\n",
      "* [SIM=0.893568] The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees. [{'source': 'news'}]\n",
      "* I had chocolate chip pancakes and scrambled eggs for breakfast this morning. [{'source': 'tweet'}]\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from uuid import uuid4\n",
    "from langchain_core.documents import Document\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
    ")\n",
    "\n",
    "document_1 = Document(\n",
    "    page_content=\"I had chocolate chip pancakes and scrambled eggs for breakfast this morning.\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=1,\n",
    ")\n",
    "\n",
    "document_2 = Document(\n",
    "    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    "    id=2,\n",
    ")\n",
    "\n",
    "document_3 = Document(\n",
    "    page_content=\"Building an exciting new project with LangChain - come check it out!\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=3,\n",
    ")\n",
    "\n",
    "document_4 = Document(\n",
    "    page_content=\"Robbers broke into the city bank and stole $1 million in cash.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    "    id=4,\n",
    ")\n",
    "\n",
    "document_5 = Document(\n",
    "    page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=5,\n",
    ")\n",
    "\n",
    "document_6 = Document(\n",
    "    page_content=\"Is the new iPhone worth the price? Read this review to find out.\",\n",
    "    metadata={\"source\": \"website\"},\n",
    "    id=6,\n",
    ")\n",
    "\n",
    "document_7 = Document(\n",
    "    page_content=\"The top 10 soccer players in the world right now.\",\n",
    "    metadata={\"source\": \"website\"},\n",
    "    id=7,\n",
    ")\n",
    "\n",
    "document_8 = Document(\n",
    "    page_content=\"LangGraph is the best framework for building stateful, agentic applications!\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=8,\n",
    ")\n",
    "\n",
    "document_9 = Document(\n",
    "    page_content=\"The stock market is down 500 points today due to fears of a recession.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    "    id=9,\n",
    ")\n",
    "\n",
    "document_10 = Document(\n",
    "    page_content=\"I have a bad feeling I am going to get deleted :(\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=10,\n",
    ")\n",
    "\n",
    "documents = [\n",
    "    document_1,\n",
    "    document_2,\n",
    "    document_3,\n",
    "    document_4,\n",
    "    document_5,\n",
    "    document_6,\n",
    "    document_7,\n",
    "    document_8,\n",
    "    document_9,\n",
    "    document_10,\n",
    "]\n",
    "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "\n",
    "vector_store.add_documents(documents=documents, ids=uuids)\n",
    "\n",
    "\n",
    "results = vector_store.similarity_search(\n",
    "    \"LangChain provides abstractions to make working with LLMs easy\",\n",
    "    k=2,\n",
    "    filter={\"source\": \"tweet\"},\n",
    ")\n",
    "for res in results:\n",
    "    print(f\"* {res.page_content} [{res.metadata}]\")\n",
    "\n",
    "\n",
    "\n",
    "#k=1 specifies the number of results that the similarity search should return.\n",
    "results = vector_store.similarity_search_with_score(\n",
    "    \"Will it be hot tomorrow?\", k=1, filter={\"source\": \"news\"}\n",
    ")\n",
    "for res, score in results:\n",
    "    print(f\"* [SIM={score:3f}] {res.page_content} [{res.metadata}]\")\n",
    "\n",
    "#Search by vector\n",
    "results = vector_store.similarity_search_by_vector(\n",
    "    embedding=embeddings.embed_query(\"I love green eggs and ham!\"), k=1\n",
    ")\n",
    "for doc in results:\n",
    "    print(f\"* {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* APJ was born in india [{'source': 'D://GenAI//kalam.txt'}]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "file_path = \"D://GenAI//kalam.txt\"\n",
    "\n",
    "loader = TextLoader(file_path)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "final_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection1\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db1\",  # Where to save data locally, remove if not necessary\n",
    ")\n",
    "\n",
    "vector_store.add_documents(final_docs)\n",
    "\n",
    "results = vector_store.similarity_search(\n",
    "    \"Where is APJ Born\",\n",
    "    k=1,\n",
    ")\n",
    "for res in results:\n",
    "    print(f\"* {res.page_content} [{res.metadata}]\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Ollama \n",
    "\n",
    "# 2.1 Understanding Ollama\n",
    "- Ollama is a platform for running large language models locally. It allows you to download different models to your local   machine and use them for tasks like natural language processing (NLP), text generation, and more.\n",
    "\n",
    "# Key Features:\n",
    "- Download and run models locally.\n",
    "- Supports multiple pre-trained models.\n",
    "- Can be integrated with other applications.\n",
    "- Ensures data privacy as everything runs on your local machine.\n",
    "\n",
    "# 2.2 How to Use Ollama\n",
    "- Step 1: Install Ollama\n",
    "  To get started with Ollama, you need to first download and install the Ollama CLI tool. You can do this by following the instructions provided on the official website:\n",
    "\n",
    "# Visit Ollama's website.\n",
    "- Download the installer for your operating system (Windows, Mac, Linux).\n",
    "- Run the installer to set up the Ollama CLI on your machine.\n",
    "\n",
    "# 2.3 Advantages of Running Models Locally\n",
    "- Privacy: Your data stays on your local machine, offering more privacy than cloud-based solutions.\n",
    "- Customization: You can fine-tune models for your specific use case if needed.\n",
    "- Faster Responses: Running models locally can provide faster inference times as compared to cloud-based solutions, \n",
    "  especially if  you have powerful hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sample output generated in CMD\n",
    "\n",
    "- C:\\Users\\Adari Shanmukh>ollama run gemma2:2b\n",
    "pulling manifest\n",
    "pulling 7462734796d6... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.6 GB\n",
    "pulling e0a42594d802... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  358 B\n",
    "pulling 097a36493f71... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 8.4 KB\n",
    "pulling 2490e7468436... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   65 B\n",
    "pulling e18ad7af7efb... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  487 B\n",
    "verifying sha256 digest\n",
    "writing manifest\n",
    "success\n",
    ">>> hi\n",
    "Hi there! üëã How can I help you today? üòä\n",
    "\n",
    "\n",
    ">>> who are you\n",
    "I'm Gemma, an AI assistant developed by the Google DeepMind team.  I'm here to help you with any questions or\n",
    "tasks you might have!\n",
    "\n",
    "What can I do for you today? üòÑ\n",
    "\n",
    "\n",
    ">>> Send a message (/? for help)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OllamaEmbeddings(base_url='http://localhost:11434', model='gemma2:2b', embed_instruction='passage: ', query_instruction='query: ', mirostat=None, mirostat_eta=None, mirostat_tau=None, num_ctx=None, num_gpu=None, num_thread=None, repeat_last_n=None, repeat_penalty=None, temperature=None, stop=None, tfs_z=None, top_k=None, top_p=None, show_progress=False, headers=None, model_kwargs=None)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#olama embedding\n",
    "#https://ollama.com/blog/embedding-models\n",
    "\n",
    "\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"gemma2:2b\")\n",
    "\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2304\n"
     ]
    }
   ],
   "source": [
    "r1 = embeddings.embed_documents(\n",
    "    [\n",
    "        \"Hi this is shanmukh\",\n",
    "        \"Welcome to INDIA\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(len(r1[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2304\n"
     ]
    }
   ],
   "source": [
    "r2 = embeddings.embed_query(\"Who is shanmukh Adari\")\n",
    "print(len(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "\n",
    "r3 = embeddings.embed_query(\"Who is shanmukh Adari\")\n",
    "print(len(r3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Embedding Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "#https://huggingface.co/models\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "r3 = embeddings.embed_documents(\n",
    "    [\n",
    "        \"Hi this is shanmukh\",\n",
    "        \"Welcome to INDIA\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(len(r3[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Stores Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- FAISS (Facebook AI Similarity Search) is a library for efficient similarity search and clustering of high-dimensional vectors (embeddings). It's commonly used for tasks like:\n",
    "\n",
    "- Search: Finding similar items (e.g., documents, images) based on their vector embeddings.\n",
    "- Clustering: Grouping vectors into clusters for tasks like document categorization.\n",
    "Key Features:\n",
    "- Fast Search: Optimized for high-speed nearest neighbor search in large datasets.\n",
    "- Efficient Indexing: Supports different indexing methods to balance speed and memory usage.\n",
    "- Scalability: Works well with large datasets, even in millions or billions of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain_community.vectorstores.faiss.FAISS object at 0x000002285DFAFB50>\n",
      "* - Musk is the CEO of **Tesla**, an electric vehicle (EV) company focused on sustainable energy. [{'source': 'D://GenAI//elon.txt'}]\n"
     ]
    }
   ],
   "source": [
    "#How to load txt file\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "file_path = \"D://GenAI//elon.txt\"\n",
    "\n",
    "loader = TextLoader(file_path)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "final_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"gemma2:2b\")\n",
    "\n",
    "vector_store = FAISS.from_documents(final_docs,embeddings)\n",
    "\n",
    "print(vector_store)\n",
    "\n",
    "\n",
    "results = vector_store.similarity_search(\n",
    "    \"is the CEO of Tesla\",\n",
    "    k=1,\n",
    ")\n",
    "\n",
    "for res in results:\n",
    "    print(f\"* {res.page_content} [{res.metadata}]\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='f77c08a2-0898-4729-8487-dbc0fa00a169', metadata={'source': 'D://GenAI//elon.txt'}, page_content='- Musk is the CEO of **Tesla**, an electric vehicle (EV) company focused on sustainable energy.'),\n",
       " Document(id='35d34ef8-cb6c-41e4-a14c-9359a3692fb3', metadata={'source': 'D://GenAI//elon.txt'}, page_content='**Mars colonization plan**.'),\n",
       " Document(id='ec9fe998-fdde-4668-bf23-7be46e7a604a', metadata={'source': 'D://GenAI//elon.txt'}, page_content='### 1. **Tesla**:'),\n",
       " Document(id='1017837c-aed3-499a-bb94-79348d2fa564', metadata={'source': 'D://GenAI//elon.txt'}, page_content='merge humans with artificial intelligence.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert vector store db into retriver and start querying\n",
    "retriver = vector_store.as_retriever()\n",
    "query=\"CEO of Telsa\"\n",
    "retriver.invoke(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(id='f77c08a2-0898-4729-8487-dbc0fa00a169', metadata={'source': 'D://GenAI//elon.txt'}, page_content='- Musk is the CEO of **Tesla**, an electric vehicle (EV) company focused on sustainable energy.'),\n",
       "  6368.032),\n",
       " (Document(id='35d34ef8-cb6c-41e4-a14c-9359a3692fb3', metadata={'source': 'D://GenAI//elon.txt'}, page_content='**Mars colonization plan**.'),\n",
       "  7681.753),\n",
       " (Document(id='ec9fe998-fdde-4668-bf23-7be46e7a604a', metadata={'source': 'D://GenAI//elon.txt'}, page_content='### 1. **Tesla**:'),\n",
       "  7851.2397),\n",
       " (Document(id='1017837c-aed3-499a-bb94-79348d2fa564', metadata={'source': 'D://GenAI//elon.txt'}, page_content='merge humans with artificial intelligence.'),\n",
       "  8894.1)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Similarity Search With Score\n",
    "#Lower the score higher the chances of result\n",
    "\n",
    "docs_and_similarity_search_score = vector_store.similarity_search_with_score(query)\n",
    "docs_and_similarity_search_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instead of passing sentences we can also pass vectos\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"gemma2:2b\")\n",
    "\n",
    "embedding_query =  embeddings.embed_query(query)\n",
    "\n",
    "\n",
    "docs_and_similarity_search_score = vector_store.similarity_search_with_score(embedding_query)\n",
    "\n",
    "docs_and_similarity_search_score\n",
    "\n",
    "#Save vector store in db\n",
    "vector_store.save_local(\"elon_local_fssai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain_community.vectorstores.faiss.FAISS object at 0x000002287FF30B50>\n",
      "[(Document(id='3a0cb75c-847f-466b-97d7-5a1f5463c2cd', metadata={'source': 'D://GenAI//elon.txt'}, page_content='and leading multiple high-profile technology companies. Here are some key highlights:'), 12123.922), (Document(id='35d34ef8-cb6c-41e4-a14c-9359a3692fb3', metadata={'source': 'D://GenAI//elon.txt'}, page_content='**Mars colonization plan**.'), 13420.537), (Document(id='05d21c78-3517-4a4b-9162-7fbeff5df288', metadata={'source': 'D://GenAI//elon.txt'}, page_content='including automotive, aerospace, and energy.'), 13474.359), (Document(id='1017837c-aed3-499a-bb94-79348d2fa564', metadata={'source': 'D://GenAI//elon.txt'}, page_content='merge humans with artificial intelligence.'), 14485.816)]\n"
     ]
    }
   ],
   "source": [
    "new_db = FAISS.load_local('elon_local_fssai',embeddings,allow_dangerous_deserialization=True)\n",
    "print(new_db)\n",
    "\n",
    "embedding_query =  embeddings.embed_query(query)\n",
    "docs_and_similarity_search_score = vector_store.similarity_search_with_score(embedding_query)\n",
    "\n",
    "print(docs_and_similarity_search_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain_chroma.vectorstores.Chroma object at 0x00000228508FE160>\n",
      "* - Musk is the CEO of **Tesla**, an electric vehicle (EV) company focused on sustainable energy. [{'source': 'D://GenAI//elon.txt'}]\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "\n",
    "file_path = \"D://GenAI//elon.txt\"\n",
    "\n",
    "loader = TextLoader(file_path)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "final_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"gemma2:2b\")\n",
    "\n",
    "vector_store = Chroma.from_documents(final_docs,embeddings,persist_directory=\"./chroma_langchain_db1\") # Where to save data locally, remove if not necessary)\n",
    "\n",
    "print(vector_store)\n",
    "\n",
    "\n",
    "results = vector_store.similarity_search(\n",
    "    \"is the CEO of Tesla\",\n",
    "    k=1,\n",
    ")\n",
    "\n",
    "for res in results:\n",
    "    print(f\"* {res.page_content} [{res.metadata}]\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='d667071a-1bf9-4240-b8e6-6fc3f24e9a73', metadata={'source': 'D://GenAI//elon.txt'}, page_content='- Musk is the CEO of **Tesla**, an electric vehicle (EV) company focused on sustainable energy.'), Document(id='50584165-573e-4d73-8b56-334a29d0ddbb', metadata={'source': 'D://GenAI//elon.txt'}, page_content='### 1. **Tesla**:'), Document(id='b6d7c1e6-90fe-4ca7-89a8-d66a19737894', metadata={'source': 'D://GenAI//elon.txt'}, page_content='- Musk has been involved in various other projects, such as **SolarCity** (solar energy),'), Document(id='71a19eff-e8e1-4751-9e8e-d308269dc502', metadata={'source': 'D://GenAI//elon.txt'}, page_content='and his outspoken personality. He has significantly impacted multiple industries, including')]\n",
      "[(Document(id='3c55c439-7a6d-4937-ad60-a4289b9a175f', metadata={'source': 'D://GenAI//elon.txt'}, page_content='and leading multiple high-profile technology companies. Here are some key highlights:'), 12123.922911225569), (Document(id='05c4a11e-167a-4034-9825-da485f54809a', metadata={'source': 'D://GenAI//elon.txt'}, page_content='**Mars colonization plan**.'), 13420.53750575094), (Document(id='57cb7fa0-5a43-4133-bccd-a6babf31f06d', metadata={'source': 'D://GenAI//elon.txt'}, page_content='including automotive, aerospace, and energy.'), 13474.360132605152), (Document(id='911e9ba9-c141-4cc3-bda9-58ab71d4473d', metadata={'source': 'D://GenAI//elon.txt'}, page_content='merge humans with artificial intelligence.'), 14485.816356209842)]\n",
      "[Document(id='d667071a-1bf9-4240-b8e6-6fc3f24e9a73', metadata={'source': 'D://GenAI//elon.txt'}, page_content='- Musk is the CEO of **Tesla**, an electric vehicle (EV) company focused on sustainable energy.'), Document(id='05c4a11e-167a-4034-9825-da485f54809a', metadata={'source': 'D://GenAI//elon.txt'}, page_content='**Mars colonization plan**.'), Document(id='50584165-573e-4d73-8b56-334a29d0ddbb', metadata={'source': 'D://GenAI//elon.txt'}, page_content='### 1. **Tesla**:'), Document(id='911e9ba9-c141-4cc3-bda9-58ab71d4473d', metadata={'source': 'D://GenAI//elon.txt'}, page_content='merge humans with artificial intelligence.')]\n"
     ]
    }
   ],
   "source": [
    "#load the disk\n",
    "load_vector_db = Chroma(persist_directory=\"./chroma_langchain_db1\",embedding_function=embeddings)\n",
    "\n",
    "results = load_vector_db.similarity_search(\n",
    "    \"is the CEO of Tesla\",\n",
    ")\n",
    "print(results)\n",
    "\n",
    "#get result by score\n",
    "\n",
    "embedding_query =  embeddings.embed_query(query)\n",
    "\n",
    "docs_and_similarity_search_score = load_vector_db.similarity_search_with_score(embedding_query)\n",
    "\n",
    "print(docs_and_similarity_search_score)\n",
    "\n",
    "\n",
    "#convert vector store db into retriver and start querying\n",
    "retriver = load_vector_db.as_retriever()\n",
    "query=\"CEO of Telsa\"\n",
    "print(retriver.invoke(query))\n",
    "\n",
    "\n",
    "\n",
    "#Lots of research needs to be done on this \n",
    "#https://python.langchain.com/docs/how_to/#vector-stores\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
