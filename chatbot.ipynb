{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Simple Chatbot \n",
    "- Memory is Stored in Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Shanmukh! üëã\\n\\nIt's great to meet you. A Python backend developer, eh? That's awesome! \\n\\nWhat kind of projects are you working on these days?  Anything exciting?\\n\\nI'm always eager to learn more about what people are building with Python.  Maybe you can tell me about:\\n\\n* **Your favorite Python frameworks or libraries:** Django, Flask, FastAPI, or something else?\\n* **The most challenging project you've tackled:** What were the hurdles, and how did you overcome them?\\n* **Your dream project:** If you could build anything with Python, what would it be?\\n\\n\\nI'm here to chat and help if you have any questions about Python or anything else. üêç\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 159, 'prompt_tokens': 18, 'total_tokens': 177, 'completion_time': 0.289090909, 'prompt_time': 0.000115149, 'queue_time': 0.018497909, 'total_time': 0.289206058}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-98ef099b-17e0-4123-a2ab-a7a386da3fd9-0', usage_metadata={'input_tokens': 18, 'output_tokens': 159, 'total_tokens': 177})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "groq_api_key=os.getenv('GROQ_API_KEY')\n",
    "\n",
    "groq_model = ChatGroq(model=\"gemma2-9b-it\",groq_api_key=groq_api_key)\n",
    "\n",
    "groq_model.invoke([HumanMessage(content=\"hi i am shanmukh python backend developer\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You are Shanmukh, and you are a Python backend developer.  \\n\\nIs there anything else I can help you with? üòÑ  \\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 191, 'total_tokens': 222, 'completion_time': 0.056363636, 'prompt_time': 0.005869395, 'queue_time': 0.020831545, 'total_time': 0.062233031}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-ce21172f-bc34-461e-bd3e-116ab05be27f-0', usage_metadata={'input_tokens': 191, 'output_tokens': 31, 'total_tokens': 222})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "groq_model.invoke([\n",
    "                    HumanMessage(content=\"hi i am shanmukh python backend developer\"),\n",
    "                    AIMessage(content=\"Hello Shanmukh! üëã\\n\\nIt's great to meet you. A Python backend developer, eh? That's awesome! \\n\\nWhat kind of projects are you working on these days?  Anything exciting?\\n\\nI'm always eager to learn more about what people are building with Python.  Maybe you can tell me about:\\n\\n* **Your favorite Python frameworks or libraries:** Django, Flask, FastAPI, or something else?\\n* **The most challenging project you've tackled:** What were the hurdles, and how did you overcome them?\\n* **Your dream project:** If you could build anything with Python, what would it be?\\n\\n\\nI'm here to chat and help if you have any questions about Python or anything else. üêç\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 159, 'prompt_tokens': 18, 'total_tokens': 177, 'completion_time': 0.289090909, 'prompt_time': 0.000115149, 'queue_time': 0.018497909, 'total_time': 0.289206058}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-98ef099b-17e0-4123-a2ab-a7a386da3fd9-0', usage_metadata={'input_tokens': 18, 'output_tokens': 159, 'total_tokens': 177}),\n",
    "                    HumanMessage(content=\" tell my name and job name\"),\n",
    "                 ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Bob! üëã\n",
      "\n",
      "It's nice to meet you. What can I do for you today? üòÑ\n",
      "\n",
      "Your name is Bob!  üòä  \n",
      "\n",
      "You told me at the beginning of our conversation.  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(groq_model, get_session_history)\n",
    "config = {\"configurable\": {\"session_id\": \"abc2\"}}\n",
    "\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi! I'm Bob\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(response.content)\n",
    "\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Since I don't have access to past conversations, I don't know your name.  \\n\\nWould you like to tell me what it is? üòä  \\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc3\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As a Data Scientist, NumPy is one of my most essential tools! \\n\\n**NumPy (Numerical Python)** is a fundamental Python library for numerical computing. It provides:\\n\\n* **Powerful N-dimensional arrays:** \\n    *  These arrays are much more efficient than standard Python lists for numerical operations, especially when dealing with large datasets. \\n    * They allow for element-wise operations, broadcasting, and advanced indexing, making calculations much faster and more concise.\\n\\n* **Mathematical functions:** NumPy offers a vast collection of mathematical functions optimized for array operations. These include linear algebra, Fourier transforms, random number generation, and more. This saves us from writing repetitive code and allows us to perform complex mathematical manipulations efficiently.\\n\\n* **Broadcasting:** NumPy's broadcasting mechanism simplifies operations between arrays of different shapes under certain conditions. This makes it easier to perform calculations on arrays with varying dimensions.\\n\\n* **Integration with other libraries:** NumPy is the foundation for many other scientific and data science libraries in Python, including Pandas, Scikit-learn, and Matplotlib.\\n\\n**Here's why NumPy is so crucial for Data Scientists:**\\n\\n1. **Efficiency:**  NumPy's optimized arrays and functions significantly speed up numerical computations, which are at the heart of most data science tasks.\\n\\n2. **Conciseness:**  NumPy's syntax allows us to express complex mathematical operations in a compact and readable way, making code cleaner and easier to maintain.\\n\\n3. **Foundation:** NumPy is the bedrock upon which many other data science libraries are built, providing a consistent and efficient way to work with numerical data.\\n\\n4. **Data manipulation:** NumPy's array operations are essential for tasks like data cleaning, transformation, feature engineering, and data aggregation.\\n\\n\\n\\nLet me know if you'd like a deeper dive into specific NumPy functionalities or examples!\\n\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#In summary, MessagesPlaceholder is mandatory when you want to allow for dynamic content (such as user input or generated responses) to be inserted into your template in a flexible and dynamic manner. Without it, the prompt would lack this flexibility.\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a DataScientist.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | groq_model\n",
    "\n",
    "response = chain.invoke({\"messages\": [HumanMessage(content=\"What is numpy\")]})\n",
    "\n",
    "response.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Jim! üëã\\n\\nIt's nice to meet you. What can I do for you today? \\n\\nAre you working on a project that needs some data science expertise? Or are you just curious about what a data scientist does? \\n\\nI'm happy to chat about anything related to data! üòä  \\n\\n\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history = RunnableWithMessageHistory(chain, get_session_history)\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"abc6\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi! I'm Jim\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You asked: \"Hi myname is Jim\". \\n\\nIt\\'s nice to meet you, Jim! üòä  \\n\\nWhat can I do for you today?\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What is first question i asked start with Hi myname, and answer\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"2 + 2 = 4 \\n\\nLet me know if you have any other calculations you'd like me to do! üßÆ  üòÑ\\n\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"2+2\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You have asked me 4 questions so far.  üòä \\n'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Total count of questions i asked\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¬°Hola Bob! üëã \\n\\n¬øC√≥mo puedo ayudarte hoy? üòä \\n'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | groq_model\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"hi! I'm bob\")], \"language\": \"Spanish\"}\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")\n",
    "config = {\"configurable\": {\"session_id\": \"abc11\"}}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§ü‡•â‡§°! \\n\\n‡§Æ‡•à‡§Ç ‡§Ü‡§™‡§ï‡•Ä ‡§Æ‡§¶‡§¶ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ø‡§π‡§æ‡§Å ‡§π‡•Ç‡§Å‡•§ ‡§Ü‡§™ ‡§Æ‡•Å‡§ù‡§∏‡•á ‡§ï‡•Å‡§õ ‡§≠‡•Ä ‡§™‡•Ç‡§õ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç, ‡§Æ‡•à‡§Ç ‡§Ö‡§™‡§®‡•Ä ‡§™‡•Ç‡§∞‡•Ä ‡§ï‡•ã‡§∂‡§ø‡§∂ ‡§ï‡§∞‡•Ç‡§Å‡§ó‡§æ! üòä \\n\\n'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"hi! I'm todd\")], \"language\": \"hindi\"},\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing Conversational History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LangChain comes with a few built-in helpers for managing a list of messages. In this case we'll use the trim_messages helper  to   reduce how many messages we're sending to the model. The trimmer allows us to specify how many tokens we want to keep, along   with other parameters like if we want to always keep the system message and whether to allow partial messages:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant your name is sara\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Welcome!!! to India Bob', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=51,\n",
    "    strategy=\"last\",\n",
    "    token_counter=groq_model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant your name is sara\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "    AIMessage(content=\"Welcome!!! to India Bob\"),\n",
    "\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)\n",
    "#few msgs are trimmed they are ignored because it exceeds 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You told me your name is Bob! üòÑ \\n\\nIs there anything else I can help you with today?\\n'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\") | trimmer)\n",
    "    | prompt\n",
    "    | groq_model\n",
    ")\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"what's my name?\")],\n",
    "        \"language\": \"English\",\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"12211\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As a helpful assistant, I don't have access to past conversations or personal information about you, including your name. \\n\\nWould you like to tell me your name? üòä \\n\\n\""
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"whats my name?\")],\n",
    "        \"language\": \"English\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
