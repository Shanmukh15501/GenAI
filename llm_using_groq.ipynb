{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Groq is a company that develops high-performance hardware and software solutions for AI inference. Their core product, the LPU™   Inference Engine, is designed to accelerate AI tasks, offering significant speed improvements. It provides superior compute power, enabling faster processing of AI models. The platform is optimized for energy efficiency, reducing operational costs. Overall, Groq aims to enhance AI performance across various applications, including machine learning and deep learning workloads.\n",
    "- https://console.groq.com/playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_key=os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "groq_api_key=os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "groq_model = ChatGroq(model=\"gemma2-9b-it\",groq_api_key=groq_api_key)\n",
    "\n",
    "openai_model = ChatOpenAI(model=\"gpt-4o\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='नमस्ते, आप कैसे हैं? \\n\\n(Namaste, aap kaise hain?) \\n' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 18, 'total_tokens': 42, 'completion_time': 0.043636364, 'prompt_time': 8.0859e-05, 'queue_time': 0.021173831, 'total_time': 0.043717223}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run-92769055-5a5d-47e6-b43e-481892a155fd-0' usage_metadata={'input_tokens': 18, 'output_tokens': 24, 'total_tokens': 42}\n",
      "\n",
      "\n",
      "\n",
      "content='नमस्ते, आप कैसे हैं?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 20, 'total_tokens': 30, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_50cad350e4', 'finish_reason': 'stop', 'logprobs': None} id='run-b34ea185-8989-4af9-adbb-93f9bdf51f40-0' usage_metadata={'input_tokens': 20, 'output_tokens': 10, 'total_tokens': 30, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage,AIMessage,HumanMessage\n",
    "\n",
    "message = [\n",
    "        SystemMessage(\"Translate from English to Hindi\"),\n",
    "        HumanMessage(content=\"Hello how are you\")\n",
    "]\n",
    "\n",
    "response1 = groq_model.invoke(message)\n",
    "response2 = openai_model.invoke(message)\n",
    "\n",
    "print(response1)\n",
    "print(\"\\n\"*2)\n",
    "print(response2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'नमस्ते, आप कैसे हैं? \\n\\n(Namaste, aap kaise hain?) \\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "parser.invoke(response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using LCEL we can chain the components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "नमस्ते, आप कैसे हैं? \n",
      "(Namaste, aap kaise hain?) \n",
      "\n",
      "\n",
      "This translates to \"Hello, how are you?\" in Hindi. \n",
      "\n",
      "नमस्ते, आप कैसे हैं?\n"
     ]
    }
   ],
   "source": [
    "chain1= groq_model|parser\n",
    "\n",
    "print(chain1.invoke(message))\n",
    "\n",
    "chain2= openai_model|parser\n",
    "\n",
    "print(chain2.invoke(message))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'नमस्ते, मैं शंमूख हूँ। \\n\\n(Namaste, main Shanmukh hoon.) \\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#another efficient method is using prompt templates\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "generic_prompt = \"Translate into following {language}\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\",generic_prompt),(\"user\",\"{text}\")])\n",
    "\n",
    "prompt.invoke({\"language\":\"Hindi\",\"text\":\"Hello this is shanmukh\"})\n",
    "\n",
    "chain3 = prompt | groq_model | parser\n",
    "\n",
    "chain3.invoke({\"language\":\"Hindi\",\"text\":\"Hello this is shanmukh\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lama_rag.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lama_rag.py\n",
    "\n",
    "import streamlit as st \n",
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.vectorstores import FAISS  \n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key=os.getenv('GROQ_API_KEY','xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx')\n",
    "\n",
    "groq_model = ChatGroq(model_name=\"llama-3.2-1b-preview\",groq_api_key=groq_api_key)\n",
    "\n",
    "generic_prompt = \"\"\"\n",
    "                    Answer the following questions based on given context only.\n",
    "                    Please provide most accurate response based on question.\n",
    "                    <context>\n",
    "                    {context}\n",
    "                    </context>\n",
    "                \"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\",generic_prompt),(\"user\",\"{input}\")])\n",
    "\n",
    "\n",
    "def create_vector_embeddings_ollama():\n",
    "    if \"vectors\" not in st.session_state:\n",
    "        st.session_state.embeddings = OpenAIEmbeddings()\n",
    "        st.session_state.loader = PyPDFDirectoryLoader(\"research\") #data ingestion step\n",
    "        st.session_state.docs  = st.session_state.loader.load() #documents loading\n",
    "        st.session_state.text_splitter =  RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "        st.session_state.final_documents = st.session_state.text_splitter.split_documents(st.session_state.docs)\n",
    "        st.session_state.vectors = FAISS.from_documents(st.session_state.final_documents,st.session_state.embeddings)\n",
    "        \n",
    "user_prompt = st.text_input(\"Enter the query from the documents\")\n",
    "if st.button(\"Document Embedding\"):\n",
    "    create_vector_embeddings_ollama()\n",
    "    st.write(\"Vector db is ready\")\n",
    "\n",
    "if user_prompt:\n",
    "    document_chain = create_stuff_documents_chain(groq_model,prompt)\n",
    "    retriver  =  st.session_state.vectors.as_retriever()\n",
    "    retriver_chain = create_retrieval_chain(retriver,document_chain)\n",
    "    response = retriver_chain.invoke({'input':user_prompt})\n",
    "    st.write(response['answer'])\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process <Popen: returncode: None args: ['streamlit', 'run', 'lama_rag.py']>\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "process = subprocess.Popen([\"streamlit\",\"run\",\"lama_rag.py\"])\n",
    "\n",
    "print(\"process\",process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
